{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMPdPkOSrDcQMzX2a+kIVQn"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "cSo8gIINXE1P"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "lLuSZbSdQ4uQ"
      },
      "outputs": [],
      "source": [
        "from abc import ABC, abstractmethod\n",
        "\n",
        "class SupervisedMachineLearningModel(ABC):\n",
        "    @abstractmethod\n",
        "    def train(self, X, y):\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def predict(self, X):\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Linear Regression\n",
        "\n",
        "#### Data\n",
        "```\n",
        " ----------\n",
        "|  X |  y  |\n",
        " ----------\n",
        "\n",
        "y is continous value\n",
        "```\n",
        "\n",
        "#### 1. Model\n",
        "\n",
        "```\n",
        "y_pred = wX + b\n",
        "\n",
        "where,\n",
        "X = Independent variable\n",
        "y_pred = Dependent variable\n",
        "w = weight or slope\n",
        "b = intercept or bias\n",
        "```\n",
        "\n",
        "#### 2. Loss\n",
        "\n",
        "```\n",
        "J(w) = (1 / (2 * m)) * Σ[(y_pred(i) - y(i))^2]\n",
        "\n",
        "where,\n",
        "m is the number of training examples.\n",
        "y(i) is the actual output for the i-th example.\n",
        "y_pred(i) is the predicted output for the i-th example.\n",
        "```\n",
        "\n",
        "#### 3. Gradient\n",
        "\n",
        "```\n",
        "∂J(w) / ∂wj = (1 / m) * Σ[(y_pred(i) - y(i)) * Xj(i)]\n",
        "\n",
        "Compute the error term (y_pred(i) - y(i)).\n",
        "Multiply the error by the corresponding feature Xj(i).\n",
        "Take the average over all examples.\n",
        "```\n",
        "\n",
        "#### 4. Gradient Descent Update\n",
        "\n",
        "```\n",
        "wj = wj - α * ∂J(w) / ∂wj\n",
        "\n",
        "where,\n",
        "∂J(w) / ∂wj is gradient\n",
        "```\n"
      ],
      "metadata": {
        "id": "Meg5Fe3FR8tf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LinearRegression(SupervisedMachineLearningModel):\n",
        "    def __init__(self, learning_rate: float = 0.01, epochs: int = 1000) -> None:\n",
        "        self.learning_rate = learning_rate\n",
        "        self.epochs = epochs\n",
        "        self.weights = None\n",
        "\n",
        "    def train(self, X, y, verbose=False) -> None:\n",
        "        \"\"\"Train the linear regression model.\"\"\"\n",
        "        m, n = X.shape\n",
        "        # Initialize weights (including bias as part of weights)\n",
        "        self.weights = np.zeros(n + 1)\n",
        "\n",
        "        # Add bias as the first column of X\n",
        "        X = np.c_[np.ones(m), X]\n",
        "\n",
        "        for epoch in range(self.epochs):\n",
        "            # Predictions\n",
        "            y_pred = X @ self.weights\n",
        "\n",
        "            # Error calculation\n",
        "            error = y_pred - y\n",
        "\n",
        "            # Loss\n",
        "            loss = (1 / (2 * m)) * np.sum(error ** 2)\n",
        "\n",
        "            # Gradient calculation\n",
        "            gradient = (1 / m) * (X.T @ error)\n",
        "\n",
        "            # Update weights\n",
        "            self.weights -= self.learning_rate * gradient\n",
        "\n",
        "            if verbose:\n",
        "                # Display\n",
        "                print(f\"iteration: ({epoch+1}/{self.epochs}) - error: {error} - loss: {loss} - gradient - {gradient}\")\n",
        "\n",
        "    def predict(self, X) -> np.ndarray:\n",
        "        \"\"\"Predict using the linear regression model.\"\"\"\n",
        "        m = X.shape[0]\n",
        "        # Add bias as the first column of X\n",
        "        X = np.c_[np.ones(m), X]\n",
        "        return X @ self.weights"
      ],
      "metadata": {
        "id": "sssU-INvR5Sz"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Logistic Regression\n",
        "\n",
        "#### Data\n",
        "\n",
        "```\n",
        " ----------\n",
        "|  X |  y  |\n",
        " ----------\n",
        "\n",
        "y is discrete value\n",
        "```\n",
        "\n",
        "#### 1. Model\n",
        "\n",
        "```\n",
        "y_pred = sigmoid(X @ weights)\n",
        "\n",
        "where,\n",
        "sigmoid = 1/(1+exp(-z))\n",
        "\n",
        "z = w0 + w1*x1 + w2*x2 + ... + wn*xn\n",
        "w0 is bias term\n",
        "```\n",
        "\n",
        "#### 2. Loss\n",
        "\n",
        "```\n",
        "Cost = -(1/m) * Σ [y * log(y_pred) + (1 - y) * log(1 - y_pred)]\n",
        "```\n",
        "\n",
        "#### 3. Gradient Calculation\n",
        "\n",
        "```\n",
        "# Error\n",
        "error = y_pred - y\n",
        "\n",
        "# Gradient\n",
        "gradient = (1 / m) * (X.T @ error)\n",
        "\n",
        "# Weight update\n",
        "weights -= learning_rate * gradient\n",
        "```"
      ],
      "metadata": {
        "id": "Fx9t4yvlcN-j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LogisticRegression(SupervisedMachineLearningModel):\n",
        "    def __init__(self, learning_rate: float = 0.01, epochs: int = 1000, threshold: float = 0.5):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.epochs = epochs\n",
        "        self.threshold = threshold\n",
        "        self.weights = None\n",
        "        self.mean = None\n",
        "        self.std = None\n",
        "\n",
        "    @staticmethod\n",
        "    def __sigmoid(z):\n",
        "        \"\"\"Compute the sigmoid function.\"\"\"\n",
        "        return np.clip(1 / (1 + np.exp(-z)), 1e-10, 1 - 1e-10)\n",
        "\n",
        "    def _standardize(self, X):\n",
        "        \"\"\"Standardize features (zero mean and unit variance).\"\"\"\n",
        "        if self.mean is None or self.std is None:\n",
        "            self.mean = np.mean(X, axis=0)\n",
        "            self.std = np.std(X, axis=0)\n",
        "        return (X - self.mean) / (self.std + 1e-10)\n",
        "\n",
        "    def train(self, X, y, verbose=False):\n",
        "        \"\"\"Train the logistic regression model.\"\"\"\n",
        "        m, n = X.shape\n",
        "        X = np.c_[np.ones(m), X]  # Add bias term\n",
        "        X = self._standardize(X)  # Standardize the data\n",
        "        self.weights = np.random.randn(n + 1) * 0.01  # Small random initialization\n",
        "\n",
        "        for epoch in range(self.epochs):\n",
        "            z = X @ self.weights  # Compute linear combination\n",
        "            y_pred = self.__sigmoid(z)  # Apply sigmoid\n",
        "            error = y_pred - y  # Calculate error\n",
        "            loss = -(1 / m) * np.sum(y * np.log(np.clip(y_pred, 1e-10, 1 - 1e-10)) + (1 - y) * np.log(np.clip(1 - y_pred, 1e-10, 1 - 1e-10)))\n",
        "            gradient = (1 / m) * (X.T @ error)  # Compute gradient\n",
        "\n",
        "            if np.any(np.isnan(gradient)):\n",
        "                print(f\"NaN in gradient at epoch {epoch+1}\")\n",
        "                break\n",
        "\n",
        "            self.weights -= self.learning_rate * gradient  # Update weights\n",
        "\n",
        "            if verbose:\n",
        "                print(f\"Epoch {epoch+1}/{self.epochs} - Loss: {loss:.6f} - Weights: {self.weights}\")\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Make predictions using the logistic regression model.\"\"\"\n",
        "        X = np.c_[np.ones(X.shape[0]), X]  # Add bias term\n",
        "        X = self._standardize(X)  # Standardize the data\n",
        "        probabilities = self.__sigmoid(X @ self.weights)  # Get predicted probabilities\n",
        "        return (probabilities >= self.threshold).astype(int)  # Apply threshold"
      ],
      "metadata": {
        "id": "PK_FSjaVdrLr"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1. Decision Tree Overview**\n",
        "A decision tree is a tree-like model used for classification and regression. It splits data into subsets based on feature values, using a tree structure to make decisions.\n",
        "\n",
        "---\n",
        "\n",
        "#### **2. Key Concepts**\n",
        "\n",
        "##### **2.1 Root Node**\n",
        "- The topmost node in a tree.\n",
        "- Represents the entire dataset and is split into subsets.\n",
        "\n",
        "##### **2.2 Internal Nodes**\n",
        "- Nodes where the dataset is further split based on conditions on features.\n",
        "\n",
        "##### **2.3 Leaf Nodes**\n",
        "- Terminal nodes that represent a class label (for classification) or a value (for regression).\n",
        "\n",
        "##### **2.4 Splitting Criteria**\n",
        "The choice of feature and threshold to split the data is determined by a splitting criterion.\n",
        "\n",
        "---\n",
        "\n",
        "#### **3. Splitting Criteria**\n",
        "\n",
        "##### **3.1 Classification**\n",
        "\n",
        "##### **Information Gain (IG)**  \n",
        "- Measures the reduction in entropy after a split.\n",
        "- Formula:  \n",
        "  ```\n",
        "  IG = Entropy(parent) - [Weighted average of Entropy(children)]\n",
        "  ```\n",
        "\n",
        "##### **Entropy**  \n",
        "- Measures the impurity of the dataset.  \n",
        "- Formula:  \n",
        "  ```\n",
        "  Entropy = -∑(p * log2(p))\n",
        "  ```  \n",
        "  where `p` is the proportion of data points in each class.\n",
        "\n",
        "##### **Gini Index**  \n",
        "- Another metric for impurity, ranging from 0 (pure) to 0.5 (most impure for binary classification).  \n",
        "- Formula:  \n",
        "  ```\n",
        "  Gini = 1 - ∑(p^2)\n",
        "  ```\n",
        "\n",
        "#### **3.2 Regression**\n",
        "\n",
        "###### **Mean Squared Error (MSE)**  \n",
        "- Measures the variance within the data at a node.  \n",
        "- Formula:  \n",
        "  ```\n",
        "  MSE = (1 / N) * ∑(y - y_mean)^2\n",
        "  ```  \n",
        "  where `N` is the number of samples at the node, and `y_mean` is the average value of the target variable at that node.\n",
        "\n",
        "###### **Mean Absolute Error (MAE)**  \n",
        "- Measures the average absolute deviation.  \n",
        "- Formula:  \n",
        "  ```\n",
        "  MAE = (1 / N) * ∑|y - y_mean|\n",
        "  ```\n",
        "\n",
        "---\n",
        "\n",
        "#### **4. Pruning**\n",
        "Pruning reduces the size of the tree to prevent overfitting.\n",
        "\n",
        "##### **4.1 Pre-Pruning**\n",
        "- Stop splitting when certain criteria (e.g., depth or minimum samples per leaf) are met.\n",
        "\n",
        "##### **4.2 Post-Pruning**\n",
        "- Build the full tree and then remove branches that have low importance.\n",
        "\n",
        "---\n",
        "\n",
        "#### **5. Advantages**\n",
        "- Simple to understand and interpret.\n",
        "- Requires little data preprocessing.\n",
        "- Handles both numerical and categorical features.\n",
        "\n",
        "---\n",
        "\n",
        "#### **6. Disadvantages**\n",
        "- Prone to overfitting (especially with deep trees).\n",
        "- Can be sensitive to small changes in data (unstable splits).\n",
        "- Biased towards features with many levels (e.g., categorical features with high cardinality).\n",
        "\n",
        "---\n",
        "\n",
        "#### **7. Decision Tree Model Equation**\n",
        "The model prediction is computed as:\n",
        "- **For Classification:** The class of the majority samples in the leaf node.  \n",
        "  ```\n",
        "  y_pred = majority_class(leaf_node)\n",
        "  ```\n",
        "\n",
        "- **For Regression:** The average of the target values in the leaf node.  \n",
        "  ```\n",
        "  y_pred = mean(target_values_in_leaf_node)\n",
        "  ```\n",
        "\n",
        "---\n",
        "\n",
        "#### **8. Common Hyperparameters**\n",
        "- `max_depth`: Maximum depth of the tree.\n",
        "- `min_samples_split`: Minimum number of samples required to split an internal node.\n",
        "- `min_samples_leaf`: Minimum number of samples required to be in a leaf node.\n",
        "- `criterion`: Metric used to evaluate splits (e.g., Gini, Entropy, MSE).\n",
        "\n"
      ],
      "metadata": {
        "id": "Rz14AA0m5MVD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecisionTree(SupervisedMachineLearningModel):\n",
        "    def __init__(self, max_depth=None, criterion='gini'):\n",
        "        self.max_depth = max_depth\n",
        "        self.criterion = criterion\n",
        "        self.tree = None\n",
        "\n",
        "    def __class_probabilities(self, y):\n",
        "        \"\"\"Compute the class probabilities for a given set of labels.\"\"\"\n",
        "        unique, counts = np.unique(y, return_counts=True)\n",
        "        return counts / len(y)\n",
        "\n",
        "    def __entropy(self, y):\n",
        "        \"\"\"Compute the entropy of a dataset.\"\"\"\n",
        "        probabilities = self.__class_probabilities(y)\n",
        "        return -np.sum(probabilities * np.log2(probabilities + 1e-10))\n",
        "\n",
        "    def __gini(self, y):\n",
        "        \"\"\"Compute the Gini index of a dataset.\"\"\"\n",
        "        probabilities = self.__class_probabilities(y)\n",
        "        return 1 - np.sum(probabilities ** 2)\n",
        "\n",
        "    def __impurity(self, y, criterion):\n",
        "        \"\"\"Compute the impurity of a dataset based on the specified criterion.\"\"\"\n",
        "        if criterion == 'entropy':\n",
        "            return self.__entropy(y)\n",
        "        elif criterion == 'gini':\n",
        "            return self.__gini(y)\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported criterion: {criterion}, choose between 'entropy' and 'gini'.\")\n",
        "\n",
        "    def __split(self, X, y, feature_index, threshold):\n",
        "        \"\"\"Split the dataset based on a feature and threshold.\"\"\"\n",
        "        left_indices = X[:, feature_index] < threshold\n",
        "        right_indices = ~left_indices\n",
        "        return left_indices, right_indices\n",
        "\n",
        "    def __best_split(self, X, y):\n",
        "        \"\"\"Find the best split for a dataset.\"\"\"\n",
        "        best_impurity = float('inf')\n",
        "        best_feature_index, best_threshold = None, None\n",
        "\n",
        "        for feature_index in range(X.shape[1]):\n",
        "            thresholds = np.unique(X[:, feature_index])\n",
        "            for threshold in thresholds:\n",
        "                left_indices, right_indices = self.__split(X, y, feature_index, threshold)\n",
        "                if len(left_indices) == 0 or len(right_indices) == 0:\n",
        "                    continue\n",
        "\n",
        "                left_impurity = self.__impurity(y[left_indices], self.criterion)\n",
        "                right_impurity = self.__impurity(y[right_indices], self.criterion)\n",
        "                weighted_impurity = (len(left_indices) / len(y)) * left_impurity + (len(right_indices) / len(y)) * right_impurity\n",
        "\n",
        "                print(f\"Feature: {feature_index}, Threshold: {threshold}, Weighted Impurity: {weighted_impurity}\")\n",
        "\n",
        "                if weighted_impurity < best_impurity:\n",
        "                    best_impurity = weighted_impurity\n",
        "                    best_feature_index = feature_index\n",
        "                    best_threshold = threshold\n",
        "\n",
        "        return best_feature_index, best_threshold\n",
        "\n",
        "    def __build_tree(self, X, y, depth):\n",
        "        \"\"\"Recursively build the decision tree.\"\"\"\n",
        "        if (len(np.unique(y)) == 1) or (depth == 0):\n",
        "            leaf_class = np.bincount(y).argmax()  # Majority class\n",
        "            print(f\"Stopping with leaf class: {leaf_class}\")\n",
        "            return {'class': leaf_class}\n",
        "\n",
        "        feature_index, threshold = self.__best_split(X, y)\n",
        "        print(f\"Best split: feature_index={feature_index}, threshold={threshold}\")\n",
        "\n",
        "        if feature_index is None:\n",
        "            leaf_class = np.bincount(y).argmax()  # Majority class\n",
        "            print(f\"Stopping with leaf class: {leaf_class}\")\n",
        "            return {'class': leaf_class}\n",
        "\n",
        "        left_indices, right_indices = self.__split(X, y, feature_index, threshold)\n",
        "        left_subtree = self.__build_tree(X[left_indices], y[left_indices], depth - 1)\n",
        "        right_subtree = self.__build_tree(X[right_indices], y[right_indices], depth - 1)\n",
        "        return {\n",
        "            'feature_index': feature_index,\n",
        "            'threshold': threshold,\n",
        "            'left': left_subtree,\n",
        "            'right': right_subtree\n",
        "        }\n",
        "\n",
        "    def train(self, X, y):\n",
        "        \"\"\"Train the decision tree.\"\"\"\n",
        "        self.tree = self.__build_tree(X, y, self.max_depth)\n",
        "\n",
        "    def __predict_sample(self, x, tree):\n",
        "        \"\"\"Recursively traverse the decision tree for a single sample.\"\"\"\n",
        "        if 'class' in tree:\n",
        "            return tree['class']\n",
        "\n",
        "        feature_index = tree['feature_index']\n",
        "        threshold = tree['threshold']\n",
        "        if x[feature_index] <= threshold:\n",
        "            return self.__predict_sample(x, tree['left'])\n",
        "        else:\n",
        "            return self.__predict_sample(x, tree['right'])\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Predict using the decision tree.\"\"\"\n",
        "        return np.array([self.__predict_sample(x, self.tree) for x in X])"
      ],
      "metadata": {
        "id": "rg_teFAw5P0u"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test"
      ],
      "metadata": {
        "id": "XQdioKdVbfHM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install ipytest --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Mb5rkZkb6PH",
        "outputId": "c2ae85d3-e572-4ef6-f355-543280341853"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.6 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.5/1.6 MB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def skip(func):\n",
        "    \"\"\"Decorator to skip executing functions\"\"\"\n",
        "    def wrapper(*args, **kwargs):\n",
        "        print(f\"{func.__name__} is skipped\")\n",
        "    return wrapper"
      ],
      "metadata": {
        "id": "Q6AkJwMSiczy"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ipytest\n",
        "ipytest.autoconfig()\n",
        "\n",
        "# linear_regression\n",
        "def test_linear_regression():\n",
        "    X = np.array([[1], [2], [3], [4], [5], [6], [7]])\n",
        "    y = np.array([3, 5, 7, 9, 11, 13, 15])  # y = 2x + 1\n",
        "    model = LinearRegression(learning_rate=0.01, epochs=1000)\n",
        "    model.train(X[:4], y[:4], verbose=False)\n",
        "    predictions = model.predict(X[5:])\n",
        "    expected = y[5:]\n",
        "    assert np.allclose(predictions, expected, rtol=1e-2, atol=1e-2, equal_nan=False), f\"predictions {predictions} do not match expected {expected}\"\n",
        "\n",
        "# logistic_regression\n",
        "def test_logistic_regression():\n",
        "    X = np.array([[1], [2], [3], [4], [5], [6], [7], [8], [9], [10]])  # 10 samples, 1 feature\n",
        "    y = np.array([1, 0, 1, 0, 1, 0, 1, 0, 1, 0])  # Binary labels alternating 1 and 0\n",
        "\n",
        "    # Initialize the logistic regression model\n",
        "    model = LogisticRegression(learning_rate=0.01, epochs=15000, threshold=0.5)\n",
        "\n",
        "    # Train on the first 5 samples\n",
        "    model.train(X[:5], y[:5], verbose=False)\n",
        "\n",
        "    # Test on the remaining 5 samples\n",
        "    predictions = model.predict(X[5:])\n",
        "    expected = y[5:]\n",
        "\n",
        "    # Assert if predictions match expected labels\n",
        "    assert np.allclose(predictions, expected, rtol=1e-2, atol=1e-2, equal_nan=False), f\"Predictions {predictions} do not match expected {expected}\"\n",
        "\n",
        "    # Output results\n",
        "    accuracy = np.mean(predictions == expected)\n",
        "    print(f\"Model accuracy: {accuracy:.4f}\")\n",
        "\n",
        "\n",
        "# decision_tree\n",
        "def test_decision_tree():\n",
        "    X = np.array([\n",
        "        [1, 2],\n",
        "        [2, 3],\n",
        "        [3, 4],\n",
        "        [4, 5],\n",
        "        [5, 6],\n",
        "        [6, 7],\n",
        "        [7, 8],\n",
        "        [8, 9],\n",
        "        [9, 10],\n",
        "        [10, 11]\n",
        "    ])\n",
        "    y = np.array([0, 0, 0, 0, 1, 1, 1, 1, 1, 1])\n",
        "\n",
        "    X_train, X_test = X[:7], X[7:]\n",
        "    y_train, y_test = y[:7], y[7:]\n",
        "\n",
        "    model = DecisionTree(max_depth=3)\n",
        "    model.train(X_train, y_train)\n",
        "\n",
        "    # Print the tree structure for debugging\n",
        "    print(\"Trained Decision Tree Structure:\")\n",
        "    print(model.tree)\n",
        "\n",
        "    # Make predictions and debug output\n",
        "    y_pred = model.predict(X_test)\n",
        "    print(\"Predictions on Test Set:\", y_pred)\n",
        "    print(\"Expected Labels:\", y_test)\n",
        "\n",
        "    assert len(y_pred) == len(y_test), \"Number of predictions does not match the test set size.\"\n",
        "    assert (y_pred == y_test).all(), (\n",
        "        f\"Predictions do not match expected labels. \"\n",
        "        f\"Predicted: {y_pred}, Expected: {y_test}\"\n",
        "    )\n",
        "\n",
        "    print(\"DecisionTree multi-feature test case passed successfully!\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    ipytest.run()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EyrS2muYYMAV",
        "outputId": "517d4cea-1785-44ac-ce0e-1f25db048084"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32m.\u001b[0m\u001b[31mF\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[31m                                                                                        [100%]\u001b[0m\n",
            "============================================= FAILURES =============================================\n",
            "\u001b[31m\u001b[1m_____________________________________ test_logistic_regression _____________________________________\u001b[0m\n",
            "\n",
            "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mtest_logistic_regression\u001b[39;49;00m():\u001b[90m\u001b[39;49;00m\n",
            "        X = np.array([[\u001b[94m1\u001b[39;49;00m], [\u001b[94m2\u001b[39;49;00m], [\u001b[94m3\u001b[39;49;00m], [\u001b[94m4\u001b[39;49;00m], [\u001b[94m5\u001b[39;49;00m], [\u001b[94m6\u001b[39;49;00m], [\u001b[94m7\u001b[39;49;00m], [\u001b[94m8\u001b[39;49;00m], [\u001b[94m9\u001b[39;49;00m], [\u001b[94m10\u001b[39;49;00m]])  \u001b[90m# 10 samples, 1 feature\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        y = np.array([\u001b[94m1\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m])  \u001b[90m# Binary labels alternating 1 and 0\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m# Initialize the logistic regression model\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        model = LogisticRegression(learning_rate=\u001b[94m0.01\u001b[39;49;00m, epochs=\u001b[94m15000\u001b[39;49;00m, threshold=\u001b[94m0.5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m# Train on the first 5 samples\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        model.train(X[:\u001b[94m5\u001b[39;49;00m], y[:\u001b[94m5\u001b[39;49;00m], verbose=\u001b[94mFalse\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m# Test on the remaining 5 samples\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        predictions = model.predict(X[\u001b[94m5\u001b[39;49;00m:])\u001b[90m\u001b[39;49;00m\n",
            "        expected = y[\u001b[94m5\u001b[39;49;00m:]\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m# Assert if predictions match expected labels\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            ">       \u001b[94massert\u001b[39;49;00m np.allclose(predictions, expected, rtol=\u001b[94m1e-2\u001b[39;49;00m, atol=\u001b[94m1e-2\u001b[39;49;00m, equal_nan=\u001b[94mFalse\u001b[39;49;00m), \u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mPredictions \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mpredictions\u001b[33m}\u001b[39;49;00m\u001b[33m do not match expected \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mexpected\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       AssertionError: Predictions [1 1 1 1 1] do not match expected [0 1 0 1 0]\u001b[0m\n",
            "\u001b[1m\u001b[31mE       assert False\u001b[0m\n",
            "\u001b[1m\u001b[31mE        +  where False = <function allclose at 0x7dbe051faf70>(array([1, 1, 1, 1, 1]), array([0, 1, 0, 1, 0]), rtol=0.01, atol=0.01, equal_nan=False)\u001b[0m\n",
            "\u001b[1m\u001b[31mE        +    where <function allclose at 0x7dbe051faf70> = np.allclose\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31m<ipython-input-71-fdcd1ed2cdea>\u001b[0m:30: AssertionError\n",
            "\u001b[36m\u001b[1m===================================== short test summary info ======================================\u001b[0m\n",
            "\u001b[31mFAILED\u001b[0m t_bcec6f13b5ff4ed3912f6a3266a6d14a.py::\u001b[1mtest_logistic_regression\u001b[0m - AssertionError: Predictions [1 1 1 1 1] do not match expected [0 1 0 1 0]\n",
            "\u001b[31m\u001b[31m\u001b[1m1 failed\u001b[0m, \u001b[32m4 passed\u001b[0m\u001b[31m in 1.14s\u001b[0m\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZmcTFE53jmQG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}