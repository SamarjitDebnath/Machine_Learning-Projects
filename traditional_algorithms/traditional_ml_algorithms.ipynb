{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPY1+KB9MaS5U/yxZytnfjp"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "cSo8gIINXE1P"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "lLuSZbSdQ4uQ"
      },
      "outputs": [],
      "source": [
        "from abc import ABC, abstractmethod\n",
        "\n",
        "class SupervisedMachineLearningModel(ABC):\n",
        "    @abstractmethod\n",
        "    def train(self, X, y):\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def predict(self, X):\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Linear Regression\n",
        "\n",
        "#### Data\n",
        "```\n",
        " ----------\n",
        "|  X |  y  |\n",
        " ----------\n",
        "\n",
        "y is continous value\n",
        "```\n",
        "\n",
        "#### 1. Model\n",
        "\n",
        "```\n",
        "y_pred = wX + b\n",
        "\n",
        "where,\n",
        "X = Independent variable\n",
        "y_pred = Dependent variable\n",
        "w = weight or slope\n",
        "b = intercept or bias\n",
        "```\n",
        "\n",
        "#### 2. Loss\n",
        "\n",
        "```\n",
        "J(w) = (1 / (2 * m)) * Σ[(y_pred(i) - y(i))^2]\n",
        "\n",
        "where,\n",
        "m is the number of training examples.\n",
        "y(i) is the actual output for the i-th example.\n",
        "y_pred(i) is the predicted output for the i-th example.\n",
        "```\n",
        "\n",
        "#### 3. Gradient\n",
        "\n",
        "```\n",
        "∂J(w) / ∂wj = (1 / m) * Σ[(y_pred(i) - y(i)) * Xj(i)]\n",
        "\n",
        "Compute the error term (y_pred(i) - y(i)).\n",
        "Multiply the error by the corresponding feature Xj(i).\n",
        "Take the average over all examples.\n",
        "```\n",
        "\n",
        "#### 4. Gradient Descent Update\n",
        "\n",
        "```\n",
        "wj = wj - α * ∂J(w) / ∂wj\n",
        "\n",
        "where,\n",
        "∂J(w) / ∂wj is gradient\n",
        "```\n"
      ],
      "metadata": {
        "id": "Meg5Fe3FR8tf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LinearRegression(SupervisedMachineLearningModel):\n",
        "    def __init__(self, learning_rate: float = 0.01, epochs: int = 1000) -> None:\n",
        "        self.learning_rate = learning_rate\n",
        "        self.epochs = epochs\n",
        "        self.weights = None\n",
        "\n",
        "    def train(self, X, y, verbose=False) -> None:\n",
        "        \"\"\"Train the linear regression model.\"\"\"\n",
        "        m, n = X.shape\n",
        "        # Initialize weights (including bias as part of weights)\n",
        "        self.weights = np.zeros(n + 1)\n",
        "\n",
        "        # Add bias as the first column of X\n",
        "        X = np.c_[np.ones(m), X]\n",
        "\n",
        "        for epoch in range(self.epochs):\n",
        "            # Predictions\n",
        "            y_pred = X @ self.weights\n",
        "\n",
        "            # Error calculation\n",
        "            error = y_pred - y\n",
        "\n",
        "            # Loss\n",
        "            loss = (1 / (2 * m)) * np.sum(error ** 2)\n",
        "\n",
        "            # Gradient calculation\n",
        "            gradient = (1 / m) * (X.T @ error)\n",
        "\n",
        "            # Update weights\n",
        "            self.weights -= self.learning_rate * gradient\n",
        "\n",
        "            if verbose:\n",
        "                # Display\n",
        "                print(f\"iteration: ({epoch+1}/{self.epochs}) - error: {error} - loss: {loss} - gradient - {gradient}\")\n",
        "\n",
        "    def predict(self, X) -> np.ndarray:\n",
        "        \"\"\"Predict using the linear regression model.\"\"\"\n",
        "        m = X.shape[0]\n",
        "        # Add bias as the first column of X\n",
        "        X = np.c_[np.ones(m), X]\n",
        "        return X @ self.weights"
      ],
      "metadata": {
        "id": "sssU-INvR5Sz"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Logistic Regression\n",
        "\n",
        "#### Data\n",
        "\n",
        "```\n",
        " ----------\n",
        "|  X |  y  |\n",
        " ----------\n",
        "\n",
        "y is discrete value\n",
        "```\n",
        "\n",
        "#### 1. Model\n",
        "\n",
        "```\n",
        "y_pred = sigmoid(X @ weights)\n",
        "\n",
        "where,\n",
        "sigmoid = 1/(1+exp(-z))\n",
        "\n",
        "z = w0 + w1*x1 + w2*x2 + ... + wn*xn\n",
        "w0 is bias term\n",
        "```\n",
        "\n",
        "#### 2. Loss\n",
        "\n",
        "```\n",
        "Cost = -(1/m) * Σ [y * log(y_pred) + (1 - y) * log(1 - y_pred)]\n",
        "```\n",
        "\n",
        "#### 3. Gradient Calculation\n",
        "\n",
        "```\n",
        "# Error\n",
        "error = y_pred - y\n",
        "\n",
        "# Gradient\n",
        "gradient = (1 / m) * (X.T @ error)\n",
        "\n",
        "# Weight update\n",
        "weights -= learning_rate * gradient\n",
        "```"
      ],
      "metadata": {
        "id": "Fx9t4yvlcN-j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class LogisticRegression:\n",
        "    def __init__(self, learning_rate: float = 0.01, epochs: int = 1000, threshold: float = 0.5):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.epochs = epochs\n",
        "        self.threshold = threshold\n",
        "        self.weights = None\n",
        "        self.mean = None\n",
        "        self.std = None\n",
        "\n",
        "    @staticmethod\n",
        "    def __sigmoid(z):\n",
        "        \"\"\"Compute the sigmoid function.\"\"\"\n",
        "        return np.clip(1 / (1 + np.exp(-z)), 1e-10, 1 - 1e-10)  # Clipping for numerical stability\n",
        "\n",
        "    def __standardize(self, X):\n",
        "        \"\"\"Standardize features natively (zero mean and unit variance).\"\"\"\n",
        "        if self.mean is None or self.std is None:\n",
        "            # Calculate mean and standard deviation for scaling\n",
        "            self.mean = np.mean(X, axis=0)\n",
        "            self.std = np.std(X, axis=0)\n",
        "        return (X - self.mean) / self.std\n",
        "\n",
        "    def train(self, X, y, verbose=False):\n",
        "        \"\"\"Train the logistic regression model.\"\"\"\n",
        "        m, n = X.shape\n",
        "\n",
        "        # # Standardize the input features\n",
        "        # X = self.__standardize(X)\n",
        "\n",
        "        # Add bias term\n",
        "        X = np.c_[np.ones(m), X]\n",
        "        self.weights = np.zeros(n + 1)  # Initialize weights (including bias)\n",
        "\n",
        "        for epoch in range(self.epochs):\n",
        "            # Compute the linear combination\n",
        "            z = X @ self.weights\n",
        "\n",
        "            # Sigmoid activation\n",
        "            y_pred = self.__sigmoid(z)\n",
        "\n",
        "            # Compute the error\n",
        "            error = y_pred - y\n",
        "\n",
        "            # Compute the loss\n",
        "            loss = -(1 / m) * np.sum(y * np.log(y_pred) + (1 - y) * np.log(1 - y_pred))\n",
        "\n",
        "            # Compute the gradient\n",
        "            gradient = (1 / m) * (X.T @ error)\n",
        "\n",
        "            # Update weights\n",
        "            self.weights -= self.learning_rate * gradient\n",
        "\n",
        "            if verbose:\n",
        "                print(f\"Iteration: ({epoch+1}/{self.epochs}) - Loss: {loss:.6f} - Gradient: {gradient}\")\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Make predictions using the logistic regression model.\"\"\"\n",
        "        # # Standardize the input features using the same mean and std as during training\n",
        "        # X = self.__standardize(X)\n",
        "\n",
        "        # Add bias term\n",
        "        X = np.c_[np.ones(X.shape[0]), X]\n",
        "\n",
        "        # Compute probabilities\n",
        "        probabilities = self.__sigmoid(X @ self.weights)\n",
        "\n",
        "        # print(f\"probabilities: {probabilities}\")\n",
        "\n",
        "        # Apply threshold to get binary predictions\n",
        "        return (probabilities >= self.threshold).astype(int)"
      ],
      "metadata": {
        "id": "PK_FSjaVdrLr"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test"
      ],
      "metadata": {
        "id": "XQdioKdVbfHM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install ipytest --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Mb5rkZkb6PH",
        "outputId": "59da3701-7921-4d3d-94e7-4d73d6ea2252"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.6 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.7/1.6 MB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import ipytest\n",
        "ipytest.autoconfig()\n",
        "\n",
        "\n",
        "def test_linear_regression():\n",
        "    X = np.array([[1], [2], [3], [4], [5], [6], [7]])\n",
        "    y = np.array([3, 5, 7, 9, 11, 13, 15])  # y = 2x + 1\n",
        "    model = LinearRegression(learning_rate=0.01, epochs=1000)\n",
        "    model.train(X[:4], y[:4], verbose=False)\n",
        "    predictions = model.predict(X[5:])\n",
        "    expected = y[5:]\n",
        "    assert np.allclose(predictions, expected, rtol=1e-2, atol=1e-2, equal_nan=False), f\"predictions {predictions} do not match expected {expected}\"\n",
        "\n",
        "\n",
        "def test_logistic_regression():\n",
        "    X = np.array([[1], [2], [3], [4], [5], [6], [7], [8], [9], [10]])\n",
        "    y = np.array([1, 0, 1, 0, 1, 0, 1, 0, 1, 0])\n",
        "    model = LogisticRegression(learning_rate=0.1, epochs=1000)\n",
        "    model.train(X[:5], y[:5], verbose=False)\n",
        "    predictions = model.predict(X[6:])\n",
        "    expected = y[6:]\n",
        "    assert np.allclose(predictions, expected, rtol=1e-2, atol=1e-2, equal_nan=False), f\"predictions {predictions} do not match expected {expected}\"\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    ipytest.run()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EyrS2muYYMAV",
        "outputId": "9c719711-d2bf-47d1-fc23-7f29aef90ccd"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32m.\u001b[0m\u001b[31mF\u001b[0m\u001b[31m                                                                                           [100%]\u001b[0m\n",
            "============================================= FAILURES =============================================\n",
            "\u001b[31m\u001b[1m_____________________________________ test_logistic_regression _____________________________________\u001b[0m\n",
            "\n",
            "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mtest_logistic_regression\u001b[39;49;00m():\u001b[90m\u001b[39;49;00m\n",
            "        X = np.array([[\u001b[94m1\u001b[39;49;00m], [\u001b[94m2\u001b[39;49;00m], [\u001b[94m3\u001b[39;49;00m], [\u001b[94m4\u001b[39;49;00m], [\u001b[94m5\u001b[39;49;00m], [\u001b[94m6\u001b[39;49;00m], [\u001b[94m7\u001b[39;49;00m], [\u001b[94m8\u001b[39;49;00m], [\u001b[94m9\u001b[39;49;00m], [\u001b[94m10\u001b[39;49;00m]])\u001b[90m\u001b[39;49;00m\n",
            "        y = np.array([\u001b[94m1\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "        model = LogisticRegression(learning_rate=\u001b[94m0.1\u001b[39;49;00m, epochs=\u001b[94m1000\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "        model.train(X[:\u001b[94m5\u001b[39;49;00m], y[:\u001b[94m5\u001b[39;49;00m], verbose=\u001b[94mFalse\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "        predictions = model.predict(X[\u001b[94m6\u001b[39;49;00m:])\u001b[90m\u001b[39;49;00m\n",
            "        expected = y[\u001b[94m6\u001b[39;49;00m:]\u001b[90m\u001b[39;49;00m\n",
            ">       \u001b[94massert\u001b[39;49;00m np.allclose(predictions, expected, rtol=\u001b[94m1e-2\u001b[39;49;00m, atol=\u001b[94m1e-2\u001b[39;49;00m, equal_nan=\u001b[94mFalse\u001b[39;49;00m), \u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mpredictions \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mpredictions\u001b[33m}\u001b[39;49;00m\u001b[33m do not match expected \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mexpected\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       AssertionError: predictions [1 1 1 1] do not match expected [1 0 1 0]\u001b[0m\n",
            "\u001b[1m\u001b[31mE       assert False\u001b[0m\n",
            "\u001b[1m\u001b[31mE        +  where False = <function allclose at 0x7f5a841eb330>(array([1, 1, 1, 1]), array([1, 0, 1, 0]), rtol=0.01, atol=0.01, equal_nan=False)\u001b[0m\n",
            "\u001b[1m\u001b[31mE        +    where <function allclose at 0x7f5a841eb330> = np.allclose\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31m<ipython-input-24-b2c760e15603>\u001b[0m:22: AssertionError\n",
            "\u001b[36m\u001b[1m===================================== short test summary info ======================================\u001b[0m\n",
            "\u001b[31mFAILED\u001b[0m t_43da2ff4e20d46f6b145661204d25e61.py::\u001b[1mtest_logistic_regression\u001b[0m - AssertionError: predictions [1 1 1 1] do not match expected [1 0 1 0]\n",
            "\u001b[31m\u001b[31m\u001b[1m1 failed\u001b[0m, \u001b[32m1 passed\u001b[0m\u001b[31m in 0.13s\u001b[0m\u001b[0m\n"
          ]
        }
      ]
    }
  ]
}