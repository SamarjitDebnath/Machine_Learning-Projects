{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNqWzD15fEWsEzCa0gUcNzx"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "cSo8gIINXE1P"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "lLuSZbSdQ4uQ"
      },
      "outputs": [],
      "source": [
        "from abc import ABC, abstractmethod\n",
        "\n",
        "class SupervisedMachineLearningModel(ABC):\n",
        "    @abstractmethod\n",
        "    def train(self, X, y):\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def predict(self, X):\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Linear Regression\n",
        "\n",
        "#### Data\n",
        "```\n",
        " ----------\n",
        "|  X |  y  |\n",
        " ----------\n",
        "```\n",
        "\n",
        "#### 1. Model\n",
        "\n",
        "```\n",
        "y_pred = wX + b\n",
        "\n",
        "where,\n",
        "X = Independent variable\n",
        "y_pred = Dependent variable\n",
        "w = weight or slope\n",
        "b = intercept or bias\n",
        "```\n",
        "\n",
        "#### 2. Loss\n",
        "\n",
        "```\n",
        "J(w) = (1 / (2 * m)) * Σ[(y_pred(i) - y(i))^2]\n",
        "\n",
        "where,\n",
        "m is the number of training examples.\n",
        "y(i) is the actual output for the i-th example.\n",
        "y_pred(i) is the predicted output for the i-th example.\n",
        "```\n",
        "\n",
        "#### 3. Gradient\n",
        "\n",
        "```\n",
        "∂J(w) / ∂wj = (1 / m) * Σ[(y_pred(i) - y(i)) * Xj(i)]\n",
        "\n",
        "Compute the error term (y_pred(i) - y(i)).\n",
        "Multiply the error by the corresponding feature Xj(i).\n",
        "Take the average over all examples.\n",
        "```\n",
        "\n",
        "#### 4. Gradient Descent Update\n",
        "\n",
        "```\n",
        "wj = wj - α * ∂J(w) / ∂wj\n",
        "\n",
        "where,\n",
        "∂J(w) / ∂wj is gradient\n",
        "```\n"
      ],
      "metadata": {
        "id": "Meg5Fe3FR8tf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LinearRegression(SupervisedMachineLearningModel):\n",
        "    def __init__(self, learning_rate: float = 0.01, epochs: int = 1000) -> None:\n",
        "        self.learning_rate = learning_rate\n",
        "        self.epochs = epochs\n",
        "        self.weights = None\n",
        "\n",
        "    def train(self, X, y, verbose=False) -> None:\n",
        "        \"\"\"Train the linear regression model.\"\"\"\n",
        "        m, n = X.shape\n",
        "        # Initialize weights (including bias as part of weights)\n",
        "        self.weights = np.zeros(n + 1)\n",
        "\n",
        "        # Add bias as the first column of X\n",
        "        X = np.c_[np.ones(m), X]\n",
        "\n",
        "        for epoch in range(self.epochs):\n",
        "            # Predictions\n",
        "            y_pred = X @ self.weights\n",
        "\n",
        "            # Error calculation\n",
        "            error = y_pred - y\n",
        "\n",
        "            # Loss\n",
        "            loss = (1 / (2 * m)) * np.sum(error ** 2)\n",
        "\n",
        "            # Gradient calculation\n",
        "            gradient = (1 / m) * (X.T @ error)\n",
        "\n",
        "            # Update weights\n",
        "            self.weights -= self.learning_rate * gradient\n",
        "\n",
        "            if verbose:\n",
        "                # Display\n",
        "                print(f\"iteration: ({epoch+1}/{self.epochs}) - error: {error} - loss: {loss} - gradient - {gradient}\")\n",
        "\n",
        "    def predict(self, X) -> np.ndarray:\n",
        "        \"\"\"Predict using the linear regression model.\"\"\"\n",
        "        m = X.shape[0]\n",
        "        # Add bias as the first column of X\n",
        "        X = np.c_[np.ones(m), X]\n",
        "        return X @ self.weights"
      ],
      "metadata": {
        "id": "sssU-INvR5Sz"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test"
      ],
      "metadata": {
        "id": "XQdioKdVbfHM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install ipytest --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Mb5rkZkb6PH",
        "outputId": "2b35d6cc-fda3-4fc9-f077-9ce4bed933ab"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.6 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.4/1.6 MB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import ipytest\n",
        "ipytest.autoconfig()\n",
        "\n",
        "\n",
        "def test_linear_regression():\n",
        "    X = np.array([[1], [2], [3], [4], [5], [6], [7]])\n",
        "    y = np.array([3, 5, 7, 9, 11, 13, 16])  # y = 2x + 1\n",
        "    model = LinearRegression(learning_rate=0.01, epochs=1000)\n",
        "    model.train(X[:4], y[:4], verbose=True)\n",
        "    predictions = model.predict(X[5:])\n",
        "    expected = y[5:]\n",
        "    assert np.allclose(predictions, expected, rtol=1e-2, atol=1e-2, equal_nan=False), f\"predictions {predictions} do not match expected {expected}\"\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    ipytest.run()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EyrS2muYYMAV",
        "outputId": "8246f005-44fb-4ea9-9683-ce62ad3f9429"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mF\u001b[0m\u001b[31m                                                                                            [100%]\u001b[0m\n",
            "============================================= FAILURES =============================================\n",
            "\u001b[31m\u001b[1m______________________________________ test_linear_regression ______________________________________\u001b[0m\n",
            "\n",
            "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mtest_linear_regression\u001b[39;49;00m():\u001b[90m\u001b[39;49;00m\n",
            "        X = np.array([[\u001b[94m1\u001b[39;49;00m], [\u001b[94m2\u001b[39;49;00m], [\u001b[94m3\u001b[39;49;00m], [\u001b[94m4\u001b[39;49;00m], [\u001b[94m5\u001b[39;49;00m], [\u001b[94m6\u001b[39;49;00m], [\u001b[94m7\u001b[39;49;00m]])\u001b[90m\u001b[39;49;00m\n",
            "        y = np.array([\u001b[94m3\u001b[39;49;00m, \u001b[94m5\u001b[39;49;00m, \u001b[94m7\u001b[39;49;00m, \u001b[94m9\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m, \u001b[94m13\u001b[39;49;00m, \u001b[94m16\u001b[39;49;00m])  \u001b[90m# y = 2x + 1\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        model = LinearRegression(learning_rate=\u001b[94m0.01\u001b[39;49;00m, epochs=\u001b[94m1000\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "        model.train(X[:\u001b[94m4\u001b[39;49;00m], y[:\u001b[94m4\u001b[39;49;00m], verbose=\u001b[94mTrue\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "        predictions = model.predict(X[\u001b[94m5\u001b[39;49;00m:])\u001b[90m\u001b[39;49;00m\n",
            "        expected = y[\u001b[94m5\u001b[39;49;00m:]\u001b[90m\u001b[39;49;00m\n",
            ">       \u001b[94massert\u001b[39;49;00m np.allclose(predictions, expected, rtol=\u001b[94m1e-2\u001b[39;49;00m, atol=\u001b[94m1e-2\u001b[39;49;00m, equal_nan=\u001b[94mFalse\u001b[39;49;00m), \u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mpredictions \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mpredictions\u001b[33m}\u001b[39;49;00m\u001b[33m do not match expected \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mexpected\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       AssertionError: predictions [13.06668253 15.08847507] do not match expected [13 16]\u001b[0m\n",
            "\u001b[1m\u001b[31mE       assert False\u001b[0m\n",
            "\u001b[1m\u001b[31mE        +  where False = <function allclose at 0x7dd58ac03070>(array([13.06668253, 15.08847507]), array([13, 16]), rtol=0.01, atol=0.01, equal_nan=False)\u001b[0m\n",
            "\u001b[1m\u001b[31mE        +    where <function allclose at 0x7dd58ac03070> = np.allclose\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31m<ipython-input-42-f5561f56e679>\u001b[0m:12: AssertionError\n",
            "--------------------------------------- Captured stdout call ---------------------------------------\n",
            "iteration: (1/1000) - error: [-3. -5. -7. -9.] - loss: 20.5 - gradient - [ -6.  -17.5]\n",
            "iteration: (2/1000) - error: [-2.765 -4.59  -6.415 -8.24 ] - loss: 17.22039375 - gradient - [ -5.5025 -16.0375]\n",
            "iteration: (3/1000) - error: [-2.5496   -4.214225 -5.87885  -7.543475] - loss: 14.46563061359375 - gradient - [ -5.0465375 -14.697125 ]\n",
            "iteration: (4/1000) - error: [-2.35216337 -3.86981712 -5.38747087 -6.90512463] - loss: 12.151718204923665 - gradient - [ -4.628644   -13.46867719]\n",
            "iteration: (5/1000) - error: [-2.17119016 -3.55415714 -4.93712412 -6.3200911 ] - loss: 10.208105719995284 - gradient - [ -4.24564063 -12.3428103 ]\n",
            "iteration: (6/1000) - error: [-2.00530565 -3.26484453 -4.5243834  -5.78392228] - loss: 8.575532835461146 - gradient - [ -3.89461397 -11.31095851]\n",
            "iteration: (7/1000) - error: [-1.85324993 -2.99967922 -4.14610851 -5.2925378 ] - loss: 7.204222855313088 - gradient - [ -3.57289386 -10.36527127]\n",
            "iteration: (8/1000) - error: [-1.71386828 -2.75664486 -3.79942143 -4.84219801] - loss: 6.052365014340989 - gradient - [-3.27803314 -9.49855358]\n",
            "iteration: (9/1000) - error: [-1.58610241 -2.53389345 -3.48168449 -4.42947553] - loss: 5.084839663635403 - gradient - [-3.00778897 -8.70421123]\n",
            "iteration: (10/1000) - error: [-1.46898241 -2.32973134 -3.19048027 -4.0512292 ] - loss: 4.272147468909776 - gradient - [-2.7601058  -7.97620067]\n",
            "iteration: (11/1000) - error: [-1.36161934 -2.14260627 -2.92359319 -3.70458011] - loss: 3.5895099727897106 - gradient - [-2.53309973 -7.30898297]\n",
            "iteration: (12/1000) - error: [-1.26319852 -1.97109561 -2.6789927  -3.3868898 ] - loss: 3.016114097121573 - gradient - [-2.32504416 -6.69748176]\n",
            "iteration: (13/1000) - error: [-1.17297326 -1.81389553 -2.45481781 -3.09574008] - loss: 2.534477550098667 - gradient - [-2.13435667 -6.13704452]\n",
            "iteration: (14/1000) - error: [-1.09025925 -1.66981108 -2.24936291 -2.82891474] - loss: 2.12991578940339 - gradient - [-1.95958699 -5.62340726]\n",
            "iteration: (15/1000) - error: [-1.0144293  -1.53774706 -2.06106482 -2.58438258] - loss: 1.7900942890171403 - gradient - [-1.79940594 -5.15266204]\n",
            "iteration: (16/1000) - error: [-0.94490862 -1.41669976 -1.8884909  -2.36028203] - loss: 1.5046524582672471 - gradient - [-1.65259533 -4.72122724]\n",
            "iteration: (17/1000) - error: [-0.8811704  -1.30574926 -1.73032813 -2.15490699] - loss: 1.264887746364266 - gradient - [-1.51803869 -4.32582032]\n",
            "iteration: (18/1000) - error: [-0.82273181 -1.20405247 -1.58537313 -1.96669379] - loss: 1.0634903007446768 - gradient - [-1.3947128  -3.96343283]\n",
            "iteration: (19/1000) - error: [-0.76915035 -1.11083668 -1.45252302 -1.79420935] - loss: 0.8943200889247793 - gradient - [-1.28167985 -3.63130754]\n",
            "iteration: (20/1000) - error: [-0.72002048 -1.02539373 -1.33076699 -1.63614025] - loss: 0.7522196882883032 - gradient - [-1.17808036 -3.32691748]\n",
            "iteration: (21/1000) - error: [-0.6749705  -0.94707458 -1.21917866 -1.49128275] - loss: 0.6328570357492644 - gradient - [-1.08312662 -3.04794666]\n",
            "iteration: (22/1000) - error: [-0.63365977 -0.87528438 -1.116909   -1.35853361] - loss: 0.5325933427112017 - gradient - [-0.99609669 -2.7922725 ]\n",
            "iteration: (23/1000) - error: [-0.59577607 -0.80947797 -1.02317986 -1.23688175] - loss: 0.4483721480364129 - gradient - [-0.91632891 -2.55794964]\n",
            "iteration: (24/1000) - error: [-0.56103329 -0.74915568 -0.93727808 -1.12540047] - loss: 0.37762612623888725 - gradient - [-0.84321688 -2.3431952 ]\n",
            "iteration: (25/1000) - error: [-0.52916917 -0.69385961 -0.85855005 -1.0232405 ] - loss: 0.31819880947321677 - gradient - [-0.77620483 -2.14637513]\n",
            "iteration: (26/1000) - error: [-0.49994337 -0.64317006 -0.78639675 -0.92962344] - loss: 0.2682788366149242 - gradient - [-0.71478341 -1.96599188]\n",
            "iteration: (27/1000) - error: [-0.47313562 -0.59670239 -0.72026916 -0.84383593] - loss: 0.2263447246799628 - gradient - [-0.65848577 -1.8006729 ]\n",
            "iteration: (28/1000) - error: [-0.44854403 -0.55410407 -0.65966412 -0.76522416] - loss: 0.1911184786583127 - gradient - [-0.60688409 -1.64916029]\n",
            "iteration: (29/1000) - error: [-0.42598358 -0.51505203 -0.60412047 -0.69318891] - loss: 0.16152662532076512 - gradient - [-0.55958625 -1.51030117]\n",
            "iteration: (30/1000) - error: [-0.40528471 -0.47925014 -0.55321557 -0.627181  ] - loss: 0.13666748291568737 - gradient - [-0.51623285 -1.38303892]\n",
            "iteration: (31/1000) - error: [-0.38629199 -0.44642703 -0.50656207 -0.56669711] - loss: 0.11578366880547135 - gradient - [-0.47649455 -1.26640518]\n",
            "iteration: (32/1000) - error: [-0.368863   -0.41633398 -0.46380497 -0.51127596] - loss: 0.09823900679769834 - gradient - [-0.44006948 -1.15951243]\n",
            "iteration: (33/1000) - error: [-0.35286718 -0.38874304 -0.4246189  -0.46049477] - loss: 0.0834991300731816 - gradient - [-0.40668097 -1.06154726]\n",
            "iteration: (34/1000) - error: [-0.33818489 -0.36344529 -0.38870568 -0.41396607] - loss: 0.07111518829219059 - gradient - [-0.37607548 -0.97176419]\n",
            "iteration: (35/1000) - error: [-0.3247065  -0.34024925 -0.355792   -0.37133475] - loss: 0.060710162106849305 - gradient - [-0.34802062 -0.88947999]\n",
            "iteration: (36/1000) - error: [-0.31233149 -0.31897944 -0.32562739 -0.33227534] - loss: 0.051967367807803415 - gradient - [-0.32230342 -0.81406848]\n",
            "iteration: (37/1000) - error: [-0.30096777 -0.29947504 -0.2979823  -0.29648957] - loss: 0.04462080161065449 - gradient - [-0.29872867 -0.74495575]\n",
            "iteration: (38/1000) - error: [-0.29053093 -0.28158864 -0.27264634 -0.26370405] - loss: 0.03844702917846779 - gradient - [-0.27711749 -0.68161586]\n",
            "iteration: (39/1000) - error: [-0.28094359 -0.26518514 -0.24942669 -0.23366824] - loss: 0.03325837309106715 - gradient - [-0.25730592 -0.62356673]\n",
            "iteration: (40/1000) - error: [-0.27213487 -0.25014075 -0.22814663 -0.20615251] - loss: 0.028897190546360876 - gradient - [-0.23914369 -0.57036658]\n",
            "iteration: (41/1000) - error: [-0.26403977 -0.23634198 -0.2086442  -0.18094641] - loss: 0.025231066820225974 - gradient - [-0.22249309 -0.52161049]\n",
            "iteration: (42/1000) - error: [-0.25659873 -0.22368484 -0.19077095 -0.15785706] - loss: 0.022148777933051464 - gradient - [-0.2072279  -0.47692738]\n",
            "iteration: (43/1000) - error: [-0.24975718 -0.21207401 -0.17439085 -0.13670769] - loss: 0.019556899424233158 - gradient - [-0.19323243 -0.43597713]\n",
            "iteration: (44/1000) - error: [-0.24346508 -0.20142215 -0.15937921 -0.11733628] - loss: 0.017376957835804882 - gradient - [-0.18040068 -0.39844803]\n",
            "iteration: (45/1000) - error: [-0.23767659 -0.19164918 -0.14562176 -0.09959435] - loss: 0.015543038053654175 - gradient - [-0.16863547 -0.36405441]\n",
            "iteration: (46/1000) - error: [-0.2323497  -0.18268174 -0.13301378 -0.08334582] - loss: 0.013999773553909634 - gradient - [-0.15784776 -0.33253444]\n",
            "iteration: (47/1000) - error: [-0.22744587 -0.17445257 -0.12145927 -0.06846596] - loss: 0.012700658276907925 - gradient - [-0.14795592 -0.30364817]\n",
            "iteration: (48/1000) - error: [-0.22292983 -0.16690005 -0.11087026 -0.05484048] - loss: 0.011606628657603944 - gradient - [-0.13888516 -0.27717566]\n",
            "iteration: (49/1000) - error: [-0.21876922 -0.15996768 -0.10116614 -0.0423646 ] - loss: 0.010684872578389171 - gradient - [-0.13056691 -0.25291535]\n",
            "iteration: (50/1000) - error: [-0.2149344  -0.15360371 -0.09227301 -0.03094232] - loss: 0.009907828929164933 - gradient - [-0.12293836 -0.23068253]\n",
            "iteration: (51/1000) - error: [-0.21139819 -0.14776067 -0.08412315 -0.02048563] - loss: 0.009252347271157474 - gradient - [-0.11594191 -0.21030788]\n",
            "iteration: (52/1000) - error: [-0.2081357 -0.1423951 -0.0766545 -0.0109139] - loss: 0.008698981982537003 - gradient - [-0.1095248  -0.19163624]\n",
            "iteration: (53/1000) - error: [-0.20512408 -0.13746712 -0.06981016 -0.0021532 ] - loss: 0.008231399364269932 - gradient - [-0.10363864 -0.1745254 ]\n",
            "iteration: (54/1000) - error: [-0.20234244 -0.13294023 -0.06353801  0.0058642 ] - loss: 0.007835879628800077 - gradient - [-0.09823912 -0.15884503]\n",
            "iteration: (55/1000) - error: [-0.1997716  -0.12878094 -0.05779027  0.0132004 ] - loss: 0.007500898587143967 - gradient - [-0.0932856  -0.14447568]\n",
            "iteration: (56/1000) - error: [-0.19739399 -0.12495857 -0.05252314  0.01991228] - loss: 0.007216776279995917 - gradient - [-0.08874086 -0.13130786]\n",
            "iteration: (57/1000) - error: [-0.1951935 -0.121445  -0.0476965  0.026052 ] - loss: 0.006975381839570554 - gradient - [-0.08457075 -0.11924125]\n",
            "iteration: (58/1000) - error: [-0.19315538 -0.11821447 -0.04327355  0.03166736] - loss: 0.00676988558338935 - gradient - [-0.08074401 -0.10818389]\n",
            "iteration: (59/1000) - error: [-0.1912661  -0.11524335 -0.0392206   0.03680215] - loss: 0.006594550781326067 - gradient - [-0.07723197 -0.0980515 ]\n",
            "iteration: (60/1000) - error: [-0.18951327 -0.11251    -0.03550673  0.04149653] - loss: 0.006444558746866633 - gradient - [-0.07400837 -0.08876683]\n",
            "iteration: (61/1000) - error: [-0.18788552 -0.10999458 -0.03210364  0.04578729] - loss: 0.006315861919598508 - gradient - [-0.07104911 -0.08025911]\n",
            "iteration: (62/1000) - error: [-0.18637243 -0.10767891 -0.02898538  0.04970815] - loss: 0.006205060459399509 - gradient - [-0.06833214 -0.07246345]\n",
            "iteration: (63/1000) - error: [-0.18496448 -0.10554632 -0.02612816  0.05329001] - loss: 0.0061092985896700775 - gradient - [-0.06583724 -0.06532039]\n",
            "iteration: (64/1000) - error: [-0.1836529  -0.10358154 -0.02351017  0.05656119] - loss: 0.006026177529104318 - gradient - [-0.06354585 -0.05877543]\n",
            "iteration: (65/1000) - error: [-0.18242969 -0.10177057 -0.02111145  0.05954767] - loss: 0.00595368235727965 - gradient - [-0.06144101 -0.05277863]\n",
            "iteration: (66/1000) - error: [-0.18128749 -0.10010059 -0.01891368  0.06227322] - loss: 0.005890120584190631 - gradient - [-0.05950713 -0.0472842 ]\n",
            "iteration: (67/1000) - error: [-0.18021958 -0.09855983 -0.01690008  0.06475966] - loss: 0.0058340705507046176 - gradient - [-0.05772996 -0.04225021]\n",
            "iteration: (68/1000) - error: [-0.17921978 -0.09713753 -0.01505528  0.06702697] - loss: 0.00578433808666487 - gradient - [-0.0560964  -0.03763819]\n",
            "iteration: (69/1000) - error: [-0.17828243 -0.0958238  -0.01336517  0.06909346] - loss: 0.005739920105141694 - gradient - [-0.05459448 -0.03341292]\n",
            "iteration: (70/1000) - error: [-0.17740236 -0.0946096  -0.01181684  0.07097593] - loss: 0.005699974022817019 - gradient - [-0.05321322 -0.02954209]\n",
            "iteration: (71/1000) - error: [-0.17657481 -0.09348662 -0.01039844  0.07268974] - loss: 0.005663792074126792 - gradient - [-0.05194253 -0.0259961 ]\n",
            "iteration: (72/1000) - error: [-0.17579542 -0.09244728 -0.00909913  0.07424901] - loss: 0.005630779735996214 - gradient - [-0.0507732  -0.02274783]\n",
            "iteration: (73/1000) - error: [-0.17506021 -0.09148459 -0.00790897  0.07566666] - loss: 0.005600437605337156 - gradient - [-0.04969678 -0.01977241]\n",
            "iteration: (74/1000) - error: [-0.17436552 -0.09059217 -0.00681883  0.07695452] - loss: 0.005572346176749256 - gradient - [-0.0487055  -0.01704706]\n",
            "iteration: (75/1000) - error: [-0.17370799 -0.08976417 -0.00582036  0.07812346] - loss: 0.005546153056296511 - gradient - [-0.04779227 -0.0145509 ]\n",
            "iteration: (76/1000) - error: [-0.17308456 -0.08899523 -0.00490591  0.07918342] - loss: 0.005521562221505525 - gradient - [-0.04695057 -0.01226477]\n",
            "iteration: (77/1000) - error: [-0.17249241 -0.08828043 -0.00406846  0.08014351] - loss: 0.005498325000122002 - gradient - [-0.04617445 -0.01017115]\n",
            "iteration: (78/1000) - error: [-0.17192895 -0.08761527 -0.00330158  0.0810121 ] - loss: 0.005476232492566577 - gradient - [-0.04545842 -0.00825395]\n",
            "iteration: (79/1000) - error: [-0.17139183 -0.0869956  -0.00259938  0.08179685] - loss: 0.005455109207050244 - gradient - [-0.04479749 -0.00649845]\n",
            "iteration: (80/1000) - error: [-0.17087887 -0.08641766 -0.00195645  0.08250476] - loss: 0.005434807713283268 - gradient - [-0.04418705 -0.00489112]\n",
            "iteration: (81/1000) - error: [-0.17038809 -0.08587797 -0.00136785  0.08314227] - loss: 0.005415204151768691 - gradient - [-0.04362291 -0.00341961]\n",
            "iteration: (82/1000) - error: [-0.16991766 -0.08537334 -0.00082903  0.08371529] - loss: 0.005396194461758423 - gradient - [-0.04310119 -0.00207257]\n",
            "iteration: (83/1000) - error: [-0.16946592 -0.08490088 -0.00033584  0.0842292 ] - loss: 0.005377691212861842 - gradient - [-0.04261836 -0.0008396 ]\n",
            "iteration: (84/1000) - error: [-1.69031343e-01 -8.44579052e-02  1.15532358e-04  8.46889699e-02] - loss: 0.0053596209437025335 - gradient - [-0.04217119  0.00028883]\n",
            "iteration: (85/1000) - error: [-0.16861252 -0.08404197  0.00052858  0.08509913] - loss: 0.005341921926478318 - gradient - [-0.0417567   0.00132145]\n",
            "iteration: (86/1000) - error: [-0.16820817 -0.08365083  0.0009065   0.08546384] - loss: 0.005324542289266208 - gradient - [-0.04137216  0.00226626]\n",
            "iteration: (87/1000) - error: [-0.16781711 -0.08328244  0.00125224  0.08578691] - loss: 0.005307438438820784 - gradient - [-0.0410151   0.00313059]\n",
            "iteration: (88/1000) - error: [-0.16743826 -0.0829349   0.00156847  0.08607184] - loss: 0.0052905737357773 - gradient - [-0.04068321  0.00392117]\n",
            "iteration: (89/1000) - error: [-0.16707064 -0.08260649  0.00185767  0.08632182] - loss: 0.005273917381866101 - gradient - [-0.04037441  0.00464417]\n",
            "iteration: (90/1000) - error: [-0.16671334 -0.08229563  0.00212209  0.0865398 ] - loss: 0.00525744348521008 - gradient - [-0.04008677  0.00530521]\n",
            "iteration: (91/1000) - error: [-0.16636552 -0.08200086  0.0023638   0.08672846] - loss: 0.00524113027520494 - gradient - [-0.03981853  0.00590949]\n",
            "iteration: (92/1000) - error: [-0.16602643 -0.08172087  0.0025847   0.08689026] - loss: 0.005224959443044732 - gradient - [-0.03956809  0.00646174]\n",
            "iteration: (93/1000) - error: [-0.16569537 -0.08145442  0.00278653  0.08702747] - loss: 0.005208915587785523 - gradient - [-0.03933395  0.00696632]\n",
            "iteration: (94/1000) - error: [-0.16537169 -0.08120041  0.00297088  0.08714216] - loss: 0.005192985751056575 - gradient - [-0.03911477  0.00742719]\n",
            "iteration: (95/1000) - error: [-0.16505482 -0.08095781  0.00313921  0.08723622] - loss: 0.0051771590262334605 - gradient - [-0.0389093   0.00784802]\n",
            "iteration: (96/1000) - error: [-0.16474421 -0.08072567  0.00329286  0.08731139] - loss: 0.0051614262301559 - gradient - [-0.03871641  0.00823215]\n",
            "iteration: (97/1000) - error: [-0.16443936 -0.08050315  0.00343306  0.08736927] - loss: 0.005145779627381962 - gradient - [-0.03853505  0.00858265]\n",
            "iteration: (98/1000) - error: [-0.16413984 -0.08028945  0.00356093  0.08741132] - loss: 0.0051302126985702644 - gradient - [-0.03836426  0.00890233]\n",
            "iteration: (99/1000) - error: [-0.16384522 -0.08008386  0.0036775   0.08743887] - loss: 0.005114719945928698 - gradient - [-0.03820318  0.00919376]\n",
            "iteration: (100/1000) - error: [-0.16355513 -0.0798857   0.00378372  0.08745315] - loss: 0.0050992967297974765 - gradient - [-0.03805099  0.00945931]\n",
            "iteration: (101/1000) - error: [-0.16326921 -0.07969438  0.00388045  0.08745528] - loss: 0.0050839391313844615 - gradient - [-0.03790696  0.00970113]\n",
            "iteration: (102/1000) - error: [-0.16298715 -0.07950933  0.00396849  0.08744631] - loss: 0.005068643837466837 - gradient - [-0.03777042  0.00992122]\n",
            "iteration: (103/1000) - error: [-0.16270866 -0.07933005  0.00404856  0.08742716] - loss: 0.005053408043544393 - gradient - [-0.03764075  0.01012139]\n",
            "iteration: (104/1000) - error: [-0.16243346 -0.07915607  0.00412132  0.08739872] - loss: 0.005038229372491425 - gradient - [-0.03751737  0.01030331]\n",
            "iteration: (105/1000) - error: [-0.16216132 -0.07898696  0.0041874   0.08736176] - loss: 0.005023105806226549 - gradient - [-0.03739978  0.01046849]\n",
            "iteration: (106/1000) - error: [-0.16189201 -0.07882234  0.00424734  0.08731702] - loss: 0.0050080356283175535 - gradient - [-0.0372875   0.01061835]\n",
            "iteration: (107/1000) - error: [-0.16162532 -0.07866183  0.00430166  0.08726516] - loss: 0.004993017375771407 - gradient - [-0.03718008  0.01075416]\n",
            "iteration: (108/1000) - error: [-0.16136106 -0.07850511  0.00435084  0.08720679] - loss: 0.00497804979853903 - gradient - [-0.03707713  0.0108771 ]\n",
            "iteration: (109/1000) - error: [-0.16109906 -0.07835188  0.0043953   0.08714248] - loss: 0.0049631318255003685 - gradient - [-0.03697829  0.01098825]\n",
            "iteration: (110/1000) - error: [-0.16083916 -0.07820186  0.00443543  0.08707273] - loss: 0.004948262535893297 - gradient - [-0.03688321  0.01108859]\n",
            "iteration: (111/1000) - error: [-0.16058121 -0.0780548   0.00447161  0.08699802] - loss: 0.004933441135313954 - gradient - [-0.0367916   0.01117902]\n",
            "iteration: (112/1000) - error: [-0.16032509 -0.07791047  0.00450415  0.08691878] - loss: 0.004918666935558485 - gradient - [-0.03670316  0.01126039]\n",
            "iteration: (113/1000) - error: [-0.16007066 -0.07776864  0.00453337  0.08683539] - loss: 0.0049039393376895364 - gradient - [-0.03661763  0.01133344]\n",
            "iteration: (114/1000) - error: [-0.15981782 -0.07762914  0.00455955  0.08674823] - loss: 0.0048892578178136486 - gradient - [-0.03653479  0.01139887]\n",
            "iteration: (115/1000) - error: [-0.15956646 -0.07749176  0.00458293  0.08665762] - loss: 0.004874621915133502 - gradient - [-0.03645442  0.01145732]\n",
            "iteration: (116/1000) - error: [-0.15931649 -0.07735637  0.00460375  0.08656387] - loss: 0.004860031221912909 - gradient - [-0.03637631  0.01150938]\n",
            "iteration: (117/1000) - error: [-0.15906782 -0.07722279  0.00462224  0.08646726] - loss: 0.0048454853750467385 - gradient - [-0.03630028  0.01155559]\n",
            "iteration: (118/1000) - error: [-0.15882037 -0.0770909   0.00463857  0.08636804] - loss: 0.004830984048980475 - gradient - [-0.03622617  0.01159643]\n",
            "iteration: (119/1000) - error: [-0.15857407 -0.07696057  0.00465294  0.08626645] - loss: 0.004816526949762108 - gradient - [-0.03615381  0.01163235]\n",
            "iteration: (120/1000) - error: [-0.15832886 -0.07683168  0.00466551  0.08616269] - loss: 0.004802113810046247 - gradient - [-0.03608308  0.01166377]\n",
            "iteration: (121/1000) - error: [-0.15808467 -0.07670412  0.00467642  0.08605697] - loss: 0.004787744384897153 - gradient - [-0.03601385  0.01169106]\n",
            "iteration: (122/1000) - error: [-0.15784144 -0.0765778   0.00468583  0.08594947] - loss: 0.004773418448263394 - gradient - [-0.03594599  0.01171458]\n",
            "iteration: (123/1000) - error: [-0.15759912 -0.07645264  0.00469385  0.08584034] - loss: 0.0047591357900165715 - gradient - [-0.03587939  0.01173463]\n",
            "iteration: (124/1000) - error: [-0.15735768 -0.07632853  0.00470061  0.08572975] - loss: 0.004744896213463695 - gradient - [-0.03581396  0.01175152]\n",
            "iteration: (125/1000) - error: [-0.15711705 -0.07620542  0.0047062   0.08561783] - loss: 0.004730699533257349 - gradient - [-0.03574961  0.01176551]\n",
            "iteration: (126/1000) - error: [-0.15687721 -0.07608324  0.00471073  0.08550471] - loss: 0.004716545573640452 - gradient - [-0.03568625  0.01177683]\n",
            "iteration: (127/1000) - error: [-0.15663812 -0.07596191  0.00471429  0.0853905 ] - loss: 0.0047024341669713995 - gradient - [-0.03562381  0.01178573]\n",
            "iteration: (128/1000) - error: [-0.15639974 -0.07584139  0.00471696  0.0852753 ] - loss: 0.004688365152485141 - gradient - [-0.03556222  0.01179239]\n",
            "iteration: (129/1000) - error: [-0.15616204 -0.07572162  0.00471881  0.08515923] - loss: 0.0046743383752525375 - gradient - [-0.0355014   0.01179702]\n",
            "iteration: (130/1000) - error: [-0.15592499 -0.07560254  0.00471991  0.08504236] - loss: 0.004660353685305804 - gradient - [-0.03544132  0.01179978]\n",
            "iteration: (131/1000) - error: [-0.15568858 -0.07548412  0.00472033  0.08492479] - loss: 0.0046464109369039695 - gradient - [-0.0353819   0.01180083]\n",
            "iteration: (132/1000) - error: [-0.15545277 -0.07536632  0.00472013  0.08480657] - loss: 0.004632509987915471 - gradient - [-0.0353231   0.01180031]\n",
            "iteration: (133/1000) - error: [-0.15521754 -0.0752491   0.00471935  0.08468779] - loss: 0.004618650699299601 - gradient - [-0.03526488  0.01179837]\n",
            "iteration: (134/1000) - error: [-0.15498288 -0.07513242  0.00471804  0.0845685 ] - loss: 0.004604832934670643 - gradient - [-0.03520719  0.01179511]\n",
            "iteration: (135/1000) - error: [-0.15474875 -0.07501625  0.00471626  0.08444877] - loss: 0.004591056559931656 - gradient - [-0.03514999  0.01179066]\n",
            "iteration: (136/1000) - error: [-0.15451516 -0.07490056  0.00471404  0.08432865] - loss: 0.004577321442966486 - gradient - [-0.03509326  0.01178511]\n",
            "iteration: (137/1000) - error: [-0.15428208 -0.07478533  0.00471142  0.08420817] - loss: 0.004563627453381229 - gradient - [-0.03503695  0.01177856]\n",
            "iteration: (138/1000) - error: [-0.1540495  -0.07467053  0.00470844  0.0840874 ] - loss: 0.004549974462286638 - gradient - [-0.03498105  0.01177109]\n",
            "iteration: (139/1000) - error: [-0.1538174  -0.07455614  0.00470511  0.08396637] - loss: 0.004536362342115225 - gradient - [-0.03492551  0.01176278]\n",
            "iteration: (140/1000) - error: [-0.15358577 -0.07444214  0.00470148  0.08384511] - loss: 0.004522790966467657 - gradient - [-0.03487033  0.01175371]\n",
            "iteration: (141/1000) - error: [-0.1533546  -0.07432851  0.00469758  0.08372367] - loss: 0.004509260209983032 - gradient - [-0.03481547  0.01174394]\n",
            "iteration: (142/1000) - error: [-0.15312389 -0.07421524  0.00469341  0.08360206] - loss: 0.004495769948230536 - gradient - [-0.03476091  0.01173353]\n",
            "iteration: (143/1000) - error: [-0.15289361 -0.0741023   0.00468902  0.08348033] - loss: 0.004482320057617508 - gradient - [-0.03470664  0.01172254]\n",
            "iteration: (144/1000) - error: [-0.15266377 -0.07398968  0.00468441  0.0833585 ] - loss: 0.004468910415312535 - gradient - [-0.03465264  0.01171102]\n",
            "iteration: (145/1000) - error: [-0.15243436 -0.07387738  0.0046796   0.08323658] - loss: 0.004455540899180505 - gradient - [-0.03459889  0.01169901]\n",
            "iteration: (146/1000) - error: [-0.15220536 -0.07376537  0.00467462  0.08311461] - loss: 0.004442211387727853 - gradient - [-0.03454537  0.01168655]\n",
            "iteration: (147/1000) - error: [-0.15197677 -0.07365365  0.00466948  0.0829926 ] - loss: 0.004428921760056424 - gradient - [-0.03449208  0.0116737 ]\n",
            "iteration: (148/1000) - error: [-0.15174859 -0.0735422   0.00466419  0.08287058] - loss: 0.0044156718958247715 - gradient - [-0.03443901  0.01166047]\n",
            "iteration: (149/1000) - error: [-0.1515208  -0.07343102  0.00465876  0.08274855] - loss: 0.004402461675214884 - gradient - [-0.03438613  0.01164691]\n",
            "iteration: (150/1000) - error: [-0.15129341 -0.0733201   0.00465322  0.08262653] - loss: 0.004389290978904982 - gradient - [-0.03433344  0.01163305]\n",
            "iteration: (151/1000) - error: [-0.1510664  -0.07320942  0.00464756  0.08250454] - loss: 0.004376159688045478 - gradient - [-0.03428093  0.0116189 ]\n",
            "iteration: (152/1000) - error: [-0.15083978 -0.07309899  0.0046418   0.0823826 ] - loss: 0.004363067684239565 - gradient - [-0.03422859  0.01160451]\n",
            "iteration: (153/1000) - error: [-0.15061354 -0.07298879  0.00463595  0.0822607 ] - loss: 0.00435001484952594 - gradient - [-0.03417642  0.01158989]\n",
            "iteration: (154/1000) - error: [-0.15038768 -0.07287883  0.00463002  0.08213887] - loss: 0.0043370010663647506 - gradient - [-0.0341244   0.01157505]\n",
            "iteration: (155/1000) - error: [-0.15016218 -0.07276909  0.00462401  0.08201711] - loss: 0.004324026217625213 - gradient - [-0.03407254  0.01156004]\n",
            "iteration: (156/1000) - error: [-0.14993706 -0.07265956  0.00461794  0.08189544] - loss: 0.004311090186575295 - gradient - [-0.03402081  0.01154485]\n",
            "iteration: (157/1000) - error: [-0.1497123  -0.07255025  0.0046118   0.08177385] - loss: 0.00429819285687268 - gradient - [-0.03396922  0.0115295 ]\n",
            "iteration: (158/1000) - error: [-0.1494879  -0.07244115  0.00460561  0.08165236] - loss: 0.0042853341125572006 - gradient - [-0.03391777  0.01151402]\n",
            "iteration: (159/1000) - error: [-0.14926386 -0.07233225  0.00459937  0.08153098] - loss: 0.004272513838044295 - gradient - [-0.03386644  0.01149841]\n",
            "iteration: (160/1000) - error: [-0.14904018 -0.07222355  0.00459308  0.08140971] - loss: 0.004259731918119127 - gradient - [-0.03381524  0.01148269]\n",
            "iteration: (161/1000) - error: [-0.14881686 -0.07211506  0.00458675  0.08128855] - loss: 0.004246988237931741 - gradient - [-0.03376415  0.01146687]\n",
            "iteration: (162/1000) - error: [-0.14859389 -0.07200675  0.00458038  0.08116752] - loss: 0.004234282682992814 - gradient - [-0.03371318  0.01145096]\n",
            "iteration: (163/1000) - error: [-0.14837126 -0.07189864  0.00457399  0.08104661] - loss: 0.004221615139169691 - gradient - [-0.03366233  0.01143497]\n",
            "iteration: (164/1000) - error: [-0.14814899 -0.07179071  0.00456756  0.08092584] - loss: 0.004208985492683199 - gradient - [-0.03361158  0.0114189 ]\n",
            "iteration: (165/1000) - error: [-0.14792706 -0.07168298  0.00456111  0.0808052 ] - loss: 0.004196393630104537 - gradient - [-0.03356093  0.01140278]\n",
            "iteration: (166/1000) - error: [-0.14770548 -0.07157542  0.00455464  0.0806847 ] - loss: 0.0041838394383527265 - gradient - [-0.03351039  0.01138659]\n",
            "iteration: (167/1000) - error: [-0.14748424 -0.07146805  0.00454814  0.08056434] - loss: 0.004171322804692233 - gradient - [-0.03345995  0.01137036]\n",
            "iteration: (168/1000) - error: [-0.14726335 -0.07136086  0.00454163  0.08044412] - loss: 0.004158843616730621 - gradient - [-0.03340961  0.01135408]\n",
            "iteration: (169/1000) - error: [-0.14704279 -0.07125384  0.00453511  0.08032405] - loss: 0.0041464017624167635 - gradient - [-0.03335937  0.01133776]\n",
            "iteration: (170/1000) - error: [-0.14682258 -0.07114701  0.00452857  0.08020414] - loss: 0.004133997130039032 - gradient - [-0.03330922  0.01132141]\n",
            "iteration: (171/1000) - error: [-0.1466027  -0.07104034  0.00452202  0.08008437] - loss: 0.004121629608223365 - gradient - [-0.03325916  0.01130504]\n",
            "iteration: (172/1000) - error: [-0.14638316 -0.07093385  0.00451546  0.07996476] - loss: 0.004109299085931979 - gradient - [-0.0332092   0.01128864]\n",
            "iteration: (173/1000) - error: [-0.14616395 -0.07082753  0.00450889  0.07984531] - loss: 0.004097005452461733 - gradient - [-0.03315932  0.01127222]\n",
            "iteration: (174/1000) - error: [-0.14594508 -0.07072138  0.00450232  0.07972601] - loss: 0.004084748597442816 - gradient - [-0.03310953  0.01125579]\n",
            "iteration: (175/1000) - error: [-0.14572654 -0.0706154   0.00449574  0.07960688] - loss: 0.00407252841083726 - gradient - [-0.03305983  0.01123934]\n",
            "iteration: (176/1000) - error: [-0.14550834 -0.07050959  0.00448916  0.0794879 ] - loss: 0.0040603447829378695 - gradient - [-0.03301022  0.01122289]\n",
            "iteration: (177/1000) - error: [-0.14529047 -0.07040395  0.00448257  0.07936909] - loss: 0.004048197604366824 - gradient - [-0.03296069  0.01120643]\n",
            "iteration: (178/1000) - error: [-0.14507292 -0.07029847  0.00447598  0.07925044] - loss: 0.004036086766074629 - gradient - [-0.03291124  0.01118996]\n",
            "iteration: (179/1000) - error: [-0.14485571 -0.07019316  0.0044694   0.07913195] - loss: 0.004024012159338807 - gradient - [-0.03286188  0.0111735 ]\n",
            "iteration: (180/1000) - error: [-0.14463883 -0.07008801  0.00446281  0.07901363] - loss: 0.00401197367576301 - gradient - [-0.0328126   0.01115703]\n",
            "iteration: (181/1000) - error: [-0.14442227 -0.06998302  0.00445623  0.07889548] - loss: 0.003999971207275655 - gradient - [-0.0327634   0.01114057]\n",
            "iteration: (182/1000) - error: [-0.14420604 -0.0698782   0.00444964  0.07877749] - loss: 0.0039880046461291075 - gradient - [-0.03271428  0.01112411]\n",
            "iteration: (183/1000) - error: [-0.14399014 -0.06977354  0.00444306  0.07865967] - loss: 0.003976073884898427 - gradient - [-0.03266524  0.01110766]\n",
            "iteration: (184/1000) - error: [-0.14377456 -0.06966904  0.00443649  0.07854201] - loss: 0.0039641788164804835 - gradient - [-0.03261628  0.01109122]\n",
            "iteration: (185/1000) - error: [-0.14355931 -0.0695647   0.00442991  0.07842453] - loss: 0.00395231933409281 - gradient - [-0.03256739  0.01107478]\n",
            "iteration: (186/1000) - error: [-0.14334439 -0.06946052  0.00442334  0.07830721] - loss: 0.003940495331272713 - gradient - [-0.03251859  0.01105836]\n",
            "iteration: (187/1000) - error: [-0.14312979 -0.0693565   0.00441678  0.07819006] - loss: 0.003928706701876184 - gradient - [-0.03246986  0.01104195]\n",
            "iteration: (188/1000) - error: [-0.14291551 -0.06925264  0.00441022  0.07807308] - loss: 0.003916953340076928 - gradient - [-0.03242121  0.01102555]\n",
            "iteration: (189/1000) - error: [-0.14270155 -0.06914894  0.00440366  0.07795627] - loss: 0.0039052351403654154 - gradient - [-0.03237264  0.01100916]\n",
            "iteration: (190/1000) - error: [-0.14248792 -0.0690454   0.00439712  0.07783963] - loss: 0.003893551997547944 - gradient - [-0.03232414  0.01099279]\n",
            "iteration: (191/1000) - error: [-0.1422746  -0.06894201  0.00439057  0.07772316] - loss: 0.0038819038067455184 - gradient - [-0.03227572  0.01097643]\n",
            "iteration: (192/1000) - error: [-0.14206161 -0.06883879  0.00438404  0.07760686] - loss: 0.003870290463393107 - gradient - [-0.03222737  0.01096009]\n",
            "iteration: (193/1000) - error: [-0.14184894 -0.06873571  0.00437751  0.07749073] - loss: 0.003858711863238479 - gradient - [-0.0321791   0.01094377]\n",
            "iteration: (194/1000) - error: [-0.14163658 -0.0686328   0.00437099  0.07737477] - loss: 0.0038471679023414154 - gradient - [-0.03213091  0.01092747]\n",
            "iteration: (195/1000) - error: [-0.14142455 -0.06853004  0.00436447  0.07725898] - loss: 0.003835658477072728 - gradient - [-0.03208278  0.01091118]\n",
            "iteration: (196/1000) - error: [-0.14121283 -0.06842743  0.00435796  0.07714336] - loss: 0.0038241834841132166 - gradient - [-0.03203473  0.01089491]\n",
            "iteration: (197/1000) - error: [-0.14100143 -0.06832498  0.00435146  0.07702791] - loss: 0.0038127428204528316 - gradient - [-0.03198676  0.01087866]\n",
            "iteration: (198/1000) - error: [-0.14079035 -0.06822269  0.00434497  0.07691263] - loss: 0.003801336383389826 - gradient - [-0.03193886  0.01086243]\n",
            "iteration: (199/1000) - error: [-0.14057959 -0.06812055  0.00433849  0.07679753] - loss: 0.00378996407052961 - gradient - [-0.03189103  0.01084622]\n",
            "iteration: (200/1000) - error: [-0.14036914 -0.06801856  0.00433201  0.07668259] - loss: 0.003778625779783994 - gradient - [-0.03184328  0.01083003]\n",
            "iteration: (201/1000) - error: [-0.14015901 -0.06791673  0.00432554  0.07656782] - loss: 0.0037673214093702522 - gradient - [-0.03179559  0.01081386]\n",
            "iteration: (202/1000) - error: [-0.13994919 -0.06781505  0.00431908  0.07645322] - loss: 0.003756050857810087 - gradient - [-0.03174799  0.01079771]\n",
            "iteration: (203/1000) - error: [-0.13973969 -0.06771353  0.00431263  0.07633879] - loss: 0.0037448140239288697 - gradient - [-0.03170045  0.01078158]\n",
            "iteration: (204/1000) - error: [-0.1395305  -0.06761216  0.00430619  0.07622453] - loss: 0.0037336108068546986 - gradient - [-0.03165298  0.01076547]\n",
            "iteration: (205/1000) - error: [-0.13932162 -0.06751093  0.00429975  0.07611044] - loss: 0.00372244110601736 - gradient - [-0.03160559  0.01074939]\n",
            "iteration: (206/1000) - error: [-0.13911306 -0.06740987  0.00429333  0.07599652] - loss: 0.003711304821147573 - gradient - [-0.03155827  0.01073332]\n",
            "iteration: (207/1000) - error: [-0.13890481 -0.06730895  0.00428691  0.07588277] - loss: 0.0037002018522760364 - gradient - [-0.03151102  0.01071728]\n",
            "iteration: (208/1000) - error: [-0.13869688 -0.06720819  0.0042805   0.07576919] - loss: 0.0036891320997325242 - gradient - [-0.03146384  0.01070126]\n",
            "iteration: (209/1000) - error: [-0.13848925 -0.06710757  0.0042741   0.07565578] - loss: 0.0036780954641449986 - gradient - [-0.03141673  0.01068526]\n",
            "iteration: (210/1000) - error: [-0.13828193 -0.06700711  0.00426771  0.07554254] - loss: 0.0036670918464386647 - gradient - [-0.0313697   0.01066928]\n",
            "iteration: (211/1000) - error: [-0.13807493 -0.0669068   0.00426133  0.07542946] - loss: 0.0036561211478352233 - gradient - [-0.03132273  0.01065333]\n",
            "iteration: (212/1000) - error: [-0.13786824 -0.06680664  0.00425496  0.07531656] - loss: 0.0036451832698518104 - gradient - [-0.03127584  0.0106374 ]\n",
            "iteration: (213/1000) - error: [-0.13766185 -0.06670663  0.0042486   0.07520382] - loss: 0.0036342781143002263 - gradient - [-0.03122902  0.01062149]\n",
            "iteration: (214/1000) - error: [-0.13745578 -0.06660677  0.00424224  0.07509125] - loss: 0.003623405583286007 - gradient - [-0.03118226  0.0106056 ]\n",
            "iteration: (215/1000) - error: [-0.13725001 -0.06650706  0.0042359   0.07497885] - loss: 0.0036125655792075876 - gradient - [-0.03113558  0.01058974]\n",
            "iteration: (216/1000) - error: [-0.13704455 -0.0664075   0.00422956  0.07486662] - loss: 0.0036017580047553125 - gradient - [-0.03108897  0.0105739 ]\n",
            "iteration: (217/1000) - error: [-0.1368394  -0.06630808  0.00422323  0.07475455] - loss: 0.0035909827629107514 - gradient - [-0.03104243  0.01055808]\n",
            "iteration: (218/1000) - error: [-0.13663456 -0.06620882  0.00421691  0.07464265] - loss: 0.0035802397569456596 - gradient - [-0.03099595  0.01054229]\n",
            "iteration: (219/1000) - error: [-0.13643002 -0.06610971  0.00421061  0.07453092] - loss: 0.0035695288904211524 - gradient - [-0.03094955  0.01052651]\n",
            "iteration: (220/1000) - error: [-0.13622579 -0.06601074  0.00420431  0.07441935] - loss: 0.003558850067186943 - gradient - [-0.03090322  0.01051076]\n",
            "iteration: (221/1000) - error: [-0.13602187 -0.06591193  0.00419801  0.07430796] - loss: 0.0035482031913802834 - gradient - [-0.03085696  0.01049504]\n",
            "iteration: (222/1000) - error: [-0.13581825 -0.06581326  0.00419173  0.07419672] - loss: 0.0035375881674253517 - gradient - [-0.03081076  0.01047933]\n",
            "iteration: (223/1000) - error: [-0.13561493 -0.06571474  0.00418546  0.07408566] - loss: 0.003527004900032087 - gradient - [-0.03076464  0.01046365]\n",
            "iteration: (224/1000) - error: [-0.13541192 -0.06561636  0.0041792   0.07397476] - loss: 0.0035164532941956575 - gradient - [-0.03071858  0.01044799]\n",
            "iteration: (225/1000) - error: [-0.13520922 -0.06551814  0.00417294  0.07386402] - loss: 0.00350593325519536 - gradient - [-0.0306726   0.01043236]\n",
            "iteration: (226/1000) - error: [-0.13500681 -0.06542006  0.0041667   0.07375346] - loss: 0.0034954446885938923 - gradient - [-0.03062668  0.01041675]\n",
            "iteration: (227/1000) - error: [-0.13480472 -0.06532213  0.00416046  0.07364305] - loss: 0.003484987500236523 - gradient - [-0.03058083  0.01040116]\n",
            "iteration: (228/1000) - error: [-0.13460292 -0.06522434  0.00415424  0.07353281] - loss: 0.0034745615962500963 - gradient - [-0.03053505  0.01038559]\n",
            "iteration: (229/1000) - error: [-0.13440142 -0.0651267   0.00414802  0.07342274] - loss: 0.003464166883042406 - gradient - [-0.03048934  0.01037005]\n",
            "iteration: (230/1000) - error: [-0.13420023 -0.06502921  0.00414181  0.07331283] - loss: 0.0034538032673011774 - gradient - [-0.0304437   0.01035453]\n",
            "iteration: (231/1000) - error: [-0.13399934 -0.06493186  0.00413561  0.07320309] - loss: 0.0034434706559933227 - gradient - [-0.03039813  0.01033903]\n",
            "iteration: (232/1000) - error: [-0.13379875 -0.06483466  0.00412942  0.07309351] - loss: 0.003433168956364002 - gradient - [-0.03035262  0.01032356]\n",
            "iteration: (233/1000) - error: [-0.13359846 -0.06473761  0.00412324  0.07298409] - loss: 0.003422898075935985 - gradient - [-0.03030718  0.01030811]\n",
            "iteration: (234/1000) - error: [-0.13339847 -0.0646407   0.00411707  0.07287484] - loss: 0.0034126579225085742 - gradient - [-0.03026181  0.01029268]\n",
            "iteration: (235/1000) - error: [-0.13319878 -0.06454393  0.00411091  0.07276575] - loss: 0.0034024484041570682 - gradient - [-0.03021651  0.01027727]\n",
            "iteration: (236/1000) - error: [-0.13299938 -0.06444731  0.00410476  0.07265683] - loss: 0.003392269429231643 - gradient - [-0.03017128  0.01026189]\n",
            "iteration: (237/1000) - error: [-0.13280029 -0.06435084  0.00409861  0.07254806] - loss: 0.0033821209063566522 - gradient - [-0.03012611  0.01024653]\n",
            "iteration: (238/1000) - error: [-0.13260149 -0.06425451  0.00409248  0.07243946] - loss: 0.0033720027444299494 - gradient - [-0.03008102  0.01023119]\n",
            "iteration: (239/1000) - error: [-0.132403   -0.06415832  0.00408635  0.07233102] - loss: 0.0033619148526217776 - gradient - [-0.03003599  0.01021588]\n",
            "iteration: (240/1000) - error: [-0.13220479 -0.06406228  0.00408023  0.07222275] - loss: 0.00335185714037418 - gradient - [-0.02999102  0.01020059]\n",
            "iteration: (241/1000) - error: [-0.13200689 -0.06396638  0.00407413  0.07211464] - loss: 0.003341829517400133 - gradient - [-0.02994613  0.01018532]\n",
            "iteration: (242/1000) - error: [-0.13180928 -0.06387063  0.00406803  0.07200668] - loss: 0.0033318318936827198 - gradient - [-0.0299013   0.01017007]\n",
            "iteration: (243/1000) - error: [-0.13161197 -0.06377501  0.00406194  0.07189889] - loss: 0.0033218641794742873 - gradient - [-0.02985654  0.01015485]\n",
            "iteration: (244/1000) - error: [-0.13141495 -0.06367955  0.00405586  0.07179127] - loss: 0.003311926285295708 - gradient - [-0.02981184  0.01013965]\n",
            "iteration: (245/1000) - error: [-0.13121823 -0.06358422  0.00404979  0.0716838 ] - loss: 0.0033020181219355596 - gradient - [-0.02976722  0.01012447]\n",
            "iteration: (246/1000) - error: [-0.1310218  -0.06348904  0.00404373  0.07157649] - loss: 0.003292139600449258 - gradient - [-0.02972266  0.01010932]\n",
            "iteration: (247/1000) - error: [-0.13082567 -0.063394    0.00403767  0.07146935] - loss: 0.0032822906321583133 - gradient - [-0.02967816  0.01009418]\n",
            "iteration: (248/1000) - error: [-0.13062983 -0.0632991   0.00403163  0.07136236] - loss: 0.0032724711286496585 - gradient - [-0.02963374  0.01007907]\n",
            "iteration: (249/1000) - error: [-0.13043428 -0.06320434  0.0040256   0.07125553] - loss: 0.0032626810017745435 - gradient - [-0.02958937  0.01006399]\n",
            "iteration: (250/1000) - error: [-0.13023903 -0.06310973  0.00401957  0.07114887] - loss: 0.00325292016364805 - gradient - [-0.02954508  0.01004892]\n",
            "iteration: (251/1000) - error: [-0.13004407 -0.06301526  0.00401355  0.07104236] - loss: 0.0032431885266481094 - gradient - [-0.02950085  0.01003388]\n",
            "iteration: (252/1000) - error: [-0.1298494  -0.06292093  0.00400754  0.07093602] - loss: 0.0032334860034148784 - gradient - [-0.02945669  0.01001886]\n",
            "iteration: (253/1000) - error: [-0.12965502 -0.06282674  0.00400155  0.07082983] - loss: 0.0032238125068497978 - gradient - [-0.0294126   0.01000386]\n",
            "iteration: (254/1000) - error: [-0.12946093 -0.06273269  0.00399556  0.0707238 ] - loss: 0.0032141679501148813 - gradient - [-0.02936857  0.00998889]\n",
            "iteration: (255/1000) - error: [-0.12926714 -0.06263878  0.00398957  0.07061793] - loss: 0.003204552246631952 - gradient - [-0.0293246   0.00997394]\n",
            "iteration: (256/1000) - error: [-0.12907363 -0.06254501  0.0039836   0.07051222] - loss: 0.0031949653100818054 - gradient - [-0.02928071  0.00995901]\n",
            "iteration: (257/1000) - error: [-0.12888041 -0.06245139  0.00397764  0.07040667] - loss: 0.0031854070544035744 - gradient - [-0.02923687  0.0099441 ]\n",
            "iteration: (258/1000) - error: [-0.12868748 -0.0623579   0.00397169  0.07030127] - loss: 0.0031758773937937603 - gradient - [-0.02919311  0.00992921]\n",
            "iteration: (259/1000) - error: [-0.12849485 -0.06226455  0.00396574  0.07019603] - loss: 0.0031663762427055564 - gradient - [-0.02914941  0.00991435]\n",
            "iteration: (260/1000) - error: [-0.12830249 -0.06217135  0.0039598   0.07009095] - loss: 0.0031569035158481792 - gradient - [-0.02910577  0.00989951]\n",
            "iteration: (261/1000) - error: [-0.12811043 -0.06207828  0.00395388  0.06998603] - loss: 0.0031474591281858743 - gradient - [-0.0290622   0.00988469]\n",
            "iteration: (262/1000) - error: [-0.12791866 -0.06198535  0.00394796  0.06988126] - loss: 0.0031380429949373553 - gradient - [-0.0290187   0.00986989]\n",
            "iteration: (263/1000) - error: [-0.12772717 -0.06189256  0.00394205  0.06977666] - loss: 0.003128655031575035 - gradient - [-0.02897526  0.00985512]\n",
            "iteration: (264/1000) - error: [-0.12753597 -0.06179991  0.00393615  0.0696722 ] - loss: 0.0031192951538241203 - gradient - [-0.02893188  0.00984037]\n",
            "iteration: (265/1000) - error: [-0.12734505 -0.0617074   0.00393025  0.06956791] - loss: 0.003109963277661938 - gradient - [-0.02888857  0.00982564]\n",
            "iteration: (266/1000) - error: [-0.12715442 -0.06161503  0.00392437  0.06946377] - loss: 0.003100659319317226 - gradient - [-0.02884533  0.00981093]\n",
            "iteration: (267/1000) - error: [-0.12696408 -0.06152279  0.0039185   0.06935978] - loss: 0.003091383195269318 - gradient - [-0.02880215  0.00979624]\n",
            "iteration: (268/1000) - error: [-0.12677402 -0.06143069  0.00391263  0.06925596] - loss: 0.003082134822247373 - gradient - [-0.02875903  0.00978158]\n",
            "iteration: (269/1000) - error: [-0.12658425 -0.06133874  0.00390677  0.06915228] - loss: 0.003072914117229735 - gradient - [-0.02871598  0.00976693]\n",
            "iteration: (270/1000) - error: [-0.12639476 -0.06124691  0.00390093  0.06904877] - loss: 0.0030637209974430724 - gradient - [-0.02867299  0.00975231]\n",
            "iteration: (271/1000) - error: [-0.12620555 -0.06115523  0.00389509  0.0689454 ] - loss: 0.0030545553803617677 - gradient - [-0.02863007  0.00973771]\n",
            "iteration: (272/1000) - error: [-0.12601662 -0.06106368  0.00388926  0.0688422 ] - loss: 0.00304541718370695 - gradient - [-0.02858721  0.00972314]\n",
            "iteration: (273/1000) - error: [-0.12582798 -0.06097228  0.00388343  0.06873914] - loss: 0.0030363063254460103 - gradient - [-0.02854442  0.00970858]\n",
            "iteration: (274/1000) - error: [-0.12563963 -0.060881    0.00387762  0.06863624] - loss: 0.0030272227237916915 - gradient - [-0.02850169  0.00969405]\n",
            "iteration: (275/1000) - error: [-0.12545155 -0.06078987  0.00387182  0.0685335 ] - loss: 0.003018166297201504 - gradient - [-0.02845903  0.00967954]\n",
            "iteration: (276/1000) - error: [-0.12526375 -0.06069887  0.00386602  0.06843091] - loss: 0.003009136964376793 - gradient - [-0.02841642  0.00966505]\n",
            "iteration: (277/1000) - error: [-0.12507624 -0.060608    0.00386023  0.06832847] - loss: 0.003000134644262196 - gradient - [-0.02837389  0.00965058]\n",
            "iteration: (278/1000) - error: [-0.12488901 -0.06051728  0.00385445  0.06822618] - loss: 0.0029911592560448452 - gradient - [-0.02833141  0.00963613]\n",
            "iteration: (279/1000) - error: [-0.12470205 -0.06042669  0.00384868  0.06812405] - loss: 0.002982210719153566 - gradient - [-0.028289    0.00962171]\n",
            "iteration: (280/1000) - error: [-0.12451538 -0.06033623  0.00384292  0.06802207] - loss: 0.0029732889532583023 - gradient - [-0.02824665  0.00960731]\n",
            "iteration: (281/1000) - error: [-0.12432899 -0.06024591  0.00383717  0.06792025] - loss: 0.0029643938782692716 - gradient - [-0.02820437  0.00959292]\n",
            "iteration: (282/1000) - error: [-0.12414287 -0.06015572  0.00383143  0.06781858] - loss: 0.002955525414336331 - gradient - [-0.02816215  0.00957856]\n",
            "iteration: (283/1000) - error: [-0.12395704 -0.06006567  0.00382569  0.06771705] - loss: 0.002946683481848138 - gradient - [-0.02811999  0.00956423]\n",
            "iteration: (284/1000) - error: [-0.12377148 -0.05997576  0.00381996  0.06761569] - loss: 0.0029378680014316805 - gradient - [-0.0280779   0.00954991]\n",
            "iteration: (285/1000) - error: [-0.1235862  -0.05988598  0.00381425  0.06751447] - loss: 0.002929078893951216 - gradient - [-0.02803587  0.00953561]\n",
            "iteration: (286/1000) - error: [-0.1234012  -0.05979633  0.00380854  0.0674134 ] - loss: 0.0029203160805079207 - gradient - [-0.0279939   0.00952134]\n",
            "iteration: (287/1000) - error: [-0.12321647 -0.05970682  0.00380283  0.06731249] - loss: 0.002911579482438861 - gradient - [-0.02795199  0.00950709]\n",
            "iteration: (288/1000) - error: [-0.12303202 -0.05961744  0.00379714  0.06721172] - loss: 0.0029028690213165256 - gradient - [-0.02791015  0.00949285]\n",
            "iteration: (289/1000) - error: [-0.12284785 -0.0595282   0.00379146  0.06711111] - loss: 0.002894184618948015 - gradient - [-0.02786837  0.00947864]\n",
            "iteration: (290/1000) - error: [-0.12266395 -0.05943909  0.00378578  0.06701065] - loss: 0.0028855261973743454 - gradient - [-0.02782665  0.00946445]\n",
            "iteration: (291/1000) - error: [-0.12248033 -0.05935011  0.00378011  0.06691034] - loss: 0.002876893678869761 - gradient - [-0.027785    0.00945029]\n",
            "iteration: (292/1000) - error: [-0.12229698 -0.05926126  0.00377446  0.06681018] - loss: 0.002868286985941066 - gradient - [-0.0277434   0.00943614]\n",
            "iteration: (293/1000) - error: [-0.12211391 -0.05917255  0.00376881  0.06671016] - loss: 0.002859706041326826 - gradient - [-0.02770187  0.00942201]\n",
            "iteration: (294/1000) - error: [-0.12193111 -0.05908397  0.00376316  0.0666103 ] - loss: 0.002851150767996859 - gradient - [-0.02766041  0.00940791]\n",
            "iteration: (295/1000) - error: [-0.12174859 -0.05899553  0.00375753  0.06651059] - loss: 0.002842621089151267 - gradient - [-0.027619    0.00939383]\n",
            "iteration: (296/1000) - error: [-0.12156634 -0.05890722  0.00375191  0.06641103] - loss: 0.002834116928220097 - gradient - [-0.02757765  0.00937977]\n",
            "iteration: (297/1000) - error: [-0.12138436 -0.05881903  0.00374629  0.06631161] - loss: 0.002825638208862322 - gradient - [-0.02753637  0.00936572]\n",
            "iteration: (298/1000) - error: [-0.12120265 -0.05873098  0.00374068  0.06621235] - loss: 0.002817184854965436 - gradient - [-0.02749515  0.0093517 ]\n",
            "iteration: (299/1000) - error: [-0.12102122 -0.05864307  0.00373508  0.06611323] - loss: 0.0028087567906444743 - gradient - [-0.02745399  0.00933771]\n",
            "iteration: (300/1000) - error: [-0.12084005 -0.05855528  0.00372949  0.06601426] - loss: 0.0028003539402415988 - gradient - [-0.0274129   0.00932373]\n",
            "iteration: (301/1000) - error: [-0.12065916 -0.05846763  0.00372391  0.06591544] - loss: 0.0027919762283253077 - gradient - [-0.02737186  0.00930977]\n",
            "iteration: (302/1000) - error: [-0.12047854 -0.0583801   0.00371833  0.06581677] - loss: 0.0027836235796897544 - gradient - [-0.02733089  0.00929583]\n",
            "iteration: (303/1000) - error: [-0.12029819 -0.05829271  0.00371277  0.06571825] - loss: 0.002775295919354049 - gradient - [-0.02728997  0.00928192]\n",
            "iteration: (304/1000) - error: [-0.12011811 -0.05820545  0.00370721  0.06561987] - loss: 0.0027669931725617086 - gradient - [-0.02724912  0.00926802]\n",
            "iteration: (305/1000) - error: [-0.1199383  -0.05811832  0.00370166  0.06552164] - loss: 0.0027587152647797772 - gradient - [-0.02720833  0.00925415]\n",
            "iteration: (306/1000) - error: [-0.11975876 -0.05803132  0.00369612  0.06542356] - loss: 0.0027504621216983697 - gradient - [-0.0271676  0.0092403]\n",
            "iteration: (307/1000) - error: [-0.11957948 -0.05794445  0.00369059  0.06532562] - loss: 0.0027422336692298956 - gradient - [-0.02712693  0.00922646]\n",
            "iteration: (308/1000) - error: [-0.11940048 -0.05785771  0.00368506  0.06522783] - loss: 0.002734029833508344 - gradient - [-0.02708632  0.00921265]\n",
            "iteration: (309/1000) - error: [-0.11922174 -0.0577711   0.00367954  0.06513019] - loss: 0.002725850540888745 - gradient - [-0.02704578  0.00919886]\n",
            "iteration: (310/1000) - error: [-0.11904327 -0.05768462  0.00367404  0.06503269] - loss: 0.002717695717946452 - gradient - [-0.02700529  0.00918509]\n",
            "iteration: (311/1000) - error: [-0.11886507 -0.05759827  0.00366854  0.06493534] - loss: 0.002709565291476472 - gradient - [-0.02696487  0.00917134]\n",
            "iteration: (312/1000) - error: [-0.11868714 -0.05751205  0.00366305  0.06483814] - loss: 0.002701459188492775 - gradient - [-0.0269245   0.00915761]\n",
            "iteration: (313/1000) - error: [-0.11850947 -0.05742595  0.00365756  0.06474108] - loss: 0.0026933773362277577 - gradient - [-0.0268842  0.0091439]\n",
            "iteration: (314/1000) - error: [-0.11833206 -0.05733999  0.00365209  0.06464416] - loss: 0.0026853196621314445 - gradient - [-0.02684395  0.00913022]\n",
            "iteration: (315/1000) - error: [-0.11815493 -0.05725415  0.00364662  0.06454739] - loss: 0.0026772860938709406 - gradient - [-0.02680377  0.00911655]\n",
            "iteration: (316/1000) - error: [-0.11797806 -0.05716845  0.00364116  0.06445077] - loss: 0.002669276559329755 - gradient - [-0.02676364  0.0091029 ]\n",
            "iteration: (317/1000) - error: [-0.11780145 -0.05708287  0.00363571  0.06435429] - loss: 0.002661290986607103 - gradient - [-0.02672358  0.00908928]\n",
            "iteration: (318/1000) - error: [-0.1176251  -0.05699742  0.00363027  0.06425795] - loss: 0.002653329304017329 - gradient - [-0.02668358  0.00907567]\n",
            "iteration: (319/1000) - error: [-0.11744903 -0.0569121   0.00362483  0.06416176] - loss: 0.002645391440089251 - gradient - [-0.02664363  0.00906208]\n",
            "iteration: (320/1000) - error: [-0.11727321 -0.0568269   0.00361941  0.06406572] - loss: 0.002637477323565499 - gradient - [-0.02660375  0.00904852]\n",
            "iteration: (321/1000) - error: [-0.11709766 -0.05674183  0.00361399  0.06396981] - loss: 0.0026295868834018002 - gradient - [-0.02656392  0.00903497]\n",
            "iteration: (322/1000) - error: [-0.11692237 -0.05665689  0.00360858  0.06387405] - loss: 0.0026217200487665864 - gradient - [-0.02652416  0.00902145]\n",
            "iteration: (323/1000) - error: [-0.11674734 -0.05657208  0.00360318  0.06377844] - loss: 0.002613876749040065 - gradient - [-0.02648445  0.00900794]\n",
            "iteration: (324/1000) - error: [-0.11657258 -0.0564874   0.00359778  0.06368296] - loss: 0.00260605691381369 - gradient - [-0.02644481  0.00899446]\n",
            "iteration: (325/1000) - error: [-0.11639807 -0.05640284  0.0035924   0.06358763] - loss: 0.0025982604728896973 - gradient - [-0.02640522  0.00898099]\n",
            "iteration: (326/1000) - error: [-0.11622383 -0.05631841  0.00358702  0.06349245] - loss: 0.0025904873562802274 - gradient - [-0.02636569  0.00896755]\n",
            "iteration: (327/1000) - error: [-0.11604985 -0.0562341   0.00358165  0.0633974 ] - loss: 0.002582737494206776 - gradient - [-0.02632622  0.00895413]\n",
            "iteration: (328/1000) - error: [-0.11587613 -0.05614992  0.00357629  0.0633025 ] - loss: 0.002575010817099672 - gradient - [-0.02628682  0.00894072]\n",
            "iteration: (329/1000) - error: [-0.11570267 -0.05606587  0.00357094  0.06320774] - loss: 0.0025673072555973467 - gradient - [-0.02624747  0.00892734]\n",
            "iteration: (330/1000) - error: [-0.11552947 -0.05598194  0.00356559  0.06311312] - loss: 0.0025596267405456645 - gradient - [-0.02620817  0.00891398]\n",
            "iteration: (331/1000) - error: [-0.11535652 -0.05589814  0.00356025  0.06301864] - loss: 0.0025519692029975284 - gradient - [-0.02616894  0.00890063]\n",
            "iteration: (332/1000) - error: [-0.11518384 -0.05581446  0.00355492  0.0629243 ] - loss: 0.0025443345742119515 - gradient - [-0.02612977  0.00888731]\n",
            "iteration: (333/1000) - error: [-0.11501142 -0.05573091  0.0035496   0.06283011] - loss: 0.0025367227856536857 - gradient - [-0.02609065  0.008874  ]\n",
            "iteration: (334/1000) - error: [-0.11483925 -0.05564748  0.00354429  0.06273606] - loss: 0.0025291337689925157 - gradient - [-0.0260516   0.00886072]\n",
            "iteration: (335/1000) - error: [-0.11466734 -0.05556418  0.00353898  0.06264214] - loss: 0.002521567456102599 - gradient - [-0.0260126   0.00884746]\n",
            "iteration: (336/1000) - error: [-0.11449569 -0.055481    0.00353368  0.06254837] - loss: 0.0025140237790619467 - gradient - [-0.02597366  0.00883421]\n",
            "iteration: (337/1000) - error: [-0.11432429 -0.05539795  0.00352839  0.06245474] - loss: 0.002506502670151749 - gradient - [-0.02593478  0.00882099]\n",
            "iteration: (338/1000) - error: [-0.11415316 -0.05531502  0.00352311  0.06236125] - loss: 0.0024990040618557783 - gradient - [-0.02589595  0.00880778]\n",
            "iteration: (339/1000) - error: [-0.11398228 -0.05523222  0.00351784  0.0622679 ] - loss: 0.002491527886859823 - gradient - [-0.02585719  0.0087946 ]\n",
            "iteration: (340/1000) - error: [-0.11381165 -0.05514954  0.00351257  0.06217468] - loss: 0.002484074078051031 - gradient - [-0.02581848  0.00878143]\n",
            "iteration: (341/1000) - error: [-0.11364128 -0.05506698  0.00350731  0.06208161] - loss: 0.0024766425685173446 - gradient - [-0.02577983  0.00876829]\n",
            "iteration: (342/1000) - error: [-0.11347116 -0.05498455  0.00350206  0.06198868] - loss: 0.0024692332915468495 - gradient - [-0.02574124  0.00875516]\n",
            "iteration: (343/1000) - error: [-0.1133013  -0.05490224  0.00349682  0.06189588] - loss: 0.0024618461806272507 - gradient - [-0.02570271  0.00874206]\n",
            "iteration: (344/1000) - error: [-0.1131317  -0.05482005  0.00349159  0.06180323] - loss: 0.0024544811694452453 - gradient - [-0.02566423  0.00872897]\n",
            "iteration: (345/1000) - error: [-0.11296234 -0.05473799  0.00348636  0.06171071] - loss: 0.0024471381918858364 - gradient - [-0.02562582  0.0087159 ]\n",
            "iteration: (346/1000) - error: [-0.11279324 -0.05465605  0.00348114  0.06161834] - loss: 0.002439817182031938 - gradient - [-0.02558745  0.00870285]\n",
            "iteration: (347/1000) - error: [-0.1126244  -0.05457423  0.00347593  0.0615261 ] - loss: 0.0024325180741635406 - gradient - [-0.02554915  0.00868983]\n",
            "iteration: (348/1000) - error: [-0.11245581 -0.05449254  0.00347073  0.06143399] - loss: 0.0024252408027573423 - gradient - [-0.02551091  0.00867682]\n",
            "iteration: (349/1000) - error: [-0.11228746 -0.05441097  0.00346553  0.06134203] - loss: 0.0024179853024860684 - gradient - [-0.02547272  0.00866383]\n",
            "iteration: (350/1000) - error: [-0.11211938 -0.05432952  0.00346034  0.0612502 ] - loss: 0.0024107515082178207 - gradient - [-0.02543459  0.00865086]\n",
            "iteration: (351/1000) - error: [-0.11195154 -0.05424819  0.00345516  0.06115852] - loss: 0.0024035393550155305 - gradient - [-0.02539651  0.00863791]\n",
            "iteration: (352/1000) - error: [-0.11178395 -0.05416698  0.00344999  0.06106696] - loss: 0.002396348778136524 - gradient - [-0.02535849  0.00862498]\n",
            "iteration: (353/1000) - error: [-0.11161662 -0.05408589  0.00344483  0.06097555] - loss: 0.0023891797130316797 - gradient - [-0.02532053  0.00861207]\n",
            "iteration: (354/1000) - error: [-0.11144953 -0.05400493  0.00343967  0.06088427] - loss: 0.0023820320953450874 - gradient - [-0.02528263  0.00859918]\n",
            "iteration: (355/1000) - error: [-0.1112827  -0.05392409  0.00343452  0.06079313] - loss: 0.002374905860913298 - gradient - [-0.02524478  0.0085863 ]\n",
            "iteration: (356/1000) - error: [-0.11111611 -0.05384337  0.00342938  0.06070213] - loss: 0.0023678009457648525 - gradient - [-0.02520699  0.00857345]\n",
            "iteration: (357/1000) - error: [-0.11094978 -0.05376277  0.00342425  0.06061126] - loss: 0.0023607172861196705 - gradient - [-0.02516926  0.00856062]\n",
            "iteration: (358/1000) - error: [-0.11078369 -0.05368229  0.00341912  0.06052053] - loss: 0.0023536548183884626 - gradient - [-0.02513158  0.0085478 ]\n",
            "iteration: (359/1000) - error: [-0.11061785 -0.05360193  0.003414    0.06042993] - loss: 0.002346613479172179 - gradient - [-0.02509396  0.00853501]\n",
            "iteration: (360/1000) - error: [-0.11045226 -0.05352169  0.00340889  0.06033947] - loss: 0.002339593205261486 - gradient - [-0.0250564   0.00852223]\n",
            "iteration: (361/1000) - error: [-0.11028692 -0.05344157  0.00340379  0.06024914] - loss: 0.002332593933636083 - gradient - [-0.02501889  0.00850947]\n",
            "iteration: (362/1000) - error: [-0.11012183 -0.05336157  0.00339869  0.06015895] - loss: 0.0023256156014642973 - gradient - [-0.02498144  0.00849673]\n",
            "iteration: (363/1000) - error: [-0.10995698 -0.05328169  0.00339361  0.0600689 ] - loss: 0.0023186581461022617 - gradient - [-0.02494404  0.00848402]\n",
            "iteration: (364/1000) - error: [-0.10979238 -0.05320193  0.00338853  0.05997898] - loss: 0.0023117215050936996 - gradient - [-0.0249067   0.00847132]\n",
            "iteration: (365/1000) - error: [-0.10962803 -0.05312229  0.00338345  0.05988919] - loss: 0.0023048056161690434 - gradient - [-0.02486942  0.00845863]\n",
            "iteration: (366/1000) - error: [-0.10946392 -0.05304277  0.00337839  0.05979954] - loss: 0.0022979104172451423 - gradient - [-0.02483219  0.00844597]\n",
            "iteration: (367/1000) - error: [-0.10930006 -0.05296336  0.00337333  0.05971003] - loss: 0.0022910358464244846 - gradient - [-0.02479502  0.00843333]\n",
            "iteration: (368/1000) - error: [-0.10913644 -0.05288408  0.00336828  0.05962064] - loss: 0.002284181841994747 - gradient - [-0.0247579  0.0084207]\n",
            "iteration: (369/1000) - error: [-0.10897307 -0.05280491  0.00336324  0.05953139] - loss: 0.0022773483424282856 - gradient - [-0.02472084  0.0084081 ]\n",
            "iteration: (370/1000) - error: [-0.10880994 -0.05272587  0.00335821  0.05944228] - loss: 0.002270535286381433 - gradient - [-0.02468383  0.00839551]\n",
            "iteration: (371/1000) - error: [-0.10864706 -0.05264694  0.00335318  0.0593533 ] - loss: 0.0022637426126940925 - gradient - [-0.02464688  0.00838295]\n",
            "iteration: (372/1000) - error: [-0.10848442 -0.05256813  0.00334816  0.05926445] - loss: 0.0022569702603891777 - gradient - [-0.02460999  0.0083704 ]\n",
            "iteration: (373/1000) - error: [-0.10832202 -0.05248944  0.00334315  0.05917573] - loss: 0.002250218168671974 - gradient - [-0.02457315  0.00835787]\n",
            "iteration: (374/1000) - error: [-0.10815987 -0.05241086  0.00333814  0.05908715] - loss: 0.0022434862769296258 - gradient - [-0.02453636  0.00834535]\n",
            "iteration: (375/1000) - error: [-0.10799796 -0.05233241  0.00333314  0.0589987 ] - loss: 0.0022367745247306734 - gradient - [-0.02449963  0.00833286]\n",
            "iteration: (376/1000) - error: [-0.10783629 -0.05225407  0.00332816  0.05891038] - loss: 0.002230082851824402 - gradient - [-0.02446296  0.00832039]\n",
            "iteration: (377/1000) - error: [-0.10767487 -0.05217585  0.00332317  0.05882219] - loss: 0.00222341119814037 - gradient - [-0.02442634  0.00830793]\n",
            "iteration: (378/1000) - error: [-0.10751368 -0.05209774  0.0033182   0.05873414] - loss: 0.0022167595037878114 - gradient - [-0.02438977  0.0082955 ]\n",
            "iteration: (379/1000) - error: [-0.10735274 -0.05201975  0.00331323  0.05864622] - loss: 0.0022101277090551554 - gradient - [-0.02435326  0.00828308]\n",
            "iteration: (380/1000) - error: [-0.10719204 -0.05194188  0.00330827  0.05855843] - loss: 0.002203515754409511 - gradient - [-0.02431681  0.00827068]\n",
            "iteration: (381/1000) - error: [-0.10703158 -0.05186413  0.00330332  0.05847077] - loss: 0.002196923580496001 - gradient - [-0.0242804  0.0082583]\n",
            "iteration: (382/1000) - error: [-0.10687136 -0.05178649  0.00329837  0.05838324] - loss: 0.0021903511281373927 - gradient - [-0.02424406  0.00824594]\n",
            "iteration: (383/1000) - error: [-0.10671137 -0.05170897  0.00329344  0.05829584] - loss: 0.002183798338333459 - gradient - [-0.02420777  0.00823359]\n",
            "iteration: (384/1000) - error: [-0.10655163 -0.05163156  0.00328851  0.05820858] - loss: 0.0021772651522604554 - gradient - [-0.02417153  0.00822127]\n",
            "iteration: (385/1000) - error: [-0.10639213 -0.05155427  0.00328358  0.05812144] - loss: 0.002170751511270715 - gradient - [-0.02413534  0.00820896]\n",
            "iteration: (386/1000) - error: [-0.10623287 -0.0514771   0.00327867  0.05803444] - loss: 0.002164257356891892 - gradient - [-0.02409921  0.00819667]\n",
            "iteration: (387/1000) - error: [-0.10607384 -0.05140004  0.00327376  0.05794756] - loss: 0.002157782630826645 - gradient - [-0.02406314  0.0081844 ]\n",
            "iteration: (388/1000) - error: [-0.10591505 -0.0513231   0.00326886  0.05786082] - loss: 0.0021513272749520717 - gradient - [-0.02402712  0.00817215]\n",
            "iteration: (389/1000) - error: [-0.1057565  -0.05124627  0.00326397  0.0577742 ] - loss: 0.0021448912313190823 - gradient - [-0.02399115  0.00815992]\n",
            "iteration: (390/1000) - error: [-0.10559819 -0.05116956  0.00325908  0.05768772] - loss: 0.0021384744421520116 - gradient - [-0.02395524  0.0081477 ]\n",
            "iteration: (391/1000) - error: [-0.10544012 -0.05109296  0.0032542   0.05760136] - loss: 0.00213207684984796 - gradient - [-0.02391938  0.00813551]\n",
            "iteration: (392/1000) - error: [-0.10528228 -0.05101647  0.00324933  0.05751513] - loss: 0.0021256983969764305 - gradient - [-0.02388357  0.00812333]\n",
            "iteration: (393/1000) - error: [-0.10512467 -0.0509401   0.00324447  0.05742904] - loss: 0.0021193390262787414 - gradient - [-0.02384782  0.00811117]\n",
            "iteration: (394/1000) - error: [-0.10496731 -0.05086385  0.00323961  0.05734307] - loss: 0.0021129986806674187 - gradient - [-0.02381212  0.00809902]\n",
            "iteration: (395/1000) - error: [-0.10481018 -0.05078771  0.00323476  0.05725723] - loss: 0.002106677303225904 - gradient - [-0.02377647  0.0080869 ]\n",
            "iteration: (396/1000) - error: [-0.10465328 -0.05071168  0.00322992  0.05717152] - loss: 0.0021003748372077974 - gradient - [-0.02374088  0.00807479]\n",
            "iteration: (397/1000) - error: [-0.10449662 -0.05063577  0.00322508  0.05708593] - loss: 0.002094091226036552 - gradient - [-0.02370534  0.00806271]\n",
            "iteration: (398/1000) - error: [-0.10434019 -0.05055997  0.00322026  0.05700048] - loss: 0.002087826413304834 - gradient - [-0.02366986  0.00805064]\n",
            "iteration: (399/1000) - error: [-0.104184   -0.05048428  0.00321543  0.05691515] - loss: 0.0020815803427740773 - gradient - [-0.02363442  0.00803859]\n",
            "iteration: (400/1000) - error: [-0.10402804 -0.05040871  0.00321062  0.05682995] - loss: 0.0020753529583739075 - gradient - [-0.02359904  0.00802655]\n",
            "iteration: (401/1000) - error: [-0.10387232 -0.05033325  0.00320582  0.05674488] - loss: 0.002069144204201779 - gradient - [-0.02356372  0.00801454]\n",
            "iteration: (402/1000) - error: [-0.10371683 -0.05025791  0.00320102  0.05665994] - loss: 0.002062954024522379 - gradient - [-0.02352844  0.00800254]\n",
            "iteration: (403/1000) - error: [-0.10356157 -0.05018267  0.00319622  0.05657512] - loss: 0.0020567823637670725 - gradient - [-0.02349322  0.00799056]\n",
            "iteration: (404/1000) - error: [-0.10340654 -0.05010755  0.00319144  0.05649043] - loss: 0.0020506291665334806 - gradient - [-0.02345806  0.0079786 ]\n",
            "iteration: (405/1000) - error: [-0.10325175 -0.05003254  0.00318666  0.05640587] - loss: 0.002044494377585032 - gradient - [-0.02342294  0.00796666]\n",
            "iteration: (406/1000) - error: [-0.10309718 -0.04995765  0.00318189  0.05632143] - loss: 0.002038377941850332 - gradient - [-0.02338788  0.00795473]\n",
            "iteration: (407/1000) - error: [-0.10294285 -0.04988286  0.00317713  0.05623712] - loss: 0.0020322798044227946 - gradient - [-0.02335287  0.00794282]\n",
            "iteration: (408/1000) - error: [-0.10278875 -0.04980819  0.00317237  0.05615294] - loss: 0.0020261999105600616 - gradient - [-0.02331791  0.00793093]\n",
            "iteration: (409/1000) - error: [-0.10263488 -0.04973363  0.00316762  0.05606888] - loss: 0.0020201382056835577 - gradient - [-0.023283    0.00791906]\n",
            "iteration: (410/1000) - error: [-0.10248124 -0.04965918  0.00316288  0.05598494] - loss: 0.002014094635378002 - gradient - [-0.02324815  0.00790721]\n",
            "iteration: (411/1000) - error: [-0.10232783 -0.04958484  0.00315815  0.05590114] - loss: 0.0020080691453908802 - gradient - [-0.02321335  0.00789537]\n",
            "iteration: (412/1000) - error: [-0.10217465 -0.04951062  0.00315342  0.05581746] - loss: 0.0020020616816319873 - gradient - [-0.0231786   0.00788355]\n",
            "iteration: (413/1000) - error: [-0.1020217 -0.0494365  0.0031487  0.0557339] - loss: 0.001996072190172953 - gradient - [-0.0231439   0.00787175]\n",
            "iteration: (414/1000) - error: [-0.10186898 -0.0493625   0.00314399  0.05565047] - loss: 0.001990100617246737 - gradient - [-0.02310926  0.00785997]\n",
            "iteration: (415/1000) - error: [-0.10171649 -0.0492886   0.00313928  0.05556716] - loss: 0.001984146909247128 - gradient - [-0.02307466  0.0078482 ]\n",
            "iteration: (416/1000) - error: [-0.10156422 -0.04921482  0.00313458  0.05548398] - loss: 0.0019782110127283645 - gradient - [-0.02304012  0.00783645]\n",
            "iteration: (417/1000) - error: [-0.10141219 -0.04914115  0.00312989  0.05540093] - loss: 0.001972292874404449 - gradient - [-0.02300563  0.00782472]\n",
            "iteration: (418/1000) - error: [-0.10126038 -0.04906759  0.0031252   0.05531799] - loss: 0.001966392441148896 - gradient - [-0.02297119  0.00781301]\n",
            "iteration: (419/1000) - error: [-0.1011088  -0.04899414  0.00312052  0.05523518] - loss: 0.0019605096599941336 - gradient - [-0.02293681  0.00780131]\n",
            "iteration: (420/1000) - error: [-0.10095744 -0.04892079  0.00311585  0.0551525 ] - loss: 0.00195464447813104 - gradient - [-0.02290247  0.00778963]\n",
            "iteration: (421/1000) - error: [-0.10080631 -0.04884756  0.00311119  0.05506994] - loss: 0.001948796842908497 - gradient - [-0.02286819  0.00777797]\n",
            "iteration: (422/1000) - error: [-0.10065541 -0.04877444  0.00310653  0.0549875 ] - loss: 0.0019429667018328754 - gradient - [-0.02283395  0.00776633]\n",
            "iteration: (423/1000) - error: [-0.10050473 -0.04870143  0.00310188  0.05490519] - loss: 0.0019371540025676143 - gradient - [-0.02279977  0.0077547 ]\n",
            "iteration: (424/1000) - error: [-0.10035428 -0.04862852  0.00309724  0.054823  ] - loss: 0.0019313586929327296 - gradient - [-0.02276564  0.00774309]\n",
            "iteration: (425/1000) - error: [-0.10020406 -0.04855573  0.0030926   0.05474093] - loss: 0.0019255807209042888 - gradient - [-0.02273156  0.0077315 ]\n",
            "iteration: (426/1000) - error: [-0.10005406 -0.04848304  0.00308797  0.05465899] - loss: 0.0019198200346141076 - gradient - [-0.02269754  0.00771993]\n",
            "iteration: (427/1000) - error: [-0.09990428 -0.04841047  0.00308335  0.05457716] - loss: 0.0019140765823490184 - gradient - [-0.02266356  0.00770837]\n",
            "iteration: (428/1000) - error: [-0.09975473 -0.048338    0.00307873  0.05449547] - loss: 0.0019083503125506861 - gradient - [-0.02262963  0.00769683]\n",
            "iteration: (429/1000) - error: [-0.0996054  -0.04826564  0.00307413  0.05441389] - loss: 0.0019026411738149896 - gradient - [-0.02259576  0.00768531]\n",
            "iteration: (430/1000) - error: [-0.0994563  -0.04819339  0.00306952  0.05433243] - loss: 0.0018969491148915572 - gradient - [-0.02256193  0.00767381]\n",
            "iteration: (431/1000) - error: [-0.09930742 -0.04812124  0.00306493  0.0542511 ] - loss: 0.0018912740846833873 - gradient - [-0.02252816  0.00766232]\n",
            "iteration: (432/1000) - error: [-0.09915876 -0.04804921  0.00306034  0.05416989] - loss: 0.0018856160322463405 - gradient - [-0.02249443  0.00765085]\n",
            "iteration: (433/1000) - error: [-0.09901032 -0.04797728  0.00305576  0.0540888 ] - loss: 0.0018799749067886246 - gradient - [-0.02246076  0.0076394 ]\n",
            "iteration: (434/1000) - error: [-0.09886211 -0.04790546  0.00305118  0.05400783] - loss: 0.0018743506576704792 - gradient - [-0.02242714  0.00762796]\n",
            "iteration: (435/1000) - error: [-0.09871412 -0.04783375  0.00304662  0.05392698] - loss: 0.0018687432344035898 - gradient - [-0.02239357  0.00761654]\n",
            "iteration: (436/1000) - error: [-0.09856635 -0.04776214  0.00304206  0.05384626] - loss: 0.00186315258665066 - gradient - [-0.02236004  0.00760514]\n",
            "iteration: (437/1000) - error: [-0.0984188  -0.04769065  0.0030375   0.05376565] - loss: 0.001857578664225106 - gradient - [-0.02232657  0.00759376]\n",
            "iteration: (438/1000) - error: [-0.09827147 -0.04761926  0.00303296  0.05368517] - loss: 0.0018520214170903546 - gradient - [-0.02229315  0.00758239]\n",
            "iteration: (439/1000) - error: [-0.09812436 -0.04754797  0.00302842  0.0536048 ] - loss: 0.0018464807953595722 - gradient - [-0.02225978  0.00757104]\n",
            "iteration: (440/1000) - error: [-0.09797747 -0.0474768   0.00302388  0.05352456] - loss: 0.0018409567492951749 - gradient - [-0.02222646  0.00755971]\n",
            "iteration: (441/1000) - error: [-0.09783081 -0.04740573  0.00301936  0.05344444] - loss: 0.0018354492293084054 - gradient - [-0.02219318  0.00754839]\n",
            "iteration: (442/1000) - error: [-0.09768436 -0.04733476  0.00301484  0.05336443] - loss: 0.0018299581859587874 - gradient - [-0.02215996  0.00753709]\n",
            "iteration: (443/1000) - error: [-0.09753813 -0.0472639   0.00301032  0.05328455] - loss: 0.001824483569953821 - gradient - [-0.02212679  0.00752581]\n",
            "iteration: (444/1000) - error: [-0.09739212 -0.04719315  0.00300582  0.05320478] - loss: 0.0018190253321484424 - gradient - [-0.02209367  0.00751454]\n",
            "iteration: (445/1000) - error: [-0.09724633 -0.04712251  0.00300132  0.05312514] - loss: 0.001813583423544622 - gradient - [-0.02206059  0.00750329]\n",
            "iteration: (446/1000) - error: [-0.09710076 -0.04705197  0.00299682  0.05304561] - loss: 0.001808157795290896 - gradient - [-0.02202757  0.00749206]\n",
            "iteration: (447/1000) - error: [-0.0969554  -0.04698153  0.00299234  0.05296621] - loss: 0.0018027483986819818 - gradient - [-0.0219946   0.00748085]\n",
            "iteration: (448/1000) - error: [-0.09681026 -0.0469112   0.00298786  0.05288692] - loss: 0.0017973551851582482 - gradient - [-0.02196167  0.00746965]\n",
            "iteration: (449/1000) - error: [-0.09666534 -0.04684098  0.00298339  0.05280775] - loss: 0.0017919781063054294 - gradient - [-0.0219288   0.00745846]\n",
            "iteration: (450/1000) - error: [-0.09652064 -0.04677086  0.00297892  0.0527287 ] - loss: 0.001786617113854016 - gradient - [-0.02189597  0.0074473 ]\n",
            "iteration: (451/1000) - error: [-0.09637615 -0.04670085  0.00297446  0.05264977] - loss: 0.001781272159678926 - gradient - [-0.02186319  0.00743615]\n",
            "iteration: (452/1000) - error: [-0.09623188 -0.04663094  0.00297001  0.05257095] - loss: 0.0017759431957990947 - gradient - [-0.02183046  0.00742502]\n",
            "iteration: (453/1000) - error: [-0.09608783 -0.04656113  0.00296556  0.05249226] - loss: 0.0017706301743769574 - gradient - [-0.02179779  0.00741391]\n",
            "iteration: (454/1000) - error: [-0.09594399 -0.04649143  0.00296112  0.05241368] - loss: 0.0017653330477180448 - gradient - [-0.02176516  0.00740281]\n",
            "iteration: (455/1000) - error: [-0.09580037 -0.04642184  0.00295669  0.05233522] - loss: 0.0017600517682706417 - gradient - [-0.02173257  0.00739173]\n",
            "iteration: (456/1000) - error: [-0.09565696 -0.04635235  0.00295226  0.05225687] - loss: 0.0017547862886252291 - gradient - [-0.02170004  0.00738066]\n",
            "iteration: (457/1000) - error: [-0.09551376 -0.04628296  0.00294784  0.05217865] - loss: 0.001749536561514114 - gradient - [-0.02166756  0.00736961]\n",
            "iteration: (458/1000) - error: [-0.09537078 -0.04621368  0.00294343  0.05210054] - loss: 0.0017443025398110998 - gradient - [-0.02163512  0.00735858]\n",
            "iteration: (459/1000) - error: [-0.09522802 -0.0461445   0.00293903  0.05202255] - loss: 0.0017390841765308816 - gradient - [-0.02160274  0.00734756]\n",
            "iteration: (460/1000) - error: [-0.09508547 -0.04607542  0.00293463  0.05194467] - loss: 0.0017338814248287495 - gradient - [-0.0215704   0.00733657]\n",
            "iteration: (461/1000) - error: [-0.09494313 -0.04600645  0.00293023  0.05186691] - loss: 0.0017286942380001764 - gradient - [-0.02153811  0.00732558]\n",
            "iteration: (462/1000) - error: [-0.094801   -0.04593758  0.00292585  0.05178927] - loss: 0.0017235225694802647 - gradient - [-0.02150587  0.00731462]\n",
            "iteration: (463/1000) - error: [-0.09465909 -0.04586881  0.00292147  0.05171175] - loss: 0.0017183663728435375 - gradient - [-0.02147367  0.00730367]\n",
            "iteration: (464/1000) - error: [-0.09451739 -0.04580015  0.00291709  0.05163434] - loss: 0.0017132256018033286 - gradient - [-0.02144153  0.00729273]\n",
            "iteration: (465/1000) - error: [-0.0943759  -0.04573159  0.00291273  0.05155704] - loss: 0.0017081002102114877 - gradient - [-0.02140943  0.00728182]\n",
            "iteration: (466/1000) - error: [-0.09423463 -0.04566313  0.00290837  0.05147986] - loss: 0.001702990152057898 - gradient - [-0.02137738  0.00727092]\n",
            "iteration: (467/1000) - error: [-0.09409356 -0.04559477  0.00290401  0.0514028 ] - loss: 0.0016978953814700683 - gradient - [-0.02134538  0.00726003]\n",
            "iteration: (468/1000) - error: [-0.09395271 -0.04552652  0.00289967  0.05132585] - loss: 0.001692815852712815 - gradient - [-0.02131343  0.00724916]\n",
            "iteration: (469/1000) - error: [-0.09381207 -0.04545837  0.00289533  0.05124902] - loss: 0.0016877515201877131 - gradient - [-0.02128152  0.00723831]\n",
            "iteration: (470/1000) - error: [-0.09367163 -0.04539032  0.00289099  0.0511723 ] - loss: 0.001682702338432798 - gradient - [-0.02124967  0.00722748]\n",
            "iteration: (471/1000) - error: [-0.09353141 -0.04532237  0.00288666  0.0510957 ] - loss: 0.0016776682621220391 - gradient - [-0.02121786  0.00721666]\n",
            "iteration: (472/1000) - error: [-0.0933914  -0.04525453  0.00288234  0.05101921] - loss: 0.0016726492460651108 - gradient - [-0.02118609  0.00720586]\n",
            "iteration: (473/1000) - error: [-0.0932516  -0.04518679  0.00287803  0.05094284] - loss: 0.0016676452452068088 - gradient - [-0.02115438  0.00719507]\n",
            "iteration: (474/1000) - error: [-0.093112   -0.04511914  0.00287372  0.05086658] - loss: 0.0016626562146267317 - gradient - [-0.02112271  0.0071843 ]\n",
            "iteration: (475/1000) - error: [-0.09297262 -0.0450516   0.00286942  0.05079044] - loss: 0.0016576821095388918 - gradient - [-0.02109109  0.00717354]\n",
            "iteration: (476/1000) - error: [-0.09283344 -0.04498416  0.00286512  0.05071441] - loss: 0.0016527228852912003 - gradient - [-0.02105952  0.0071628 ]\n",
            "iteration: (477/1000) - error: [-0.09269448 -0.04491682  0.00286083  0.05063849] - loss: 0.0016477784973652986 - gradient - [-0.02102799  0.00715208]\n",
            "iteration: (478/1000) - error: [-0.09255572 -0.04484958  0.00285655  0.05056268] - loss: 0.001642848901375883 - gradient - [-0.02099652  0.00714138]\n",
            "iteration: (479/1000) - error: [-0.09241717 -0.04478245  0.00285227  0.050487  ] - loss: 0.0016379340530704787 - gradient - [-0.02096509  0.00713069]\n",
            "iteration: (480/1000) - error: [-0.09227882 -0.04471541  0.002848    0.05041142] - loss: 0.0016330339083290007 - gradient - [-0.0209337   0.00712001]\n",
            "iteration: (481/1000) - error: [-0.09214069 -0.04464847  0.00284374  0.05033596] - loss: 0.001628148423163401 - gradient - [-0.02090237  0.00710935]\n",
            "iteration: (482/1000) - error: [-0.09200276 -0.04458164  0.00283948  0.0502606 ] - loss: 0.0016232775537171381 - gradient - [-0.02087108  0.00709871]\n",
            "iteration: (483/1000) - error: [-0.09186503 -0.0445149   0.00283523  0.05018537] - loss: 0.001618421256264955 - gradient - [-0.02083983  0.00708808]\n",
            "iteration: (484/1000) - error: [-0.09172751 -0.04444826  0.00283099  0.05011024] - loss: 0.0016135794872123477 - gradient - [-0.02080864  0.00707747]\n",
            "iteration: (485/1000) - error: [-0.0915902  -0.04438173  0.00282675  0.05003523] - loss: 0.001608752203095282 - gradient - [-0.02077749  0.00706688]\n",
            "iteration: (486/1000) - error: [-0.0914531  -0.04431529  0.00282252  0.04996033] - loss: 0.0016039393605797156 - gradient - [-0.02074638  0.0070563 ]\n",
            "iteration: (487/1000) - error: [-0.0913162  -0.04424895  0.0028183   0.04988554] - loss: 0.001599140916461283 - gradient - [-0.02071533  0.00704574]\n",
            "iteration: (488/1000) - error: [-0.0911795  -0.04418271  0.00281408  0.04981086] - loss: 0.0015943568276648142 - gradient - [-0.02068432  0.00703519]\n",
            "iteration: (489/1000) - error: [-0.09104301 -0.04411657  0.00280986  0.0497363 ] - loss: 0.0015895870512440582 - gradient - [-0.02065335  0.00702466]\n",
            "iteration: (490/1000) - error: [-0.09090672 -0.04405053  0.00280566  0.04966185] - loss: 0.001584831544381216 - gradient - [-0.02062244  0.00701414]\n",
            "iteration: (491/1000) - error: [-0.09077064 -0.04398459  0.00280146  0.04958751] - loss: 0.0015800902643866273 - gradient - [-0.02059157  0.00700364]\n",
            "iteration: (492/1000) - error: [-0.09063476 -0.04391875  0.00279726  0.04951328] - loss: 0.0015753631686982743 - gradient - [-0.02056074  0.00699316]\n",
            "iteration: (493/1000) - error: [-0.09049908 -0.043853    0.00279308  0.04943916] - loss: 0.001570650214881545 - gradient - [-0.02052996  0.00698269]\n",
            "iteration: (494/1000) - error: [-0.09036361 -0.04378736  0.0027889   0.04936515] - loss: 0.001565951360628702 - gradient - [-0.02049923  0.00697224]\n",
            "iteration: (495/1000) - error: [-0.09022834 -0.04372181  0.00278472  0.04929125] - loss: 0.0015612665637586628 - gradient - [-0.02046854  0.0069618 ]\n",
            "iteration: (496/1000) - error: [-0.09009327 -0.04365636  0.00278055  0.04921747] - loss: 0.0015565957822164579 - gradient - [-0.0204379   0.00695138]\n",
            "iteration: (497/1000) - error: [-0.08995841 -0.04359101  0.00277639  0.04914379] - loss: 0.001551938974072987 - gradient - [-0.02040731  0.00694097]\n",
            "iteration: (498/1000) - error: [-0.08982375 -0.04352576  0.00277223  0.04907022] - loss: 0.0015472960975245584 - gradient - [-0.02037676  0.00693058]\n",
            "iteration: (499/1000) - error: [-0.08968928 -0.0434606   0.00276808  0.04899677] - loss: 0.0015426671108925566 - gradient - [-0.02034626  0.00692021]\n",
            "iteration: (500/1000) - error: [-0.08955502 -0.04339554  0.00276394  0.04892342] - loss: 0.0015380519726230389 - gradient - [-0.0203158   0.00690985]\n",
            "iteration: (501/1000) - error: [-0.08942096 -0.04333058  0.0027598   0.04885019] - loss: 0.0015334506412864038 - gradient - [-0.02028539  0.00689951]\n",
            "iteration: (502/1000) - error: [-0.0892871  -0.04326572  0.00275567  0.04877706] - loss: 0.0015288630755769758 - gradient - [-0.02025502  0.00688918]\n",
            "iteration: (503/1000) - error: [-0.08915345 -0.04320095  0.00275155  0.04870404] - loss: 0.0015242892343126282 - gradient - [-0.0202247   0.00687887]\n",
            "iteration: (504/1000) - error: [-0.08901999 -0.04313628  0.00274743  0.04863113] - loss: 0.0015197290764344825 - gradient - [-0.02019443  0.00686857]\n",
            "iteration: (505/1000) - error: [-0.08888673 -0.04307171  0.00274331  0.04855834] - loss: 0.0015151825610064985 - gradient - [-0.0201642   0.00685829]\n",
            "iteration: (506/1000) - error: [-0.08875367 -0.04300723  0.00273921  0.04848565] - loss: 0.001510649647215035 - gradient - [-0.02013401  0.00684802]\n",
            "iteration: (507/1000) - error: [-0.08862081 -0.04294285  0.00273511  0.04841307] - loss: 0.001506130294368622 - gradient - [-0.02010387  0.00683777]\n",
            "iteration: (508/1000) - error: [-0.08848815 -0.04287857  0.00273101  0.04834059] - loss: 0.0015016244618974843 - gradient - [-0.02007378  0.00682753]\n",
            "iteration: (509/1000) - error: [-0.08835569 -0.04281438  0.00272692  0.04826823] - loss: 0.0014971321093532356 - gradient - [-0.02004373  0.00681731]\n",
            "iteration: (510/1000) - error: [-0.08822342 -0.04275029  0.00272284  0.04819598] - loss: 0.0014926531964084965 - gradient - [-0.02001372  0.00680711]\n",
            "iteration: (511/1000) - error: [-0.08809136 -0.0426863   0.00271877  0.04812383] - loss: 0.0014881876828565497 - gradient - [-0.01998376  0.00679692]\n",
            "iteration: (512/1000) - error: [-0.08795949 -0.0426224   0.0027147   0.04805179] - loss: 0.0014837355286109013 - gradient - [-0.01995385  0.00678674]\n",
            "iteration: (513/1000) - error: [-0.08782782 -0.04255859  0.00271063  0.04797986] - loss: 0.0014792966937050701 - gradient - [-0.01992398  0.00677658]\n",
            "iteration: (514/1000) - error: [-0.08769634 -0.04249488  0.00270658  0.04790803] - loss: 0.001474871138292036 - gradient - [-0.01989415  0.00676644]\n",
            "iteration: (515/1000) - error: [-0.08756507 -0.04243127  0.00270252  0.04783632] - loss: 0.0014704588226441277 - gradient - [-0.01986437  0.00675631]\n",
            "iteration: (516/1000) - error: [-0.08743399 -0.04236775  0.00269848  0.04776471] - loss: 0.0014660597071523716 - gradient - [-0.01983464  0.0067462 ]\n",
            "iteration: (517/1000) - error: [-0.0873031  -0.04230433  0.00269444  0.04769321] - loss: 0.0014616737523264233 - gradient - [-0.01980495  0.0067361 ]\n",
            "iteration: (518/1000) - error: [-0.08717241 -0.042241    0.00269041  0.04762181] - loss: 0.0014573009187939965 - gradient - [-0.0197753   0.00672601]\n",
            "iteration: (519/1000) - error: [-0.08704192 -0.04217777  0.00268638  0.04755053] - loss: 0.0014529411673005971 - gradient - [-0.0197457   0.00671594]\n",
            "iteration: (520/1000) - error: [-0.08691162 -0.04211463  0.00268236  0.04747935] - loss: 0.0014485944587092055 - gradient - [-0.01971614  0.00670589]\n",
            "iteration: (521/1000) - error: [-0.08678152 -0.04205159  0.00267834  0.04740827] - loss: 0.0014442607539998918 - gradient - [-0.01968662  0.00669585]\n",
            "iteration: (522/1000) - error: [-0.08665161 -0.04198864  0.00267433  0.0473373 ] - loss: 0.0014399400142694073 - gradient - [-0.01965715  0.00668583]\n",
            "iteration: (523/1000) - error: [-0.0865219  -0.04192579  0.00267033  0.04726644] - loss: 0.0014356322007309428 - gradient - [-0.01962773  0.00667582]\n",
            "iteration: (524/1000) - error: [-0.08639238 -0.04186302  0.00266633  0.04719569] - loss: 0.0014313372747136964 - gradient - [-0.01959835  0.00666583]\n",
            "iteration: (525/1000) - error: [-0.08626305 -0.04180036  0.00266234  0.04712504] - loss: 0.0014270551976625267 - gradient - [-0.01956901  0.00665585]\n",
            "iteration: (526/1000) - error: [-0.08613392 -0.04173778  0.00265835  0.04705449] - loss: 0.0014227859311377543 - gradient - [-0.01953971  0.00664589]\n",
            "iteration: (527/1000) - error: [-0.08600498 -0.0416753   0.00265437  0.04698405] - loss: 0.001418529436814536 - gradient - [-0.01951046  0.00663594]\n",
            "iteration: (528/1000) - error: [-0.08587624 -0.04161292  0.0026504   0.04691372] - loss: 0.0014142856764827977 - gradient - [-0.01948126  0.006626  ]\n",
            "iteration: (529/1000) - error: [-0.08574769 -0.04155063  0.00264643  0.04684349] - loss: 0.001410054612046757 - gradient - [-0.0194521   0.00661608]\n",
            "iteration: (530/1000) - error: [-0.08561933 -0.04148843  0.00264247  0.04677337] - loss: 0.001405836205524559 - gradient - [-0.01942298  0.00660618]\n",
            "iteration: (531/1000) - error: [-0.08549116 -0.04142632  0.00263852  0.04670335] - loss: 0.0014016304190480227 - gradient - [-0.0193939   0.00659629]\n",
            "iteration: (532/1000) - error: [-0.08536318 -0.04136431  0.00263457  0.04663344] - loss: 0.0013974372148622587 - gradient - [-0.01936487  0.00658642]\n",
            "iteration: (533/1000) - error: [-0.0852354  -0.04130239  0.00263062  0.04656363] - loss: 0.0013932565553252555 - gradient - [-0.01933588  0.00657656]\n",
            "iteration: (534/1000) - error: [-0.0851078  -0.04124056  0.00262669  0.04649393] - loss: 0.0013890884029077134 - gradient - [-0.01930694  0.00656671]\n",
            "iteration: (535/1000) - error: [-0.0849804  -0.04117882  0.00262275  0.04642433] - loss: 0.0013849327201925267 - gradient - [-0.01927804  0.00655688]\n",
            "iteration: (536/1000) - error: [-0.08485319 -0.04111718  0.00261883  0.04635484] - loss: 0.0013807894698746126 - gradient - [-0.01924918  0.00654707]\n",
            "iteration: (537/1000) - error: [-0.08472617 -0.04105563  0.00261491  0.04628545] - loss: 0.0013766586147604162 - gradient - [-0.01922036  0.00653727]\n",
            "iteration: (538/1000) - error: [-0.08459934 -0.04099417  0.00261099  0.04621616] - loss: 0.0013725401177676383 - gradient - [-0.01919159  0.00652748]\n",
            "iteration: (539/1000) - error: [-0.0844727  -0.04093281  0.00260708  0.04614697] - loss: 0.0013684339419250265 - gradient - [-0.01916286  0.00651771]\n",
            "iteration: (540/1000) - error: [-0.08434625 -0.04087153  0.00260318  0.0460779 ] - loss: 0.0013643400503718457 - gradient - [-0.01913418  0.00650795]\n",
            "iteration: (541/1000) - error: [-0.08421998 -0.04081035  0.00259928  0.04600892] - loss: 0.0013602584063576177 - gradient - [-0.01910553  0.00649821]\n",
            "iteration: (542/1000) - error: [-0.08409391 -0.04074926  0.00259539  0.04594005] - loss: 0.001356188973241897 - gradient - [-0.01907693  0.00648848]\n",
            "iteration: (543/1000) - error: [-0.08396803 -0.04068826  0.00259151  0.04587128] - loss: 0.0013521317144937865 - gradient - [-0.01904838  0.00647877]\n",
            "iteration: (544/1000) - error: [-0.08384233 -0.04062735  0.00258763  0.04580261] - loss: 0.001348086593691671 - gradient - [-0.01901986  0.00646907]\n",
            "iteration: (545/1000) - error: [-0.08371682 -0.04056653  0.00258376  0.04573404] - loss: 0.0013440535745229713 - gradient - [-0.01899139  0.00645939]\n",
            "iteration: (546/1000) - error: [-0.0835915  -0.04050581  0.00257989  0.04566558] - loss: 0.0013400326207836676 - gradient - [-0.01896296  0.00644972]\n",
            "iteration: (547/1000) - error: [-0.08346637 -0.04044517  0.00257603  0.04559722] - loss: 0.0013360236963780533 - gradient - [-0.01893457  0.00644006]\n",
            "iteration: (548/1000) - error: [-0.08334142 -0.04038463  0.00257217  0.04552897] - loss: 0.0013320267653184777 - gradient - [-0.01890623  0.00643042]\n",
            "iteration: (549/1000) - error: [-0.08321667 -0.04032417  0.00256832  0.04546081] - loss: 0.0013280417917248685 - gradient - [-0.01887793  0.0064208 ]\n",
            "iteration: (550/1000) - error: [-0.0830921  -0.04026381  0.00256447  0.04539276] - loss: 0.0013240687398245407 - gradient - [-0.01884967  0.00641119]\n",
            "iteration: (551/1000) - error: [-0.08296771 -0.04020354  0.00256064  0.04532481] - loss: 0.0013201075739518295 - gradient - [-0.01882145  0.00640159]\n",
            "iteration: (552/1000) - error: [-0.08284351 -0.04014335  0.0025568   0.04525696] - loss: 0.0013161582585477316 - gradient - [-0.01879328  0.00639201]\n",
            "iteration: (553/1000) - error: [-0.0827195  -0.04008326  0.00255297  0.04518921] - loss: 0.0013122207581597021 - gradient - [-0.01876514  0.00638244]\n",
            "iteration: (554/1000) - error: [-0.08259567 -0.04002326  0.00254915  0.04512157] - loss: 0.0013082950374411766 - gradient - [-0.01873705  0.00637288]\n",
            "iteration: (555/1000) - error: [-0.08247203 -0.03996335  0.00254534  0.04505402] - loss: 0.0013043810611513594 - gradient - [-0.018709    0.00636334]\n",
            "iteration: (556/1000) - error: [-0.08234857 -0.03990352  0.00254153  0.04498658] - loss: 0.0013004787941549162 - gradient - [-0.018681    0.00635382]\n",
            "iteration: (557/1000) - error: [-0.0822253  -0.03984379  0.00253772  0.04491923] - loss: 0.0012965882014215617 - gradient - [-0.01865303  0.00634431]\n",
            "iteration: (558/1000) - error: [-0.08210221 -0.03978415  0.00253392  0.04485199] - loss: 0.0012927092480258888 - gradient - [-0.01862511  0.00633481]\n",
            "iteration: (559/1000) - error: [-0.08197931 -0.03972459  0.00253013  0.04478485] - loss: 0.0012888418991469496 - gradient - [-0.01859723  0.00632533]\n",
            "iteration: (560/1000) - error: [-0.08185659 -0.03966512  0.00252634  0.04471781] - loss: 0.0012849861200679032 - gradient - [-0.01856939  0.00631586]\n",
            "iteration: (561/1000) - error: [-0.08173406 -0.03960575  0.00252256  0.04465087] - loss: 0.001281141876175855 - gradient - [-0.01854159  0.0063064 ]\n",
            "iteration: (562/1000) - error: [-0.08161171 -0.03954646  0.00251879  0.04458403] - loss: 0.0012773091329614046 - gradient - [-0.01851384  0.00629696]\n",
            "iteration: (563/1000) - error: [-0.08148954 -0.03948726  0.00251501  0.04451729] - loss: 0.00127348785601844 - gradient - [-0.01848612  0.00628754]\n",
            "iteration: (564/1000) - error: [-0.08136755 -0.03942815  0.00251125  0.04445065] - loss: 0.00126967801104375 - gradient - [-0.01845845  0.00627812]\n",
            "iteration: (565/1000) - error: [-0.08124575 -0.03936913  0.00250749  0.04438411] - loss: 0.0012658795638367316 - gradient - [-0.01843082  0.00626873]\n",
            "iteration: (566/1000) - error: [-0.08112413 -0.0393102   0.00250374  0.04431767] - loss: 0.0012620924802991165 - gradient - [-0.01840323  0.00625934]\n",
            "iteration: (567/1000) - error: [-0.08100269 -0.03925135  0.00249999  0.04425133] - loss: 0.001258316726434699 - gradient - [-0.01837568  0.00624997]\n",
            "iteration: (568/1000) - error: [-0.08088143 -0.03919259  0.00249625  0.04418509] - loss: 0.001254552268348866 - gradient - [-0.01834817  0.00624062]\n",
            "iteration: (569/1000) - error: [-0.08076036 -0.03913392  0.00249251  0.04411894] - loss: 0.0012507990722484934 - gradient - [-0.01832071  0.00623127]\n",
            "iteration: (570/1000) - error: [-0.08063946 -0.03907534  0.00248878  0.0440529 ] - loss: 0.0012470571044415327 - gradient - [-0.01829328  0.00622195]\n",
            "iteration: (571/1000) - error: [-0.08051875 -0.03901685  0.00248505  0.04398695] - loss: 0.0012433263313367366 - gradient - [-0.0182659   0.00621263]\n",
            "iteration: (572/1000) - error: [-0.08039822 -0.03895844  0.00248133  0.04392111] - loss: 0.0012396067194433296 - gradient - [-0.01823855  0.00620333]\n",
            "iteration: (573/1000) - error: [-0.08027786 -0.03890012  0.00247762  0.04385536] - loss: 0.0012358982353707386 - gradient - [-0.01821125  0.00619405]\n",
            "iteration: (574/1000) - error: [-0.08015769 -0.03884189  0.00247391  0.04378971] - loss: 0.0012322008458283003 - gradient - [-0.01818399  0.00618477]\n",
            "iteration: (575/1000) - error: [-0.0800377  -0.03878375  0.00247021  0.04372416] - loss: 0.0012285145176249414 - gradient - [-0.01815677  0.00617552]\n",
            "iteration: (576/1000) - error: [-0.07991789 -0.03872569  0.00246651  0.04365871] - loss: 0.0012248392176688165 - gradient - [-0.01812959  0.00616627]\n",
            "iteration: (577/1000) - error: [-0.07979825 -0.03866772  0.00246282  0.04359335] - loss: 0.0012211749129672185 - gradient - [-0.01810245  0.00615704]\n",
            "iteration: (578/1000) - error: [-0.0796788  -0.03860983  0.00245913  0.04352809] - loss: 0.0012175215706260135 - gradient - [-0.01807535  0.00614782]\n",
            "iteration: (579/1000) - error: [-0.07955952 -0.03855204  0.00245545  0.04346293] - loss: 0.0012138791578495554 - gradient - [-0.01804829  0.00613862]\n",
            "iteration: (580/1000) - error: [-0.07944043 -0.03849433  0.00245177  0.04339787] - loss: 0.0012102476419402607 - gradient - [-0.01802128  0.00612943]\n",
            "iteration: (581/1000) - error: [-0.07932151 -0.0384367   0.0024481   0.04333291] - loss: 0.0012066269902983977 - gradient - [-0.0179943   0.00612026]\n",
            "iteration: (582/1000) - error: [-0.07920277 -0.03837916  0.00244444  0.04326804] - loss: 0.0012030171704217332 - gradient - [-0.01796736  0.00611109]\n",
            "iteration: (583/1000) - error: [-0.07908421 -0.03832171  0.00244078  0.04320327] - loss: 0.001199418149905326 - gradient - [-0.01794047  0.00610195]\n",
            "iteration: (584/1000) - error: [-0.07896582 -0.03826435  0.00243712  0.0431386 ] - loss: 0.001195829896441121 - gradient - [-0.01791361  0.00609281]\n",
            "iteration: (585/1000) - error: [-0.07884761 -0.03820707  0.00243348  0.04307402] - loss: 0.0011922523778177358 - gradient - [-0.0178868   0.00608369]\n",
            "iteration: (586/1000) - error: [-0.07872958 -0.03814987  0.00242983  0.04300954] - loss: 0.0011886855619201808 - gradient - [-0.01786002  0.00607458]\n",
            "iteration: (587/1000) - error: [-0.07861173 -0.03809277  0.0024262   0.04294516] - loss: 0.0011851294167294893 - gradient - [-0.01783328  0.00606549]\n",
            "iteration: (588/1000) - error: [-0.07849405 -0.03803574  0.00242256  0.04288087] - loss: 0.001181583910322534 - gradient - [-0.01780659  0.00605641]\n",
            "iteration: (589/1000) - error: [-0.07837655 -0.0379788   0.00241894  0.04281668] - loss: 0.00117804901087166 - gradient - [-0.01777993  0.00604735]\n",
            "iteration: (590/1000) - error: [-0.07825922 -0.03792195  0.00241532  0.04275259] - loss: 0.0011745246866444584 - gradient - [-0.01775332  0.00603829]\n",
            "iteration: (591/1000) - error: [-0.07814207 -0.03786518  0.0024117   0.04268859] - loss: 0.0011710109060034104 - gradient - [-0.01772674  0.00602925]\n",
            "iteration: (592/1000) - error: [-0.0780251  -0.0378085   0.00240809  0.04262468] - loss: 0.0011675076374057121 - gradient - [-0.01770021  0.00602023]\n",
            "iteration: (593/1000) - error: [-0.0779083  -0.0377519   0.00240449  0.04256088] - loss: 0.001164014849402845 - gradient - [-0.01767371  0.00601122]\n",
            "iteration: (594/1000) - error: [-0.07779167 -0.03769539  0.00240089  0.04249717] - loss: 0.0011605325106404348 - gradient - [-0.01764725  0.00600222]\n",
            "iteration: (595/1000) - error: [-0.07767522 -0.03763896  0.00239729  0.04243355] - loss: 0.0011570605898578706 - gradient - [-0.01762084  0.00599323]\n",
            "iteration: (596/1000) - error: [-0.07755894 -0.03758262  0.0023937   0.04237003] - loss: 0.0011535990558881064 - gradient - [-0.01759446  0.00598426]\n",
            "iteration: (597/1000) - error: [-0.07744284 -0.03752636  0.00239012  0.0423066 ] - loss: 0.0011501478776572913 - gradient - [-0.01756812  0.0059753 ]\n",
            "iteration: (598/1000) - error: [-0.07732691 -0.03747019  0.00238654  0.04224327] - loss: 0.00114670702418456 - gradient - [-0.01754182  0.00596636]\n",
            "iteration: (599/1000) - error: [-0.07721116 -0.03741409  0.00238297  0.04218004] - loss: 0.0011432764645817463 - gradient - [-0.01751556  0.00595743]\n",
            "iteration: (600/1000) - error: [-0.07709558 -0.03735809  0.0023794   0.04211689] - loss: 0.001139856168053035 - gradient - [-0.01748934  0.00594851]\n",
            "iteration: (601/1000) - error: [-0.07698017 -0.03730216  0.00237584  0.04205385] - loss: 0.0011364461038948363 - gradient - [-0.01746316  0.0059396 ]\n",
            "iteration: (602/1000) - error: [-0.07686493 -0.03724632  0.00237229  0.0419909 ] - loss: 0.0011330462414953033 - gradient - [-0.01743702  0.00593071]\n",
            "iteration: (603/1000) - error: [-0.07674987 -0.03719057  0.00236873  0.04192804] - loss: 0.0011296565503342228 - gradient - [-0.01741092  0.00592184]\n",
            "iteration: (604/1000) - error: [-0.07663498 -0.0371349   0.00236519  0.04186527] - loss: 0.001126276999982731 - gradient - [-0.01738485  0.00591297]\n",
            "iteration: (605/1000) - error: [-0.07652026 -0.03707931  0.00236165  0.0418026 ] - loss: 0.0011229075601029463 - gradient - [-0.01735883  0.00590412]\n",
            "iteration: (606/1000) - error: [-0.07640571 -0.0370238   0.00235811  0.04174003] - loss: 0.001119548200447727 - gradient - [-0.01733284  0.00589528]\n",
            "iteration: (607/1000) - error: [-0.07629134 -0.03696838  0.00235458  0.04167754] - loss: 0.0011161988908604782 - gradient - [-0.0173069   0.00588646]\n",
            "iteration: (608/1000) - error: [-0.07617713 -0.03691304  0.00235106  0.04161515] - loss: 0.001112859601274792 - gradient - [-0.01728099  0.00587764]\n",
            "iteration: (609/1000) - error: [-0.0760631  -0.03685778  0.00234754  0.04155286] - loss: 0.0011095303017141856 - gradient - [-0.01725512  0.00586885]\n",
            "iteration: (610/1000) - error: [-0.07594924 -0.03680261  0.00234402  0.04149066] - loss: 0.001106210962291928 - gradient - [-0.01722929  0.00586006]\n",
            "iteration: (611/1000) - error: [-0.07583555 -0.03674752  0.00234052  0.04142855] - loss: 0.0011029015532106115 - gradient - [-0.0172035   0.00585129]\n",
            "iteration: (612/1000) - error: [-0.07572202 -0.03669251  0.00233701  0.04136653] - loss: 0.0010996020447620284 - gradient - [-0.01717775  0.00584253]\n",
            "iteration: (613/1000) - error: [-0.07560867 -0.03663758  0.00233351  0.04130461] - loss: 0.0010963124073268458 - gradient - [-0.01715203  0.00583378]\n",
            "iteration: (614/1000) - error: [-0.07549549 -0.03658273  0.00233002  0.04124277] - loss: 0.0010930326113743369 - gradient - [-0.01712636  0.00582505]\n",
            "iteration: (615/1000) - error: [-0.07538248 -0.03652797  0.00232653  0.04118104] - loss: 0.0010897626274620847 - gradient - [-0.01710072  0.00581633]\n",
            "iteration: (616/1000) - error: [-0.07526963 -0.03647329  0.00232305  0.04111939] - loss: 0.0010865024262358045 - gradient - [-0.01707512  0.00580762]\n",
            "iteration: (617/1000) - error: [-0.07515696 -0.03641869  0.00231957  0.04105784] - loss: 0.0010832519784289894 - gradient - [-0.01704956  0.00579893]\n",
            "iteration: (618/1000) - error: [-0.07504445 -0.03636418  0.0023161   0.04099638] - loss: 0.0010800112548627267 - gradient - [-0.01702404  0.00579025]\n",
            "iteration: (619/1000) - error: [-0.07493211 -0.03630974  0.00231263  0.04093501] - loss: 0.0010767802264453567 - gradient - [-0.01699855  0.00578158]\n",
            "iteration: (620/1000) - error: [-0.07481994 -0.03625539  0.00230917  0.04087373] - loss: 0.0010735588641722833 - gradient - [-0.01697311  0.00577293]\n",
            "iteration: (621/1000) - error: [-0.07470794 -0.03620111  0.00230571  0.04081254] - loss: 0.0010703471391256987 - gradient - [-0.0169477   0.00576428]\n",
            "iteration: (622/1000) - error: [-0.07459611 -0.03614692  0.00230226  0.04075145] - loss: 0.0010671450224742525 - gradient - [-0.01692233  0.00575566]\n",
            "iteration: (623/1000) - error: [-0.07448444 -0.03609281  0.00229882  0.04069044] - loss: 0.001063952485472848 - gradient - [-0.016897    0.00574704]\n",
            "iteration: (624/1000) - error: [-0.07437294 -0.03603878  0.00229537  0.04062953] - loss: 0.001060769499462483 - gradient - [-0.0168717   0.00573844]\n",
            "iteration: (625/1000) - error: [-0.07426161 -0.03598484  0.00229194  0.04056871] - loss: 0.001057596035869746 - gradient - [-0.01684645  0.00572985]\n",
            "iteration: (626/1000) - error: [-0.07415044 -0.03593097  0.00228851  0.04050798] - loss: 0.0010544320662068487 - gradient - [-0.01682123  0.00572127]\n",
            "iteration: (627/1000) - error: [-0.07403944 -0.03587718  0.00228508  0.04044734] - loss: 0.0010512775620711346 - gradient - [-0.01679605  0.0057127 ]\n",
            "iteration: (628/1000) - error: [-0.07392861 -0.03582347  0.00228166  0.0403868 ] - loss: 0.0010481324951449483 - gradient - [-0.01677091  0.00570415]\n",
            "iteration: (629/1000) - error: [-0.07381794 -0.03576985  0.00227825  0.04032634] - loss: 0.0010449968371953346 - gradient - [-0.0167458   0.00569561]\n",
            "iteration: (630/1000) - error: [-0.07370744 -0.0357163   0.00227484  0.04026597] - loss: 0.0010418705600738491 - gradient - [-0.01672073  0.00568709]\n",
            "iteration: (631/1000) - error: [-0.0735971  -0.03566284  0.00227143  0.0402057 ] - loss: 0.0010387536357162092 - gradient - [-0.0166957   0.00567858]\n",
            "iteration: (632/1000) - error: [-0.07348693 -0.03560945  0.00226803  0.04014551] - loss: 0.00103564603614209 - gradient - [-0.01667071  0.00567007]\n",
            "iteration: (633/1000) - error: [-0.07337693 -0.03555615  0.00226463  0.04008542] - loss: 0.0010325477334549402 - gradient - [-0.01664576  0.00566159]\n",
            "iteration: (634/1000) - error: [-0.07326708 -0.03550292  0.00226124  0.04002541] - loss: 0.0010294586998415786 - gradient - [-0.01662084  0.00565311]\n",
            "iteration: (635/1000) - error: [-0.07315741 -0.03544977  0.00225786  0.03996549] - loss: 0.0010263789075721037 - gradient - [-0.01659596  0.00564465]\n",
            "iteration: (636/1000) - error: [-0.07304789 -0.03539671  0.00225448  0.03990567] - loss: 0.001023308328999498 - gradient - [-0.01657111  0.0056362 ]\n",
            "iteration: (637/1000) - error: [-0.07293854 -0.03534372  0.0022511   0.03984593] - loss: 0.0010202469365595199 - gradient - [-0.01654631  0.00562776]\n",
            "iteration: (638/1000) - error: [-0.07282936 -0.03529081  0.00224774  0.03978628] - loss: 0.0010171947027703835 - gradient - [-0.01652154  0.00561934]\n",
            "iteration: (639/1000) - error: [-0.07272034 -0.03523798  0.00224437  0.03972672] - loss: 0.00101415160023248 - gradient - [-0.01649681  0.00561093]\n",
            "iteration: (640/1000) - error: [-0.07261148 -0.03518523  0.00224101  0.03966726] - loss: 0.0010111176016281821 - gradient - [-0.01647211  0.00560253]\n",
            "iteration: (641/1000) - error: [-0.07250278 -0.03513256  0.00223766  0.03960788] - loss: 0.001008092679721648 - gradient - [-0.01644745  0.00559414]\n",
            "iteration: (642/1000) - error: [-0.07239425 -0.03507997  0.00223431  0.03954858] - loss: 0.0010050768073584084 - gradient - [-0.01642283  0.00558577]\n",
            "iteration: (643/1000) - error: [-0.07228588 -0.03502746  0.00223096  0.03948938] - loss: 0.0010020699574653385 - gradient - [-0.01639825  0.0055774 ]\n",
            "iteration: (644/1000) - error: [-0.07217767 -0.03497502  0.00222762  0.03943027] - loss: 0.0009990721030502443 - gradient - [-0.0163737   0.00556906]\n",
            "iteration: (645/1000) - error: [-0.07206962 -0.03492267  0.00222429  0.03937124] - loss: 0.000996083217201667 - gradient - [-0.01634919  0.00556072]\n",
            "iteration: (646/1000) - error: [-0.07196174 -0.03487039  0.00222096  0.03931231] - loss: 0.0009931032730887447 - gradient - [-0.01632472  0.00555239]\n",
            "iteration: (647/1000) - error: [-0.07185402 -0.03481819  0.00221763  0.03925346] - loss: 0.000990132243960768 - gradient - [-0.01630028  0.00554408]\n",
            "iteration: (648/1000) - error: [-0.07174645 -0.03476607  0.00221431  0.0391947 ] - loss: 0.0009871701031471684 - gradient - [-0.01627588  0.00553578]\n",
            "iteration: (649/1000) - error: [-0.07163905 -0.03471403  0.002211    0.03913602] - loss: 0.0009842168240570852 - gradient - [-0.01625151  0.0055275 ]\n",
            "iteration: (650/1000) - error: [-0.07153181 -0.03466206  0.00220769  0.03907744] - loss: 0.0009812723801792562 - gradient - [-0.01622719  0.00551922]\n",
            "iteration: (651/1000) - error: [-0.07142473 -0.03461017  0.00220438  0.03901894] - loss: 0.000978336745081703 - gradient - [-0.0162029   0.00551096]\n",
            "iteration: (652/1000) - error: [-0.07131781 -0.03455836  0.00220108  0.03896053] - loss: 0.0009754098924115326 - gradient - [-0.01617864  0.00550271]\n",
            "iteration: (653/1000) - error: [-0.07121105 -0.03450663  0.00219779  0.03890221] - loss: 0.0009724917958946947 - gradient - [-0.01615442  0.00549447]\n",
            "iteration: (654/1000) - error: [-0.07110446 -0.03445498  0.0021945   0.03884398] - loss: 0.0009695824293357541 - gradient - [-0.01613024  0.00548625]\n",
            "iteration: (655/1000) - error: [-0.07099802 -0.0344034   0.00219121  0.03878583] - loss: 0.0009666817666176106 - gradient - [-0.01610609  0.00547804]\n",
            "iteration: (656/1000) - error: [-0.07089173 -0.0343519   0.00218793  0.03872777] - loss: 0.0009637897817013273 - gradient - [-0.01608198  0.00546984]\n",
            "iteration: (657/1000) - error: [-0.07078561 -0.03430048  0.00218466  0.0386698 ] - loss: 0.0009609064486258388 - gradient - [-0.01605791  0.00546165]\n",
            "iteration: (658/1000) - error: [-0.07067965 -0.03424913  0.00218139  0.03861191] - loss: 0.0009580317415077705 - gradient - [-0.01603387  0.00545347]\n",
            "iteration: (659/1000) - error: [-0.07057385 -0.03419786  0.00217812  0.03855411] - loss: 0.0009551656345412051 - gradient - [-0.01600987  0.00544531]\n",
            "iteration: (660/1000) - error: [-0.0704682  -0.03414667  0.00217486  0.03849639] - loss: 0.0009523081019973559 - gradient - [-0.0159859   0.00543716]\n",
            "iteration: (661/1000) - error: [-0.07036271 -0.03409555  0.00217161  0.03843877] - loss: 0.0009494591182244806 - gradient - [-0.01596197  0.00542902]\n",
            "iteration: (662/1000) - error: [-0.07025738 -0.03404451  0.00216836  0.03838123] - loss: 0.0009466186576475286 - gradient - [-0.01593808  0.00542089]\n",
            "iteration: (663/1000) - error: [-0.07015221 -0.03399355  0.00216511  0.03832377] - loss: 0.0009437866947679785 - gradient - [-0.01591422  0.00541278]\n",
            "iteration: (664/1000) - error: [-0.0700472  -0.03394266  0.00216187  0.0382664 ] - loss: 0.0009409632041636072 - gradient - [-0.0158904   0.00540467]\n",
            "iteration: (665/1000) - error: [-0.06994234 -0.03389185  0.00215863  0.03820912] - loss: 0.000938148160488226 - gradient - [-0.01586661  0.00539658]\n",
            "iteration: (666/1000) - error: [-0.06983764 -0.03384112  0.0021554   0.03815192] - loss: 0.000935341538471479 - gradient - [-0.01584286  0.0053885 ]\n",
            "iteration: (667/1000) - error: [-0.0697331  -0.03379046  0.00215218  0.03809481] - loss: 0.0009325433129186083 - gradient - [-0.01581914  0.00538044]\n",
            "iteration: (668/1000) - error: [-0.06962871 -0.03373988  0.00214895  0.03803778] - loss: 0.0009297534587102426 - gradient - [-0.01579546  0.00537238]\n",
            "iteration: (669/1000) - error: [-0.06952448 -0.03368937  0.00214574  0.03798084] - loss: 0.0009269719508021494 - gradient - [-0.01577182  0.00536434]\n",
            "iteration: (670/1000) - error: [-0.0694204  -0.03363894  0.00214252  0.03792399] - loss: 0.0009241987642250058 - gradient - [-0.01574821  0.00535631]\n",
            "iteration: (671/1000) - error: [-0.06931649 -0.03358858  0.00213932  0.03786722] - loss: 0.0009214338740842281 - gradient - [-0.01572463  0.00534829]\n",
            "iteration: (672/1000) - error: [-0.06921272 -0.0335383   0.00213611  0.03781053] - loss: 0.0009186772555596793 - gradient - [-0.01570109  0.00534029]\n",
            "iteration: (673/1000) - error: [-0.06910911 -0.0334881   0.00213292  0.03775393] - loss: 0.0009159288839054702 - gradient - [-0.01567759  0.00533229]\n",
            "iteration: (674/1000) - error: [-0.06900566 -0.03343797  0.00212972  0.03769742] - loss: 0.0009131887344497539 - gradient - [-0.01565412  0.00532431]\n",
            "iteration: (675/1000) - error: [-0.06890236 -0.03338791  0.00212654  0.03764099] - loss: 0.0009104567825944872 - gradient - [-0.01563069  0.00531634]\n",
            "iteration: (676/1000) - error: [-0.06879922 -0.03333793  0.00212335  0.03758464] - loss: 0.0009077330038152502 - gradient - [-0.01560729  0.00530838]\n",
            "iteration: (677/1000) - error: [-0.06869623 -0.03328803  0.00212017  0.03752838] - loss: 0.0009050173736609415 - gradient - [-0.01558393  0.00530044]\n",
            "iteration: (678/1000) - error: [-0.0685934 -0.0332382  0.002117   0.0374722] - loss: 0.0009023098677536289 - gradient - [-0.0155606  0.0052925]\n",
            "iteration: (679/1000) - error: [-0.06849071 -0.03318844  0.00211383  0.0374161 ] - loss: 0.0008996104617883225 - gradient - [-0.01553731  0.00528458]\n",
            "iteration: (680/1000) - error: [-0.06838819 -0.03313876  0.00211067  0.03736009] - loss: 0.0008969191315327332 - gradient - [-0.01551405  0.00527667]\n",
            "iteration: (681/1000) - error: [-0.06828581 -0.03308915  0.00210751  0.03730417] - loss: 0.0008942358528270707 - gradient - [-0.01549082  0.00526877]\n",
            "iteration: (682/1000) - error: [-0.06818359 -0.03303962  0.00210435  0.03724833] - loss: 0.000891560601583802 - gradient - [-0.01546763  0.00526088]\n",
            "iteration: (683/1000) - error: [-0.06808153 -0.03299016  0.0021012   0.03719257] - loss: 0.0008888933537874978 - gradient - [-0.01544448  0.00525301]\n",
            "iteration: (684/1000) - error: [-0.06797961 -0.03294078  0.00209806  0.03713689] - loss: 0.0008862340854945426 - gradient - [-0.01542136  0.00524514]\n",
            "iteration: (685/1000) - error: [-0.06787785 -0.03289147  0.00209492  0.0370813 ] - loss: 0.000883582772832963 - gradient - [-0.01539827  0.00523729]\n",
            "iteration: (686/1000) - error: [-0.06777624 -0.03284223  0.00209178  0.03702579] - loss: 0.0008809393920021984 - gradient - [-0.01537522  0.00522945]\n",
            "iteration: (687/1000) - error: [-0.06767478 -0.03279307  0.00208865  0.03697036] - loss: 0.0008783039192728999 - gradient - [-0.01535221  0.00522162]\n",
            "iteration: (688/1000) - error: [-0.06757347 -0.03274398  0.00208552  0.03691502] - loss: 0.0008756763309866887 - gradient - [-0.01532923  0.00521381]\n",
            "iteration: (689/1000) - error: [-0.06747232 -0.03269496  0.0020824   0.03685976] - loss: 0.000873056603555979 - gradient - [-0.01530628  0.005206  ]\n",
            "iteration: (690/1000) - error: [-0.06737132 -0.03264602  0.00207928  0.03680458] - loss: 0.0008704447134637787 - gradient - [-0.01528337  0.00519821]\n",
            "iteration: (691/1000) - error: [-0.06727047 -0.03259715  0.00207617  0.03674949] - loss: 0.0008678406372633891 - gradient - [-0.01526049  0.00519043]\n",
            "iteration: (692/1000) - error: [-0.06716977 -0.03254835  0.00207306  0.03669448] - loss: 0.0008652443515783046 - gradient - [-0.01523764  0.00518266]\n",
            "iteration: (693/1000) - error: [-0.06706922 -0.03249963  0.00206996  0.03663955] - loss: 0.0008626558331019233 - gradient - [-0.01521483  0.0051749 ]\n",
            "iteration: (694/1000) - error: [-0.06696882 -0.03245098  0.00206686  0.0365847 ] - loss: 0.0008600750585973963 - gradient - [-0.01519206  0.00516715]\n",
            "iteration: (695/1000) - error: [-0.06686857 -0.0324024   0.00206377  0.03652993] - loss: 0.0008575020048973691 - gradient - [-0.01516932  0.00515942]\n",
            "iteration: (696/1000) - error: [-0.06676847 -0.0323539   0.00206068  0.03647525] - loss: 0.0008549366489038099 - gradient - [-0.01514661  0.00515169]\n",
            "iteration: (697/1000) - error: [-0.06666852 -0.03230546  0.00205759  0.03642065] - loss: 0.0008523789675877835 - gradient - [-0.01512394  0.00514398]\n",
            "iteration: (698/1000) - error: [-0.06656872 -0.0322571   0.00205451  0.03636613] - loss: 0.0008498289379892721 - gradient - [-0.0151013   0.00513628]\n",
            "iteration: (699/1000) - error: [-0.06646907 -0.03220882  0.00205144  0.03631169] - loss: 0.0008472865372168869 - gradient - [-0.01507869  0.00512859]\n",
            "iteration: (700/1000) - error: [-0.06636957 -0.0321606   0.00204837  0.03625733] - loss: 0.0008447517424478014 - gradient - [-0.01505612  0.00512092]\n",
            "iteration: (701/1000) - error: [-0.06627022 -0.03211246  0.0020453   0.03620306] - loss: 0.0008422245309273756 - gradient - [-0.01503358  0.00511325]\n",
            "iteration: (702/1000) - error: [-0.06617101 -0.03206439  0.00204224  0.03614886] - loss: 0.0008397048799691548 - gradient - [-0.01501107  0.0051056 ]\n",
            "iteration: (703/1000) - error: [-0.06607196 -0.03201639  0.00203918  0.03609475] - loss: 0.0008371927669544272 - gradient - [-0.0149886   0.00509795]\n",
            "iteration: (704/1000) - error: [-0.06597305 -0.03196846  0.00203613  0.03604072] - loss: 0.0008346881693322803 - gradient - [-0.01496617  0.00509032]\n",
            "iteration: (705/1000) - error: [-0.06587429 -0.03192061  0.00203308  0.03598677] - loss: 0.0008321910646191421 - gradient - [-0.01494376  0.0050827 ]\n",
            "iteration: (706/1000) - error: [-0.06577568 -0.03187282  0.00203004  0.0359329 ] - loss: 0.0008297014303987931 - gradient - [-0.01492139  0.00507509]\n",
            "iteration: (707/1000) - error: [-0.06567722 -0.03182511  0.002027    0.03587911] - loss: 0.0008272192443220583 - gradient - [-0.01489906  0.0050675 ]\n",
            "iteration: (708/1000) - error: [-0.0655789  -0.03177747  0.00202396  0.0358254 ] - loss: 0.0008247444841065902 - gradient - [-0.01487675  0.00505991]\n",
            "iteration: (709/1000) - error: [-0.06548074 -0.0317299   0.00202093  0.03577177] - loss: 0.0008222771275367437 - gradient - [-0.01485448  0.00505234]\n",
            "iteration: (710/1000) - error: [-0.06538271 -0.0316824   0.00201791  0.03571822] - loss: 0.0008198171524632831 - gradient - [-0.01483225  0.00504477]\n",
            "iteration: (711/1000) - error: [-0.06528484 -0.03163498  0.00201489  0.03566475] - loss: 0.0008173645368032715 - gradient - [-0.01481004  0.00503722]\n",
            "iteration: (712/1000) - error: [-0.06518711 -0.03158762  0.00201187  0.03561136] - loss: 0.0008149192585398667 - gradient - [-0.01478787  0.00502968]\n",
            "iteration: (713/1000) - error: [-0.06508953 -0.03154033  0.00200886  0.03555806] - loss: 0.0008124812957220402 - gradient - [-0.01476574  0.00502215]\n",
            "iteration: (714/1000) - error: [-0.06499209 -0.03149312  0.00200585  0.03550483] - loss: 0.0008100506264644491 - gradient - [-0.01474363  0.00501463]\n",
            "iteration: (715/1000) - error: [-0.0648948  -0.03144598  0.00200285  0.03545168] - loss: 0.0008076272289472286 - gradient - [-0.01472156  0.00500713]\n",
            "iteration: (716/1000) - error: [-0.06479766 -0.0313989   0.00199985  0.03539861] - loss: 0.0008052110814157816 - gradient - [-0.01469953  0.00499963]\n",
            "iteration: (717/1000) - error: [-0.06470066 -0.0313519   0.00199686  0.03534562] - loss: 0.0008028021621806153 - gradient - [-0.01467752  0.00499215]\n",
            "iteration: (718/1000) - error: [-0.06460381 -0.03130497  0.00199387  0.03529271] - loss: 0.0008004004496171133 - gradient - [-0.01465555  0.00498467]\n",
            "iteration: (719/1000) - error: [-0.0645071  -0.03125811  0.00199088  0.03523988] - loss: 0.0007980059221653591 - gradient - [-0.01463361  0.00497721]\n",
            "iteration: (720/1000) - error: [-0.06441053 -0.03121131  0.0019879   0.03518712] - loss: 0.0007956185583299124 - gradient - [-0.0146117   0.00496976]\n",
            "iteration: (721/1000) - error: [-0.06431411 -0.03116459  0.00198493  0.03513445] - loss: 0.0007932383366796602 - gradient - [-0.01458983  0.00496232]\n",
            "iteration: (722/1000) - error: [-0.06421784 -0.03111794  0.00198196  0.03508186] - loss: 0.0007908652358475912 - gradient - [-0.01456799  0.00495489]\n",
            "iteration: (723/1000) - error: [-0.06412171 -0.03107136  0.00197899  0.03502934] - loss: 0.000788499234530639 - gradient - [-0.01454618  0.00494748]\n",
            "iteration: (724/1000) - error: [-0.06402572 -0.03102485  0.00197603  0.0349769 ] - loss: 0.0007861403114894651 - gradient - [-0.01452441  0.00494007]\n",
            "iteration: (725/1000) - error: [-0.06392988 -0.0309784   0.00197307  0.03492454] - loss: 0.00078378844554822 - gradient - [-0.01450267  0.00493268]\n",
            "iteration: (726/1000) - error: [-0.06383418 -0.03093203  0.00197012  0.03487226] - loss: 0.0007814436155944985 - gradient - [-0.01448096  0.00492529]\n",
            "iteration: (727/1000) - error: [-0.06373862 -0.03088573  0.00196717  0.03482006] - loss: 0.0007791058005789761 - gradient - [-0.01445928  0.00491792]\n",
            "iteration: (728/1000) - error: [-0.06364321 -0.03083949  0.00196422  0.03476794] - loss: 0.0007767749795153382 - gradient - [-0.01443763  0.00491056]\n",
            "iteration: (729/1000) - error: [-0.06354794 -0.03079333  0.00196128  0.03471589] - loss: 0.000774451131480051 - gradient - [-0.01441602  0.00490321]\n",
            "iteration: (730/1000) - error: [-0.06345281 -0.03074723  0.00195835  0.03466392] - loss: 0.0007721342356121543 - gradient - [-0.01439444  0.00489587]\n",
            "iteration: (731/1000) - error: [-0.06335782 -0.0307012   0.00195541  0.03461203] - loss: 0.000769824271113118 - gradient - [-0.01437289  0.00488854]\n",
            "iteration: (732/1000) - error: [-0.06326298 -0.03065525  0.00195249  0.03456022] - loss: 0.0007675212172466615 - gradient - [-0.01435138  0.00488122]\n",
            "iteration: (733/1000) - error: [-0.06316828 -0.03060936  0.00194956  0.03450849] - loss: 0.0007652250533384711 - gradient - [-0.0143299   0.00487391]\n",
            "iteration: (734/1000) - error: [-0.06307372 -0.03056354  0.00194665  0.03445683] - loss: 0.0007629357587761433 - gradient - [-0.01430844  0.00486662]\n",
            "iteration: (735/1000) - error: [-0.0629793  -0.03051778  0.00194373  0.03440525] - loss: 0.0007606533130089064 - gradient - [-0.01428703  0.00485933]\n",
            "iteration: (736/1000) - error: [-0.06288502 -0.0304721   0.00194082  0.03435375] - loss: 0.0007583776955474922 - gradient - [-0.01426564  0.00485206]\n",
            "iteration: (737/1000) - error: [-0.06279089 -0.03042648  0.00193792  0.03430232] - loss: 0.0007561088859639195 - gradient - [-0.01424428  0.00484479]\n",
            "iteration: (738/1000) - error: [-0.06269689 -0.03038094  0.00193502  0.03425097] - loss: 0.0007538468638913107 - gradient - [-0.01422296  0.00483754]\n",
            "iteration: (739/1000) - error: [-0.06260304 -0.03033546  0.00193212  0.0341997 ] - loss: 0.0007515916090237234 - gradient - [-0.01420167  0.0048303 ]\n",
            "iteration: (740/1000) - error: [-0.06250932 -0.03029005  0.00192923  0.0341485 ] - loss: 0.0007493431011159877 - gradient - [-0.01418041  0.00482307]\n",
            "iteration: (741/1000) - error: [-0.06241575 -0.03024471  0.00192634  0.03409738] - loss: 0.0007471013199834647 - gradient - [-0.01415918  0.00481585]\n",
            "iteration: (742/1000) - error: [-0.06232232 -0.03019943  0.00192346  0.03404634] - loss: 0.0007448662455019253 - gradient - [-0.01413799  0.00480864]\n",
            "iteration: (743/1000) - error: [-0.06222902 -0.03015422  0.00192058  0.03399538] - loss: 0.0007426378576073612 - gradient - [-0.01411682  0.00480144]\n",
            "iteration: (744/1000) - error: [-0.06213587 -0.03010908  0.0019177   0.03394449] - loss: 0.0007404161362957044 - gradient - [-0.01409569  0.00479425]\n",
            "iteration: (745/1000) - error: [-0.06204286 -0.03006401  0.00191483  0.03389367] - loss: 0.000738201061622862 - gradient - [-0.01407459  0.00478708]\n",
            "iteration: (746/1000) - error: [-0.06194998 -0.03001901  0.00191196  0.03384294] - loss: 0.0007359926137043055 - gradient - [-0.01405352  0.00477991]\n",
            "iteration: (747/1000) - error: [-0.06185724 -0.02997407  0.0019091   0.03379228] - loss: 0.0007337907727150079 - gradient - [-0.01403248  0.00477276]\n",
            "iteration: (748/1000) - error: [-0.06176465 -0.0299292   0.00190624  0.03374169] - loss: 0.0007315955188893013 - gradient - [-0.01401148  0.00476561]\n",
            "iteration: (749/1000) - error: [-0.06167219 -0.0298844   0.00190339  0.03369118] - loss: 0.0007294068325206238 - gradient - [-0.0139905   0.00475848]\n",
            "iteration: (750/1000) - error: [-0.06157987 -0.02983966  0.00190054  0.03364075] - loss: 0.0007272246939613476 - gradient - [-0.01396956  0.00475135]\n",
            "iteration: (751/1000) - error: [-0.06148769 -0.02979499  0.0018977   0.03359039] - loss: 0.000725049083622641 - gradient - [-0.01394865  0.00474424]\n",
            "iteration: (752/1000) - error: [-0.06139564 -0.02975039  0.00189486  0.0335401 ] - loss: 0.0007228799819742844 - gradient - [-0.01392777  0.00473714]\n",
            "iteration: (753/1000) - error: [-0.06130374 -0.02970586  0.00189202  0.0334899 ] - loss: 0.0007207173695444761 - gradient - [-0.01390692  0.00473005]\n",
            "iteration: (754/1000) - error: [-0.06121197 -0.02966139  0.00188919  0.03343976] - loss: 0.0007185612269196693 - gradient - [-0.0138861   0.00472297]\n",
            "iteration: (755/1000) - error: [-0.06112034 -0.02961699  0.00188636  0.03338971] - loss: 0.0007164115347443774 - gradient - [-0.01386531  0.0047159 ]\n",
            "iteration: (756/1000) - error: [-0.06102884 -0.02957265  0.00188354  0.03333972] - loss: 0.0007142682737210666 - gradient - [-0.01384456  0.00470884]\n",
            "iteration: (757/1000) - error: [-0.06093748 -0.02952838  0.00188072  0.03328982] - loss: 0.0007121314246098951 - gradient - [-0.01382383  0.00470179]\n",
            "iteration: (758/1000) - error: [-0.06084626 -0.02948418  0.0018779   0.03323998] - loss: 0.0007100009682285982 - gradient - [-0.01380314  0.00469475]\n",
            "iteration: (759/1000) - error: [-0.06075518 -0.02944005  0.00187509  0.03319022] - loss: 0.00070787688545225 - gradient - [-0.01378248  0.00468772]\n",
            "iteration: (760/1000) - error: [-0.06066423 -0.02939598  0.00187228  0.03314054] - loss: 0.0007057591572132227 - gradient - [-0.01376185  0.00468071]\n",
            "iteration: (761/1000) - error: [-0.06057342 -0.02935197  0.00186948  0.03309093] - loss: 0.000703647764500878 - gradient - [-0.01374125  0.0046737 ]\n",
            "iteration: (762/1000) - error: [-0.06048275 -0.02930803  0.00186668  0.03304139] - loss: 0.0007015426883614792 - gradient - [-0.01372068  0.0046667 ]\n",
            "iteration: (763/1000) - error: [-0.06039221 -0.02926416  0.00186389  0.03299193] - loss: 0.0006994439098979374 - gradient - [-0.01370014  0.00465972]\n",
            "iteration: (764/1000) - error: [-0.0603018  -0.02922035  0.0018611   0.03294255] - loss: 0.000697351410269749 - gradient - [-0.01367963  0.00465274]\n",
            "iteration: (765/1000) - error: [-0.06021153 -0.02917661  0.00185831  0.03289323] - loss: 0.0006952651706927909 - gradient - [-0.01365915  0.00464578]\n",
            "iteration: (766/1000) - error: [-0.0601214  -0.02913294  0.00185553  0.03284399] - loss: 0.0006931851724390671 - gradient - [-0.0136387   0.00463882]\n",
            "iteration: (767/1000) - error: [-0.0600314  -0.02908932  0.00185275  0.03279483] - loss: 0.0006911113968366752 - gradient - [-0.01361829  0.00463188]\n",
            "iteration: (768/1000) - error: [-0.05994154 -0.02904578  0.00184998  0.03274573] - loss: 0.0006890438252695362 - gradient - [-0.0135979   0.00462494]\n",
            "iteration: (769/1000) - error: [-0.05985181 -0.0290023   0.00184721  0.03269672] - loss: 0.0006869824391772857 - gradient - [-0.01357755  0.00461802]\n",
            "iteration: (770/1000) - error: [-0.05976221 -0.02895888  0.00184444  0.03264777] - loss: 0.0006849272200550577 - gradient - [-0.01355722  0.00461111]\n",
            "iteration: (771/1000) - error: [-0.05967275 -0.02891553  0.00184168  0.0325989 ] - loss: 0.0006828781494533738 - gradient - [-0.01353693  0.00460421]\n",
            "iteration: (772/1000) - error: [-0.05958342 -0.02887225  0.00183893  0.0325501 ] - loss: 0.0006808352089779533 - gradient - [-0.01351666  0.00459731]\n",
            "iteration: (773/1000) - error: [-0.05949423 -0.02882903  0.00183617  0.03250137] - loss: 0.0006787983802895083 - gradient - [-0.01349643  0.00459043]\n",
            "iteration: (774/1000) - error: [-0.05940517 -0.02878587  0.00183342  0.03245272] - loss: 0.000676767645103649 - gradient - [-0.01347622  0.00458356]\n",
            "iteration: (775/1000) - error: [-0.05931624 -0.02874278  0.00183068  0.03240414] - loss: 0.0006747429851906707 - gradient - [-0.01345605  0.0045767 ]\n",
            "iteration: (776/1000) - error: [-0.05922745 -0.02869976  0.00182794  0.03235563] - loss: 0.0006727243823754359 - gradient - [-0.01343591  0.00456985]\n",
            "iteration: (777/1000) - error: [-0.05913879 -0.02865679  0.0018252   0.0323072 ] - loss: 0.0006707118185371145 - gradient - [-0.0134158   0.00456301]\n",
            "iteration: (778/1000) - error: [-0.05905026 -0.0286139   0.00182247  0.03225884] - loss: 0.000668705275609166 - gradient - [-0.01339571  0.00455618]\n",
            "iteration: (779/1000) - error: [-0.05896187 -0.02857106  0.00181974  0.03221055] - loss: 0.0006667047355790464 - gradient - [-0.01337566  0.00454936]\n",
            "iteration: (780/1000) - error: [-0.0588736  -0.02852829  0.00181702  0.03216233] - loss: 0.0006647101804881278 - gradient - [-0.01335564  0.00454255]\n",
            "iteration: (781/1000) - error: [-0.05878547 -0.02848559  0.0018143   0.03211418] - loss: 0.0006627215924314711 - gradient - [-0.01333564  0.00453575]\n",
            "iteration: (782/1000) - error: [-0.05869747 -0.02844295  0.00181158  0.03206611] - loss: 0.0006607389535577509 - gradient - [-0.01331568  0.00452896]\n",
            "iteration: (783/1000) - error: [-0.05860961 -0.02840037  0.00180887  0.03201811] - loss: 0.0006587622460690178 - gradient - [-0.01329575  0.00452218]\n",
            "iteration: (784/1000) - error: [-0.05852187 -0.02835785  0.00180616  0.03197018] - loss: 0.0006567914522205713 - gradient - [-0.01327585  0.00451541]\n",
            "iteration: (785/1000) - error: [-0.05843427 -0.0283154   0.00180346  0.03192232] - loss: 0.0006548265543208072 - gradient - [-0.01325597  0.00450865]\n",
            "iteration: (786/1000) - error: [-0.05834679 -0.02827302  0.00180076  0.03187453] - loss: 0.0006528675347310396 - gradient - [-0.01323613  0.0045019 ]\n",
            "iteration: (787/1000) - error: [-0.05825945 -0.02823069  0.00179806  0.03182682] - loss: 0.0006509143758653345 - gradient - [-0.01321631  0.00449516]\n",
            "iteration: (788/1000) - error: [-0.05817224 -0.02818843  0.00179537  0.03177918] - loss: 0.0006489670601904211 - gradient - [-0.01319653  0.00448843]\n",
            "iteration: (789/1000) - error: [-0.05808516 -0.02814624  0.00179268  0.0317316 ] - loss: 0.0006470255702254005 - gradient - [-0.01317678  0.00448171]\n",
            "iteration: (790/1000) - error: [-0.05799821 -0.0281041   0.00179     0.0316841 ] - loss: 0.0006450898885417737 - gradient - [-0.01315705  0.004475  ]\n",
            "iteration: (791/1000) - error: [-0.05791139 -0.02806203  0.00178732  0.03163667] - loss: 0.0006431599977631051 - gradient - [-0.01313736  0.0044683 ]\n",
            "iteration: (792/1000) - error: [-0.0578247  -0.02802002  0.00178465  0.03158932] - loss: 0.0006412358805649495 - gradient - [-0.01311769  0.00446161]\n",
            "iteration: (793/1000) - error: [-0.05773813 -0.02797808  0.00178197  0.03154203] - loss: 0.0006393175196747112 - gradient - [-0.01309805  0.00445494]\n",
            "iteration: (794/1000) - error: [-0.0576517  -0.0279362   0.00177931  0.03149481] - loss: 0.0006374048978714912 - gradient - [-0.01307845  0.00444827]\n",
            "iteration: (795/1000) - error: [-0.0575654  -0.02789438  0.00177664  0.03144767] - loss: 0.000635497997985834 - gradient - [-0.01305887  0.00444161]\n",
            "iteration: (796/1000) - error: [-0.05747923 -0.02785262  0.00177398  0.03140059] - loss: 0.000633596802899751 - gradient - [-0.01303932  0.00443496]\n",
            "iteration: (797/1000) - error: [-0.05739319 -0.02781093  0.00177133  0.03135358] - loss: 0.0006317012955463835 - gradient - [-0.0130198   0.00442832]\n",
            "iteration: (798/1000) - error: [-0.05730727 -0.0277693   0.00176868  0.03130665] - loss: 0.0006298114589099496 - gradient - [-0.01300031  0.00442169]\n",
            "iteration: (799/1000) - error: [-0.05722148 -0.02772773  0.00176603  0.03125979] - loss: 0.0006279272760256069 - gradient - [-0.01298085  0.00441507]\n",
            "iteration: (800/1000) - error: [-0.05713583 -0.02768622  0.00176339  0.03121299] - loss: 0.0006260487299792189 - gradient - [-0.01296142  0.00440846]\n",
            "iteration: (801/1000) - error: [-0.0570503  -0.02764478  0.00176075  0.03116627] - loss: 0.0006241758039072798 - gradient - [-0.01294202  0.00440186]\n",
            "iteration: (802/1000) - error: [-0.0569649  -0.02760339  0.00175811  0.03111961] - loss: 0.0006223084809967403 - gradient - [-0.01292264  0.00439527]\n",
            "iteration: (803/1000) - error: [-0.05687962 -0.02756207  0.00175548  0.03107303] - loss: 0.0006204467444848177 - gradient - [-0.0129033   0.00438869]\n",
            "iteration: (804/1000) - error: [-0.05679448 -0.02752081  0.00175285  0.03102651] - loss: 0.0006185905776589028 - gradient - [-0.01288398  0.00438212]\n",
            "iteration: (805/1000) - error: [-0.05670946 -0.02747962  0.00175023  0.03098007] - loss: 0.0006167399638563813 - gradient - [-0.01286469  0.00437557]\n",
            "iteration: (806/1000) - error: [-0.05662457 -0.02743848  0.00174761  0.03093369] - loss: 0.0006148948864644791 - gradient - [-0.01284544  0.00436902]\n",
            "iteration: (807/1000) - error: [-0.0565398  -0.02739741  0.00174499  0.03088739] - loss: 0.0006130553289201441 - gradient - [-0.01282621  0.00436247]\n",
            "iteration: (808/1000) - error: [-0.05645516 -0.02735639  0.00174238  0.03084115] - loss: 0.0006112212747098452 - gradient - [-0.01280701  0.00435594]\n",
            "iteration: (809/1000) - error: [-0.05637065 -0.02731544  0.00173977  0.03079498] - loss: 0.0006093927073694569 - gradient - [-0.01278784  0.00434942]\n",
            "iteration: (810/1000) - error: [-0.05628627 -0.02727455  0.00173717  0.03074888] - loss: 0.0006075696104841399 - gradient - [-0.01276869  0.00434291]\n",
            "iteration: (811/1000) - error: [-0.05620201 -0.02723372  0.00173456  0.03070285] - loss: 0.0006057519676881492 - gradient - [-0.01274958  0.00433641]\n",
            "iteration: (812/1000) - error: [-0.05611788 -0.02719296  0.00173197  0.03065689] - loss: 0.0006039397626646716 - gradient - [-0.01273049  0.00432992]\n",
            "iteration: (813/1000) - error: [-0.05603387 -0.02715225  0.00172938  0.030611  ] - loss: 0.0006021329791457421 - gradient - [-0.01271144  0.00432344]\n",
            "iteration: (814/1000) - error: [-0.05594999 -0.0271116   0.00172679  0.03056518] - loss: 0.0006003316009120615 - gradient - [-0.01269241  0.00431697]\n",
            "iteration: (815/1000) - error: [-0.05586624 -0.02707102  0.0017242   0.03051942] - loss: 0.0005985356117928282 - gradient - [-0.01267341  0.0043105 ]\n",
            "iteration: (816/1000) - error: [-0.05578261 -0.0270305   0.00172162  0.03047374] - loss: 0.0005967449956656291 - gradient - [-0.01265444  0.00430405]\n",
            "iteration: (817/1000) - error: [-0.05569911 -0.02699003  0.00171904  0.03042812] - loss: 0.0005949597364563079 - gradient - [-0.01263549  0.00429761]\n",
            "iteration: (818/1000) - error: [-0.05561573 -0.02694963  0.00171647  0.03038257] - loss: 0.0005931798181387768 - gradient - [-0.01261658  0.00429118]\n",
            "iteration: (819/1000) - error: [-0.05553247 -0.02690929  0.0017139   0.03033709] - loss: 0.0005914052247348815 - gradient - [-0.01259769  0.00428475]\n",
            "iteration: (820/1000) - error: [-0.05544934 -0.026869    0.00171134  0.03029168] - loss: 0.0005896359403142853 - gradient - [-0.01257883  0.00427834]\n",
            "iteration: (821/1000) - error: [-0.05536634 -0.02682878  0.00170877  0.03024633] - loss: 0.0005878719489943055 - gradient - [-0.01256     0.00427193]\n",
            "iteration: (822/1000) - error: [-0.05528346 -0.02678862  0.00170622  0.03020105] - loss: 0.0005861132349397901 - gradient - [-0.0125412   0.00426554]\n",
            "iteration: (823/1000) - error: [-0.0552007  -0.02674852  0.00170366  0.03015584] - loss: 0.0005843597823629239 - gradient - [-0.01252243  0.00425915]\n",
            "iteration: (824/1000) - error: [-0.05511807 -0.02670848  0.00170111  0.0301107 ] - loss: 0.0005826115755231591 - gradient - [-0.01250368  0.00425278]\n",
            "iteration: (825/1000) - error: [-0.05503556 -0.0266685   0.00169856  0.03006563] - loss: 0.0005808685987270182 - gradient - [-0.01248497  0.00424641]\n",
            "iteration: (826/1000) - error: [-0.05495318 -0.02662858  0.00169602  0.03002062] - loss: 0.0005791308363279754 - gradient - [-0.01246628  0.00424005]\n",
            "iteration: (827/1000) - error: [-0.05487091 -0.02658871  0.00169348  0.02997568] - loss: 0.0005773982727263312 - gradient - [-0.01244762  0.00423371]\n",
            "iteration: (828/1000) - error: [-0.05478877 -0.02654891  0.00169095  0.02993081] - loss: 0.0005756708923690272 - gradient - [-0.01242898  0.00422737]\n",
            "iteration: (829/1000) - error: [-0.05470676 -0.02650917  0.00168842  0.029886  ] - loss: 0.0005739486797495609 - gradient - [-0.01241038  0.00422104]\n",
            "iteration: (830/1000) - error: [-0.05462486 -0.02646949  0.00168589  0.02984127] - loss: 0.0005722316194077952 - gradient - [-0.0123918   0.00421472]\n",
            "iteration: (831/1000) - error: [-0.05454309 -0.02642986  0.00168337  0.02979659] - loss: 0.0005705196959298743 - gradient - [-0.01237325  0.00420841]\n",
            "iteration: (832/1000) - error: [-0.05446145 -0.0263903   0.00168085  0.02975199] - loss: 0.0005688128939480328 - gradient - [-0.01235473  0.00420211]\n",
            "iteration: (833/1000) - error: [-0.05437992 -0.02635079  0.00167833  0.02970745] - loss: 0.0005671111981404911 - gradient - [-0.01233623  0.00419582]\n",
            "iteration: (834/1000) - error: [-0.05429852 -0.02631135  0.00167582  0.02966298] - loss: 0.0005654145932312879 - gradient - [-0.01231777  0.00418954]\n",
            "iteration: (835/1000) - error: [-0.05421723 -0.02627196  0.00167331  0.02961858] - loss: 0.0005637230639901757 - gradient - [-0.01229933  0.00418327]\n",
            "iteration: (836/1000) - error: [-0.05413607 -0.02623263  0.0016708   0.02957424] - loss: 0.0005620365952324926 - gradient - [-0.01228092  0.00417701]\n",
            "iteration: (837/1000) - error: [-0.05405503 -0.02619337  0.0016683   0.02952997] - loss: 0.0005603551718189798 - gradient - [-0.01226253  0.00417076]\n",
            "iteration: (838/1000) - error: [-0.05397412 -0.02615416  0.00166581  0.02948577] - loss: 0.0005586787786556612 - gradient - [-0.01224418  0.00416451]\n",
            "iteration: (839/1000) - error: [-0.05389332 -0.026115    0.00166331  0.02944163] - loss: 0.0005570074006937308 - gradient - [-0.01222585  0.00415828]\n",
            "iteration: (840/1000) - error: [-0.05381264 -0.02607591  0.00166082  0.02939755] - loss: 0.000555341022929404 - gradient - [-0.01220754  0.00415205]\n",
            "iteration: (841/1000) - error: [-0.05373209 -0.02603688  0.00165834  0.02935355] - loss: 0.0005536796304037793 - gradient - [-0.01218927  0.00414584]\n",
            "iteration: (842/1000) - error: [-0.05365165 -0.0259979   0.00165585  0.02930961] - loss: 0.0005520232082027289 - gradient - [-0.01217102  0.00413963]\n",
            "iteration: (843/1000) - error: [-0.05357134 -0.02595898  0.00165337  0.02926573] - loss: 0.0005503717414567048 - gradient - [-0.0121528   0.00413344]\n",
            "iteration: (844/1000) - error: [-0.05349115 -0.02592012  0.0016509   0.02922192] - loss: 0.0005487252153406497 - gradient - [-0.01213461  0.00412725]\n",
            "iteration: (845/1000) - error: [-0.05341107 -0.02588132  0.00164843  0.02917818] - loss: 0.0005470836150738897 - gradient - [-0.01211645  0.00412107]\n",
            "iteration: (846/1000) - error: [-0.05333112 -0.02584258  0.00164596  0.0291345 ] - loss: 0.0005454469259199397 - gradient - [-0.01209831  0.0041149 ]\n",
            "iteration: (847/1000) - error: [-0.05325128 -0.02580389  0.0016435   0.02909089] - loss: 0.0005438151331864146 - gradient - [-0.0120802   0.00410874]\n",
            "iteration: (848/1000) - error: [-0.05317157 -0.02576527  0.00164104  0.02904734] - loss: 0.0005421882222248858 - gradient - [-0.01206212  0.00410259]\n",
            "iteration: (849/1000) - error: [-0.05309197 -0.0257267   0.00163858  0.02900386] - loss: 0.00054056617843073 - gradient - [-0.01204406  0.00409645]\n",
            "iteration: (850/1000) - error: [-0.0530125  -0.02568819  0.00163613  0.02896044] - loss: 0.0005389489872430332 - gradient - [-0.01202603  0.00409032]\n",
            "iteration: (851/1000) - error: [-0.05293314 -0.02564973  0.00163368  0.02891709] - loss: 0.0005373366341444464 - gradient - [-0.01200803  0.00408419]\n",
            "iteration: (852/1000) - error: [-0.0528539  -0.02561134  0.00163123  0.0288738 ] - loss: 0.0005357291046610268 - gradient - [-0.01199005  0.00407808]\n",
            "iteration: (853/1000) - error: [-0.05277478 -0.025573    0.00162879  0.02883058] - loss: 0.0005341263843621701 - gradient - [-0.0119721   0.00407198]\n",
            "iteration: (854/1000) - error: [-0.05269578 -0.02553472  0.00162635  0.02878742] - loss: 0.0005325284588604052 - gradient - [-0.01195418  0.00406588]\n",
            "iteration: (855/1000) - error: [-0.0526169  -0.02549649  0.00162392  0.02874433] - loss: 0.0005309353138113323 - gradient - [-0.01193629  0.00405979]\n",
            "iteration: (856/1000) - error: [-0.05253813 -0.02545832  0.00162149  0.0287013 ] - loss: 0.0005293469349134477 - gradient - [-0.01191842  0.00405372]\n",
            "iteration: (857/1000) - error: [-0.05245949 -0.02542021  0.00161906  0.02865833] - loss: 0.0005277633079080422 - gradient - [-0.01190058  0.00404765]\n",
            "iteration: (858/1000) - error: [-0.05238096 -0.02538216  0.00161664  0.02861543] - loss: 0.0005261844185790507 - gradient - [-0.01188276  0.00404159]\n",
            "iteration: (859/1000) - error: [-0.05230255 -0.02534417  0.00161422  0.0285726 ] - loss: 0.0005246102527529438 - gradient - [-0.01186498  0.00403554]\n",
            "iteration: (860/1000) - error: [-0.05222425 -0.02530623  0.0016118   0.02852982] - loss: 0.000523040796298636 - gradient - [-0.01184721  0.0040295 ]\n",
            "iteration: (861/1000) - error: [-0.05214607 -0.02526834  0.00160939  0.02848712] - loss: 0.0005214760351272344 - gradient - [-0.01182948  0.00402347]\n",
            "iteration: (862/1000) - error: [-0.05206801 -0.02523052  0.00160698  0.02844447] - loss: 0.0005199159551920677 - gradient - [-0.01181177  0.00401744]\n",
            "iteration: (863/1000) - error: [-0.05199007 -0.02519275  0.00160457  0.02840189] - loss: 0.0005183605424884468 - gradient - [-0.01179409  0.00401143]\n",
            "iteration: (864/1000) - error: [-0.05191224 -0.02515504  0.00160217  0.02835938] - loss: 0.0005168097830536062 - gradient - [-0.01177643  0.00400542]\n",
            "iteration: (865/1000) - error: [-0.05183453 -0.02511738  0.00159977  0.02831692] - loss: 0.0005152636629665183 - gradient - [-0.01175881  0.00399943]\n",
            "iteration: (866/1000) - error: [-0.05175694 -0.02507978  0.00159738  0.02827453] - loss: 0.0005137221683478475 - gradient - [-0.0117412   0.00399344]\n",
            "iteration: (867/1000) - error: [-0.05167946 -0.02504224  0.00159499  0.02823221] - loss: 0.0005121852853597423 - gradient - [-0.01172363  0.00398746]\n",
            "iteration: (868/1000) - error: [-0.0516021  -0.02500475  0.0015926   0.02818995] - loss: 0.0005106530002057675 - gradient - [-0.01170608  0.00398149]\n",
            "iteration: (869/1000) - error: [-0.05152486 -0.02496732  0.00159021  0.02814775] - loss: 0.0005091252991307657 - gradient - [-0.01168855  0.00397553]\n",
            "iteration: (870/1000) - error: [-0.05144773 -0.02492995  0.00158783  0.02810561] - loss: 0.0005076021684207055 - gradient - [-0.01167106  0.00396958]\n",
            "iteration: (871/1000) - error: [-0.05137071 -0.02489263  0.00158546  0.02806354] - loss: 0.0005060835944026128 - gradient - [-0.01165359  0.00396364]\n",
            "iteration: (872/1000) - error: [-0.05129381 -0.02485536  0.00158308  0.02802153] - loss: 0.0005045695634443987 - gradient - [-0.01163614  0.00395771]\n",
            "iteration: (873/1000) - error: [-0.05121703 -0.02481816  0.00158071  0.02797958] - loss: 0.0005030600619547914 - gradient - [-0.01161872  0.00395178]\n",
            "iteration: (874/1000) - error: [-0.05114036 -0.02478101  0.00157835  0.0279377 ] - loss: 0.000501555076383105 - gradient - [-0.01160133  0.00394587]\n",
            "iteration: (875/1000) - error: [-0.0510638  -0.02474391  0.00157598  0.02789588] - loss: 0.0005000545932192877 - gradient - [-0.01158396  0.00393996]\n",
            "iteration: (876/1000) - error: [-0.05098736 -0.02470687  0.00157362  0.02785412] - loss: 0.0004985585989936203 - gradient - [-0.01156662  0.00393406]\n",
            "iteration: (877/1000) - error: [-0.05091104 -0.02466988  0.00157127  0.02781242] - loss: 0.0004970670802767342 - gradient - [-0.01154931  0.00392817]\n",
            "iteration: (878/1000) - error: [-0.05083483 -0.02463295  0.00156892  0.02777079] - loss: 0.0004955800236794357 - gradient - [-0.01153202  0.00392229]\n",
            "iteration: (879/1000) - error: [-0.05075873 -0.02459608  0.00156657  0.02772922] - loss: 0.000494097415852547 - gradient - [-0.01151476  0.00391642]\n",
            "iteration: (880/1000) - error: [-0.05068275 -0.02455926  0.00156422  0.02768771] - loss: 0.0004926192434868608 - gradient - [-0.01149752  0.00391056]\n",
            "iteration: (881/1000) - error: [-0.05060688 -0.0245225   0.00156188  0.02764626] - loss: 0.0004911454933129815 - gradient - [-0.01148031  0.0039047 ]\n",
            "iteration: (882/1000) - error: [-0.05053112 -0.02448579  0.00155954  0.02760488] - loss: 0.0004896761521012117 - gradient - [-0.01146312  0.00389886]\n",
            "iteration: (883/1000) - error: [-0.05045548 -0.02444913  0.00155721  0.02756355] - loss: 0.0004882112066614204 - gradient - [-0.01144596  0.00389302]\n",
            "iteration: (884/1000) - error: [-0.05037995 -0.02441253  0.00155488  0.02752229] - loss: 0.0004867506438429323 - gradient - [-0.01142883  0.0038872 ]\n",
            "iteration: (885/1000) - error: [-0.05030453 -0.02437599  0.00155255  0.02748109] - loss: 0.00048529445053444 - gradient - [-0.01141172  0.00388138]\n",
            "iteration: (886/1000) - error: [-0.05022923 -0.0243395   0.00155023  0.02743995] - loss: 0.00048384261366384674 - gradient - [-0.01139464  0.00387557]\n",
            "iteration: (887/1000) - error: [-0.05015404 -0.02430307  0.00154791  0.02739888] - loss: 0.0004823951201981546 - gradient - [-0.01137758  0.00386976]\n",
            "iteration: (888/1000) - error: [-0.05007896 -0.02426669  0.00154559  0.02735786] - loss: 0.00048095195714339164 - gradient - [-0.01136055  0.00386397]\n",
            "iteration: (889/1000) - error: [-0.05000399 -0.02423036  0.00154328  0.02731691] - loss: 0.00047951311154441144 - gradient - [-0.01134354  0.00385819]\n",
            "iteration: (890/1000) - error: [-0.04992914 -0.02419409  0.00154096  0.02727602] - loss: 0.0004780785704848373 - gradient - [-0.01132656  0.00385241]\n",
            "iteration: (891/1000) - error: [-0.0498544  -0.02415787  0.00153866  0.02723519] - loss: 0.0004766483210869669 - gradient - [-0.01130961  0.00384665]\n",
            "iteration: (892/1000) - error: [-0.04977977 -0.02412171  0.00153635  0.02719442] - loss: 0.00047522235051157774 - gradient - [-0.01129268  0.00384089]\n",
            "iteration: (893/1000) - error: [-0.04970525 -0.0240856   0.00153405  0.02715371] - loss: 0.00047380064595789576 - gradient - [-0.01127577  0.00383514]\n",
            "iteration: (894/1000) - error: [-0.04963084 -0.02404954  0.00153176  0.02711306] - loss: 0.0004723831946634163 - gradient - [-0.01125889  0.0038294 ]\n",
            "iteration: (895/1000) - error: [-0.04955655 -0.02401354  0.00152947  0.02707247] - loss: 0.0004709699839038285 - gradient - [-0.01124204  0.00382366]\n",
            "iteration: (896/1000) - error: [-0.04948237 -0.02397759  0.00152718  0.02703195] - loss: 0.00046956100099290185 - gradient - [-0.01122521  0.00381794]\n",
            "iteration: (897/1000) - error: [-0.04940829 -0.0239417   0.00152489  0.02699148] - loss: 0.000468156233282325 - gradient - [-0.01120841  0.00381222]\n",
            "iteration: (898/1000) - error: [-0.04933433 -0.02390586  0.00152261  0.02695108] - loss: 0.0004667556681616431 - gradient - [-0.01119163  0.00380652]\n",
            "iteration: (899/1000) - error: [-0.04926048 -0.02387008  0.00152033  0.02691073] - loss: 0.00046535929305813975 - gradient - [-0.01117487  0.00380082]\n",
            "iteration: (900/1000) - error: [-0.04918674 -0.02383434  0.00151805  0.02687045] - loss: 0.00046396709543669216 - gradient - [-0.01115815  0.00379513]\n",
            "iteration: (901/1000) - error: [-0.04911311 -0.02379866  0.00151578  0.02683022] - loss: 0.0004625790627996911 - gradient - [-0.01114144  0.00378945]\n",
            "iteration: (902/1000) - error: [-0.04903959 -0.02376304  0.00151351  0.02679006] - loss: 0.0004611951826869146 - gradient - [-0.01112476  0.00378378]\n",
            "iteration: (903/1000) - error: [-0.04896618 -0.02372747  0.00151124  0.02674996] - loss: 0.00045981544267541534 - gradient - [-0.01110811  0.00377811]\n",
            "iteration: (904/1000) - error: [-0.04889288 -0.02369195  0.00150898  0.02670991] - loss: 0.0004584398303794032 - gradient - [-0.01109148  0.00377246]\n",
            "iteration: (905/1000) - error: [-0.04881969 -0.02365648  0.00150672  0.02666993] - loss: 0.00045706833345014714 - gradient - [-0.01107488  0.00376681]\n",
            "iteration: (906/1000) - error: [-0.04874661 -0.02362107  0.00150447  0.02663001] - loss: 0.0004557009395758665 - gradient - [-0.0110583   0.00376117]\n",
            "iteration: (907/1000) - error: [-0.04867364 -0.02358571  0.00150222  0.02659014] - loss: 0.0004543376364815993 - gradient - [-0.01104175  0.00375554]\n",
            "iteration: (908/1000) - error: [-0.04860078 -0.0235504   0.00149997  0.02655034] - loss: 0.00045297841192914746 - gradient - [-0.01102522  0.00374992]\n",
            "iteration: (909/1000) - error: [-0.04852802 -0.02351515  0.00149772  0.02651059] - loss: 0.00045162325371684767 - gradient - [-0.01100871  0.00374431]\n",
            "iteration: (910/1000) - error: [-0.04845538 -0.02347995  0.00149548  0.02647091] - loss: 0.0004502721496796151 - gradient - [-0.01099223  0.0037387 ]\n",
            "iteration: (911/1000) - error: [-0.04838284 -0.0234448   0.00149324  0.02643128] - loss: 0.00044892508768873364 - gradient - [-0.01097578  0.0037331 ]\n",
            "iteration: (912/1000) - error: [-0.04831042 -0.02340971  0.00149101  0.02639172] - loss: 0.0004475820556517387 - gradient - [-0.01095935  0.00372752]\n",
            "iteration: (913/1000) - error: [-0.0482381  -0.02337466  0.00148877  0.02635221] - loss: 0.00044624304151239604 - gradient - [-0.01094294  0.00372194]\n",
            "iteration: (914/1000) - error: [-0.04816589 -0.02333967  0.00148655  0.02631276] - loss: 0.0004449080332504998 - gradient - [-0.01092656  0.00371636]\n",
            "iteration: (915/1000) - error: [-0.04809379 -0.02330473  0.00148432  0.02627337] - loss: 0.0004435770188818367 - gradient - [-0.01091021  0.0037108 ]\n",
            "iteration: (916/1000) - error: [-0.04802179 -0.02326985  0.0014821   0.02623404] - loss: 0.00044224998645801746 - gradient - [-0.01089387  0.00370525]\n",
            "iteration: (917/1000) - error: [-0.04794991 -0.02323501  0.00147988  0.02619477] - loss: 0.0004409269240664088 - gradient - [-0.01087757  0.0036997 ]\n",
            "iteration: (918/1000) - error: [-0.04787813 -0.02320023  0.00147766  0.02615556] - loss: 0.00043960781983001324 - gradient - [-0.01086128  0.00369416]\n",
            "iteration: (919/1000) - error: [-0.04780646 -0.0231655   0.00147545  0.02611641] - loss: 0.0004382926619073884 - gradient - [-0.01084502  0.00368863]\n",
            "iteration: (920/1000) - error: [-0.04773489 -0.02313082  0.00147324  0.02607731] - loss: 0.0004369814384924759 - gradient - [-0.01082879  0.00368311]\n",
            "iteration: (921/1000) - error: [-0.04766343 -0.0230962   0.00147104  0.02603827] - loss: 0.0004356741378145699 - gradient - [-0.01081258  0.0036776 ]\n",
            "iteration: (922/1000) - error: [-0.04759208 -0.02306162  0.00146884  0.0259993 ] - loss: 0.00043437074813815896 - gradient - [-0.01079639  0.00367209]\n",
            "iteration: (923/1000) - error: [-0.04752084 -0.0230271   0.00146664  0.02596038] - loss: 0.000433071257762866 - gradient - [-0.01078023  0.00366659]\n",
            "iteration: (924/1000) - error: [-0.04744971 -0.02299263  0.00146444  0.02592152] - loss: 0.00043177565502329577 - gradient - [-0.01076409  0.0036611 ]\n",
            "iteration: (925/1000) - error: [-0.04737868 -0.02295821  0.00146225  0.02588271] - loss: 0.00043048392828894295 - gradient - [-0.01074798  0.00365562]\n",
            "iteration: (926/1000) - error: [-0.04730775 -0.02292385  0.00146006  0.02584397] - loss: 0.0004291960659641323 - gradient - [-0.01073189  0.00365015]\n",
            "iteration: (927/1000) - error: [-0.04723693 -0.02288953  0.00145788  0.02580528] - loss: 0.00042791205648783136 - gradient - [-0.01071583  0.00364469]\n",
            "iteration: (928/1000) - error: [-0.04716622 -0.02285527  0.00145569  0.02576665] - loss: 0.00042663188833363746 - gradient - [-0.01069979  0.00363923]\n",
            "iteration: (929/1000) - error: [-0.04709562 -0.02282105  0.00145351  0.02572808] - loss: 0.00042535555000962084 - gradient - [-0.01068377  0.00363378]\n",
            "iteration: (930/1000) - error: [-0.04702512 -0.02278689  0.00145134  0.02568957] - loss: 0.0004240830300582037 - gradient - [-0.01066778  0.00362834]\n",
            "iteration: (931/1000) - error: [-0.04695472 -0.02275278  0.00144917  0.02565111] - loss: 0.0004228143170561271 - gradient - [-0.01065181  0.00362291]\n",
            "iteration: (932/1000) - error: [-0.04688443 -0.02271872  0.001447    0.02561271] - loss: 0.00042154939961426465 - gradient - [-0.01063586  0.00361749]\n",
            "iteration: (933/1000) - error: [-0.04681425 -0.02268471  0.00144483  0.02557437] - loss: 0.0004202882663776089 - gradient - [-0.01061994  0.00361207]\n",
            "iteration: (934/1000) - error: [-0.04674417 -0.02265075  0.00144267  0.02553609] - loss: 0.00041903090602507894 - gradient - [-0.01060404  0.00360667]\n",
            "iteration: (935/1000) - error: [-0.0466742  -0.02261685  0.00144051  0.02549786] - loss: 0.0004177773072694925 - gradient - [-0.01058817  0.00360127]\n",
            "iteration: (936/1000) - error: [-0.04660433 -0.02258299  0.00143835  0.02545969] - loss: 0.00041652745885740393 - gradient - [-0.01057232  0.00359588]\n",
            "iteration: (937/1000) - error: [-0.04653457 -0.02254918  0.0014362   0.02542158] - loss: 0.0004152813495690785 - gradient - [-0.01055649  0.00359049]\n",
            "iteration: (938/1000) - error: [-0.04646491 -0.02251543  0.00143405  0.02538352] - loss: 0.0004140389682183252 - gradient - [-0.01054069  0.00358512]\n",
            "iteration: (939/1000) - error: [-0.04639535 -0.02248172  0.0014319   0.02534553] - loss: 0.00041280030365238406 - gradient - [-0.01052491  0.00357975]\n",
            "iteration: (940/1000) - error: [-0.0463259  -0.02244807  0.00142976  0.02530759] - loss: 0.0004115653447519224 - gradient - [-0.01050916  0.00357439]\n",
            "iteration: (941/1000) - error: [-0.04625655 -0.02241447  0.00142762  0.0252697 ] - loss: 0.00041033408043082746 - gradient - [-0.01049342  0.00356904]\n",
            "iteration: (942/1000) - error: [-0.04618731 -0.02238091  0.00142548  0.02523187] - loss: 0.00040910649963617207 - gradient - [-0.01047772  0.0035637 ]\n",
            "iteration: (943/1000) - error: [-0.04611817 -0.02234741  0.00142335  0.0251941 ] - loss: 0.0004078825913480753 - gradient - [-0.01046203  0.00355837]\n",
            "iteration: (944/1000) - error: [-0.04604913 -0.02231396  0.00142122  0.02515639] - loss: 0.0004066623445796805 - gradient - [-0.01044637  0.00355304]\n",
            "iteration: (945/1000) - error: [-0.0459802  -0.02228055  0.00141909  0.02511873] - loss: 0.0004054457483769116 - gradient - [-0.01043073  0.00354772]\n",
            "iteration: (946/1000) - error: [-0.04591137 -0.0222472   0.00141696  0.02508113] - loss: 0.00040423279181854034 - gradient - [-0.01041512  0.00354241]\n",
            "iteration: (947/1000) - error: [-0.04584264 -0.0222139   0.00141484  0.02504358] - loss: 0.00040302346401596147 - gradient - [-0.01039953  0.00353711]\n",
            "iteration: (948/1000) - error: [-0.04577401 -0.02218064  0.00141273  0.0250061 ] - loss: 0.00040181775411319307 - gradient - [-0.01038396  0.00353181]\n",
            "iteration: (949/1000) - error: [-0.04570549 -0.02214744  0.00141061  0.02496866] - loss: 0.00040061565128667666 - gradient - [-0.01036842  0.00352653]\n",
            "iteration: (950/1000) - error: [-0.04563707 -0.02211429  0.0014085   0.02493129] - loss: 0.0003994171447452656 - gradient - [-0.01035289  0.00352125]\n",
            "iteration: (951/1000) - error: [-0.04556876 -0.02208118  0.00140639  0.02489396] - loss: 0.0003982222237300894 - gradient - [-0.0103374   0.00351598]\n",
            "iteration: (952/1000) - error: [-0.04550054 -0.02204813  0.00140428  0.0248567 ] - loss: 0.0003970308775144918 - gradient - [-0.01032192  0.00351071]\n",
            "iteration: (953/1000) - error: [-0.04543243 -0.02201512  0.00140218  0.02481949] - loss: 0.0003958430954038425 - gradient - [-0.01030647  0.00350546]\n",
            "iteration: (954/1000) - error: [-0.04536442 -0.02198217  0.00140008  0.02478234] - loss: 0.0003946588667355674 - gradient - [-0.01029104  0.00350021]\n",
            "iteration: (955/1000) - error: [-0.04529651 -0.02194926  0.00139799  0.02474524] - loss: 0.0003934781808789733 - gradient - [-0.01027564  0.00349497]\n",
            "iteration: (956/1000) - error: [-0.04522871 -0.02191641  0.0013959   0.0247082 ] - loss: 0.0003923010272351496 - gradient - [-0.01026026  0.00348974]\n",
            "iteration: (957/1000) - error: [-0.045161   -0.0218836   0.00139381  0.02467121] - loss: 0.0003911273952369133 - gradient - [-0.0102449   0.00348451]\n",
            "iteration: (958/1000) - error: [-0.0450934  -0.02185084  0.00139172  0.02463428] - loss: 0.00038995727434870805 - gradient - [-0.01022956  0.0034793 ]\n",
            "iteration: (959/1000) - error: [-0.04502589 -0.02181813  0.00138964  0.0245974 ] - loss: 0.0003887906540664595 - gradient - [-0.01021425  0.00347409]\n",
            "iteration: (960/1000) - error: [-0.04495849 -0.02178547  0.00138756  0.02456058] - loss: 0.000387627523917547 - gradient - [-0.01019896  0.00346889]\n",
            "iteration: (961/1000) - error: [-0.04489119 -0.02175286  0.00138548  0.02452381] - loss: 0.00038646787346066977 - gradient - [-0.01018369  0.0034637 ]\n",
            "iteration: (962/1000) - error: [-0.04482399 -0.02172029  0.0013834   0.0244871 ] - loss: 0.000385311692285777 - gradient - [-0.01016844  0.00345851]\n",
            "iteration: (963/1000) - error: [-0.04475689 -0.02168778  0.00138133  0.02445045] - loss: 0.0003841589700139348 - gradient - [-0.01015322  0.00345333]\n",
            "iteration: (964/1000) - error: [-0.04468989 -0.02165531  0.00137927  0.02441385] - loss: 0.0003830096962972974 - gradient - [-0.01013802  0.00344816]\n",
            "iteration: (965/1000) - error: [-0.044623  -0.0216229  0.0013772  0.0243773] - loss: 0.00038186386081891374 - gradient - [-0.01012285  0.003443  ]\n",
            "iteration: (966/1000) - error: [-0.0445562  -0.02159053  0.00137514  0.02434081] - loss: 0.0003807214532927747 - gradient - [-0.01010769  0.00343785]\n",
            "iteration: (967/1000) - error: [-0.0444895  -0.02155821  0.00137308  0.02430437] - loss: 0.00037958246346360343 - gradient - [-0.01009256  0.0034327 ]\n",
            "iteration: (968/1000) - error: [-0.0444229  -0.02152594  0.00137103  0.02426799] - loss: 0.00037844688110678253 - gradient - [-0.01007746  0.00342756]\n",
            "iteration: (969/1000) - error: [-0.0443564  -0.02149371  0.00136897  0.02423166] - loss: 0.0003773146960283423 - gradient - [-0.01006237  0.00342243]\n",
            "iteration: (970/1000) - error: [-0.04429    -0.02146154  0.00136692  0.02419539] - loss: 0.0003761858980647397 - gradient - [-0.01004731  0.00341731]\n",
            "iteration: (971/1000) - error: [-0.0442237  -0.02142941  0.00136488  0.02415917] - loss: 0.0003750604770828842 - gradient - [-0.01003227  0.00341219]\n",
            "iteration: (972/1000) - error: [-0.0441575  -0.02139733  0.00136283  0.024123  ] - loss: 0.00037393842297999005 - gradient - [-0.01001725  0.00340709]\n",
            "iteration: (973/1000) - error: [-0.0440914  -0.0213653   0.00136079  0.02408689] - loss: 0.00037281972568351397 - gradient - [-0.01000225  0.00340199]\n",
            "iteration: (974/1000) - error: [-0.0440254  -0.02133332  0.00135876  0.02405083] - loss: 0.0003717043751509901 - gradient - [-0.00998728  0.00339689]\n",
            "iteration: (975/1000) - error: [-0.04395949 -0.02130138  0.00135672  0.02401483] - loss: 0.00037059236137006277 - gradient - [-0.00997233  0.00339181]\n",
            "iteration: (976/1000) - error: [-0.04389369 -0.0212695   0.00135469  0.02397888] - loss: 0.0003694836743582747 - gradient - [-0.0099574   0.00338673]\n",
            "iteration: (977/1000) - error: [-0.04382798 -0.02123766  0.00135266  0.02394299] - loss: 0.0003683783041630738 - gradient - [-0.0099425   0.00338166]\n",
            "iteration: (978/1000) - error: [-0.04376237 -0.02120587  0.00135064  0.02390715] - loss: 0.0003672762408616758 - gradient - [-0.00992761  0.0033766 ]\n",
            "iteration: (979/1000) - error: [-0.04369686 -0.02117412  0.00134862  0.02387136] - loss: 0.00036617747456094553 - gradient - [-0.00991275  0.00337154]\n",
            "iteration: (980/1000) - error: [-0.04363145 -0.02114243  0.0013466   0.02383562] - loss: 0.00036508199539739603 - gradient - [-0.00989791  0.0033665 ]\n",
            "iteration: (981/1000) - error: [-0.04356614 -0.02111078  0.00134458  0.02379994] - loss: 0.00036398979353701903 - gradient - [-0.0098831   0.00336146]\n",
            "iteration: (982/1000) - error: [-0.04350092 -0.02107917  0.00134257  0.02376432] - loss: 0.0003629008591752226 - gradient - [-0.0098683   0.00335643]\n",
            "iteration: (983/1000) - error: [-0.0434358  -0.02104762  0.00134056  0.02372874] - loss: 0.0003618151825367528 - gradient - [-0.00985353  0.0033514 ]\n",
            "iteration: (984/1000) - error: [-0.04337078 -0.02101611  0.00133855  0.02369322] - loss: 0.0003607327538756293 - gradient - [-0.00983878  0.00334638]\n",
            "iteration: (985/1000) - error: [-0.04330586 -0.02098465  0.00133655  0.02365775] - loss: 0.0003596535634749741 - gradient - [-0.00982405  0.00334138]\n",
            "iteration: (986/1000) - error: [-0.04324103 -0.02095324  0.00133455  0.02362234] - loss: 0.00035857760164702783 - gradient - [-0.00980935  0.00333637]\n",
            "iteration: (987/1000) - error: [-0.0431763  -0.02092187  0.00133255  0.02358698] - loss: 0.00035750485873297196 - gradient - [-0.00979466  0.00333138]\n",
            "iteration: (988/1000) - error: [-0.04311167 -0.02089056  0.00133056  0.02355167] - loss: 0.00035643532510291215 - gradient - [-0.00978     0.00332639]\n",
            "iteration: (989/1000) - error: [-0.04304713 -0.02085928  0.00132857  0.02351641] - loss: 0.0003553689911557769 - gradient - [-0.00976536  0.00332141]\n",
            "iteration: (990/1000) - error: [-0.04298269 -0.02082806  0.00132658  0.02348121] - loss: 0.0003543058473191672 - gradient - [-0.00975074  0.00331644]\n",
            "iteration: (991/1000) - error: [-0.04291835 -0.02079688  0.00132459  0.02344606] - loss: 0.00035324588404935995 - gradient - [-0.00973614  0.00331148]\n",
            "iteration: (992/1000) - error: [-0.0428541  -0.02076575  0.00132261  0.02341096] - loss: 0.00035218909183119503 - gradient - [-0.00972157  0.00330652]\n",
            "iteration: (993/1000) - error: [-0.04278995 -0.02073466  0.00132063  0.02337592] - loss: 0.0003511354611779304 - gradient - [-0.00970702  0.00330157]\n",
            "iteration: (994/1000) - error: [-0.0427259  -0.02070362  0.00131865  0.02334092] - loss: 0.000350084982631256 - gradient - [-0.00969249  0.00329663]\n",
            "iteration: (995/1000) - error: [-0.04266194 -0.02067263  0.00131668  0.02330598] - loss: 0.0003490376467611031 - gradient - [-0.00967798  0.00329169]\n",
            "iteration: (996/1000) - error: [-0.04259808 -0.02064168  0.00131471  0.0232711 ] - loss: 0.00034799344416569224 - gradient - [-0.00966349  0.00328676]\n",
            "iteration: (997/1000) - error: [-0.04253431 -0.02061079  0.00131274  0.02323626] - loss: 0.00034695236547128603 - gradient - [-0.00964902  0.00328184]\n",
            "iteration: (998/1000) - error: [-0.04247064 -0.02057993  0.00131077  0.02320148] - loss: 0.00034591440133224544 - gradient - [-0.00963458  0.00327693]\n",
            "iteration: (999/1000) - error: [-0.04240706 -0.02054912  0.00130881  0.02316675] - loss: 0.0003448795424308671 - gradient - [-0.00962016  0.00327203]\n",
            "iteration: (1000/1000) - error: [-0.04234358 -0.02051836  0.00130685  0.02313207] - loss: 0.000343847779477339 - gradient - [-0.00960576  0.00326713]\n",
            "\u001b[36m\u001b[1m===================================== short test summary info ======================================\u001b[0m\n",
            "\u001b[31mFAILED\u001b[0m t_71175de287a04e5cb55ec2fc7e014835.py::\u001b[1mtest_linear_regression\u001b[0m - AssertionError: predictions [13.06668253 15.08847507] do not match expected [13 16]\n",
            "\u001b[31m\u001b[31m\u001b[1m1 failed\u001b[0m\u001b[31m in 0.23s\u001b[0m\u001b[0m\n"
          ]
        }
      ]
    }
  ]
}